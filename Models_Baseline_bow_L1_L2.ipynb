{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Models_Baseline_bow_L1_L2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Rk8kQGXA-MSr",
        "fVddnrs--Q0E",
        "JGMAJZdsI2ce",
        "6mzgNQ5pkp9u",
        "j-PEdsPhkiTi",
        "jdr_oY88EMuW"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DianaCarolinaCabrera/Thesis_UNAL/blob/main/Models_Baseline_bow_L1_L2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-IsWBj_7Ab0",
        "outputId": "283914db-3514-44ce-c743-65e224a43b4d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "kuNIwR6w7hPS",
        "outputId": "c7bda53e-69fb-443d-fd3b-48f66a926bbb"
      },
      "source": [
        "pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvK8Sorp7wSe",
        "outputId": "08f3ed33-f4d5-4e1f-cb70-ab1be1a17368"
      },
      "source": [
        "cd /content/drive/MyDrive/Tesis_Maestria/Experiment_BOW_L1_L2_QMKDC_QMKDCSGD"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Tesis_Maestria/Experiment_BOW_L1_L2_QMKDC_QMKDCSGD\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMG3jKbMpikf"
      },
      "source": [
        "#Model Baseline BOW "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rk8kQGXA-MSr"
      },
      "source": [
        "#Explore data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdKhuHVJ7xMz",
        "outputId": "ba95568f-fcb0-463e-f759-542f3a8003a2"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_style('darkgrid')\n",
        "import plotly.express as ex\n",
        "import plotly.graph_objs as go\n",
        "from wordcloud import WordCloud,STOPWORDS\n",
        "stopwords = list(STOPWORDS)\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/twitter/__init__.py:20: UserWarning:\n",
            "\n",
            "The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQWqATe675Vn"
      },
      "source": [
        "t_data = pd.read_csv('/content/drive/MyDrive/Tesis_Maestria/Experiment_BOW_L1_L2_QMKDC_QMKDCSGD/Dataset/tripadvisor_hotel_reviews.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFq5Le7_8ABf",
        "outputId": "b6694575-9d00-41aa-f25a-56f1a58248d1"
      },
      "source": [
        "t_data.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20491, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "id": "spW_fcGc8DQC",
        "outputId": "1ec4e601-5c11-4560-babc-f5c5d01d1581"
      },
      "source": [
        "t_data.head(6)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Review</th>\n",
              "      <th>Rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>nice hotel expensive parking got good deal sta...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ok nothing special charge diamond member hilto...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>nice rooms not 4* experience hotel monaco seat...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>unique, great stay, wonderful time hotel monac...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>great stay great stay, went seahawk game aweso...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>love monaco staff husband stayed hotel crazy w...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              Review  Rating\n",
              "0  nice hotel expensive parking got good deal sta...       4\n",
              "1  ok nothing special charge diamond member hilto...       2\n",
              "2  nice rooms not 4* experience hotel monaco seat...       3\n",
              "3  unique, great stay, wonderful time hotel monac...       5\n",
              "4  great stay great stay, went seahawk game aweso...       5\n",
              "5  love monaco staff husband stayed hotel crazy w...       5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNjn2JbK92-t"
      },
      "source": [
        "t_data['Rating'] = t_data['Rating']-1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJz2ggWI9942",
        "outputId": "c5650056-cc29-42d3-dc83-0099c6c924d5"
      },
      "source": [
        "t_data['Rating'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4    9054\n",
              "3    6039\n",
              "2    2184\n",
              "1    1793\n",
              "0    1421\n",
              "Name: Rating, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "hG27jz5D-EWo",
        "outputId": "15b13263-01f0-40b5-b7ec-31080c038d68"
      },
      "source": [
        "t_data.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>20491.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>2.952223</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1.233030</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>3.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>4.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>4.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             Rating\n",
              "count  20491.000000\n",
              "mean       2.952223\n",
              "std        1.233030\n",
              "min        0.000000\n",
              "25%        2.000000\n",
              "50%        3.000000\n",
              "75%        4.000000\n",
              "max        4.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yANVLFV0-CKL",
        "outputId": "71de3aca-33c6-455e-b13a-f7c6e9ea3986"
      },
      "source": [
        "t_data.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 20491 entries, 0 to 20490\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   Review  20491 non-null  object\n",
            " 1   Rating  20491 non-null  int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 320.3+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVddnrs--Q0E"
      },
      "source": [
        "#Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcUMOO69-UDs"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lEGZuNEwg0YU",
        "outputId": "e83c411a-7a09-43e7-8c32-d51595eada1e"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8ZjTQzeg11k"
      },
      "source": [
        "import string\n",
        "punc=string.punctuation\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKmTqa6qDS0W"
      },
      "source": [
        "def pre_processing_bow():\n",
        "    corpus = []\n",
        "    for i in range(0,20491):\n",
        "      review = re.sub('[^a-zA-Z]', ' ' , t_data['Review'][i])\n",
        "      review = review.lower()\n",
        "      review = review.split()\n",
        "      review = [stemmer.stem(word) for word in review if not word in stopwords.words('english')]\n",
        "      review = ' '.join(review)\n",
        "      corpus.append(review)\n",
        "    return corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJoVltS6DgG-"
      },
      "source": [
        "corpus = pre_processing_bow()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fuIkoEMtDixl",
        "outputId": "140360f2-1a4c-4e1d-93e3-11436526dea4"
      },
      "source": [
        "y_bow = t_data['Rating']\n",
        "print('X', len(corpus))\n",
        "print('y', y_bow.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X 20491\n",
            "y (20491,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLvLVJx1FPWR"
      },
      "source": [
        "###BOW Representation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dKQubS3EohO"
      },
      "source": [
        "def my_bow(X):\n",
        "\n",
        "  \"\"\"\n",
        "      X: Corpus pre_processing'\n",
        "  \"\"\"\n",
        "  vect = CountVectorizer(max_features = 1000)\n",
        "  X_vectors = vect.fit_transform(X).toarray() \n",
        "  return X_vectors\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4QE3jlzFaHk"
      },
      "source": [
        "X_vectors_bow = my_bow(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNhSUrJPFgxu",
        "outputId": "69e78049-575d-46cf-96b5-a54b1976b2f1"
      },
      "source": [
        "print('X_vectors', X_vectors_bow)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_vectors [[0 0 0 ... 0 0 0]\n",
            " [0 0 1 ... 0 0 0]\n",
            " [0 0 1 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 2 0 ... 3 0 1]\n",
            " [0 0 0 ... 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXBXYcEi6HTn"
      },
      "source": [
        "###Normalizer L1 Columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aX4XTOpV6Mw6"
      },
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "normalizer = preprocessing.Normalizer()\n",
        "X_vectors_bow = normalizer.fit_transform(X_vectors_bow, 'l1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "es1GB6hW6Oeo",
        "outputId": "2497007c-a480-4b0c-e626-379b7c078479"
      },
      "source": [
        "print(X_vectors_bow.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(20491, 1000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HE5-qXM06P3O",
        "outputId": "a9d20035-1980-4f1b-827a-3bfe2773187f"
      },
      "source": [
        "X_vectors_bow"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.04794633, ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.05488213, ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       ...,\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.03678559, 0.        , ..., 0.05517839, 0.        ,\n",
              "        0.0183928 ],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyLWz5mXhoTp"
      },
      "source": [
        "###Normalizer L2 Euclidiana"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEMR6gtXhfwi"
      },
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "normalizer = preprocessing.Normalizer()\n",
        "X_vectors_bow = normalizer.fit_transform(X_vectors_bow, 'l2')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVrHdY9xjjRa",
        "outputId": "07845b15-fd6d-4643-a209-05df26326a79"
      },
      "source": [
        "print(X_vectors_bow.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(20491, 1000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhdZ6iCKjmFu",
        "outputId": "caa4cc8c-5890-4d19-81f7-24e8b9cb9a6a"
      },
      "source": [
        "X_vectors_bow"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.04794633, ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.05488213, ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       ...,\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.03678559, 0.        , ..., 0.05517839, 0.        ,\n",
              "        0.0183928 ],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9g6P7vEh8Zk"
      },
      "source": [
        "###SMOTE Technique: Classes are imbalanced"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aB7MHPzXGAxx",
        "outputId": "fbada2da-802d-4389-b9e7-a971759929cb"
      },
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "# transform the dataset SMOTE to over sample and balance classes \n",
        "X_bow, y_bow = SMOTE().fit_resample(X_vectors_bow, y_bow)\n",
        "print('X shape', X_bow.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: FutureWarning:\n",
            "\n",
            "The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning:\n",
            "\n",
            "The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning:\n",
            "\n",
            "Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning:\n",
            "\n",
            "Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning:\n",
            "\n",
            "Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning:\n",
            "\n",
            "Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "X shape (45270, 1000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OYz-O-gpIlRD",
        "outputId": "41c908c9-5a26-431d-cc8d-3d851494cc4d"
      },
      "source": [
        "print('X_bow', X_bow.shape)\n",
        "print('y_bow', y_bow.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_bow (45270, 1000)\n",
            "y_bow (45270,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGMAJZdsI2ce"
      },
      "source": [
        "#Train_test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qW4BECZGJA1q"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train_bow, X_test_bow, y_train_bow, y_test_bow = train_test_split(X_bow, y_bow, test_size = 0.2 , random_state = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrAkGWHfJfWM",
        "outputId": "cf2aef09-21c5-4756-86e2-fc218236c817"
      },
      "source": [
        "print('X_train_bow', X_train_bow.shape)\n",
        "print('X_test_bow', X_test_bow.shape)\n",
        "print('Y_train_bow', y_train_bow.shape)\n",
        "print('Y_test_bow', y_test_bow.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train_bow (36216, 1000)\n",
            "X_test_bow (9054, 1000)\n",
            "Y_train_bow (36216,)\n",
            "Y_test_bow (9054,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d16yhnnrkQo9"
      },
      "source": [
        "from numpy import savetxt\n",
        "\n",
        "savetxt('/content/drive/MyDrive/Tesis_Maestria/Experiment_BOW_L1_L2_QMKDC_QMKDCSGD/Dataset/X_train.csv', X_train_bow, delimiter=',')\n",
        "savetxt('/content/drive/MyDrive/Tesis_Maestria/Experiment_BOW_L1_L2_QMKDC_QMKDCSGD/Dataset/X_test.csv', X_test_bow, delimiter=',')\n",
        "savetxt('/content/drive/MyDrive/Tesis_Maestria/Experiment_BOW_L1_L2_QMKDC_QMKDCSGD/Dataset/y_train.csv', y_train_bow, delimiter=',')\n",
        "savetxt('/content/drive/MyDrive/Tesis_Maestria/Experiment_BOW_L1_L2_QMKDC_QMKDCSGD/Dataset/y_test.csv', y_test_bow, delimiter=',')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mzgNQ5pkp9u"
      },
      "source": [
        "#Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhkPCGWnktuU"
      },
      "source": [
        "def evaluation_metric(y_test, y_hat, model_name):\n",
        "    \n",
        "    accuracy = accuracy_score(y_hat, y_test)\n",
        "    print(\"Model: \", model_name)\n",
        "    print(\"\\nAccuracy: \", accuracy)\n",
        "    print(classification_report(y_hat, y_test))\n",
        "    \n",
        "    plt.figure(figsize=(10,6))\n",
        "    sns.heatmap(confusion_matrix(y_hat, y_test), annot=True, fmt=\".2f\")\n",
        "    plt.show()\n",
        "    return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-PEdsPhkiTi"
      },
      "source": [
        "#Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUGfrVxCkblp"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier , GradientBoostingClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report,confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ww6phLY-qJLl"
      },
      "source": [
        "###XGBoost Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uztcyqVCk1J2"
      },
      "source": [
        "#xgb_model_w2v = XGBClassifier(max_depth=10,random_state=1,learning_rate=0.05,seed=1)\n",
        "xgb_model = XGBClassifier(silent = True, booster = 'gbtree', learning_rate = 0.1, n_estimators = 1000, max_depth = 5, min_child_weigth = 1, gamma = 0, subsample = 0.8, colsample_bytree= 0.8, objective = 'binary:logistic', nthread = 4, scale_pos_weight = 1, seed = 27)\n",
        "xgb_model.fit(X_train_bow, y_train_bow)\n",
        "y_pred = xgb_model.predict(X_test_bow)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "id": "Jv00fsHCk37T",
        "outputId": "93f33e1c-d8a4-4ebb-bd6c-3a8026f23150"
      },
      "source": [
        "xgb_accuracy = evaluation_metric(y_pred, y_test_bow, \"XGB Classifier\")\n",
        "f1_xgb = f1_score(y_test_bow, y_pred, average='micro')\n",
        "f1_xgb = round(f1_xgb, 2)\n",
        "f1_xgb"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model:  XGB Classifier\n",
            "\n",
            "Accuracy:  0.8481334216920698\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.98      0.97      1765\n",
            "           1       0.94      0.93      0.93      1822\n",
            "           2       0.90      0.88      0.89      1830\n",
            "           3       0.72      0.67      0.69      1811\n",
            "           4       0.72      0.79      0.75      1826\n",
            "\n",
            "    accuracy                           0.85      9054\n",
            "   macro avg       0.85      0.85      0.85      9054\n",
            "weighted avg       0.85      0.85      0.85      9054\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x432 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.85"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYP-INeCqN36"
      },
      "source": [
        "###LGBM CLassifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tur-kVP1p4dX"
      },
      "source": [
        "#lgb_model_W2V = LGBMClassifier()\n",
        "lgb_model = LGBMClassifier(objective = 'multiclass', boosting = 'gbdt', learning_rate = 0.05, max_depth = 8, num_leaves =80, n_estimators = 400, bagging_fraction = 0.8, feature_fraction = 0.9)\n",
        "lgb_model.fit(X_train_bow, y_train_bow)\n",
        "lgb_preds = lgb_model.predict(X_test_bow)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "id": "uUBkGYt9p7Yx",
        "outputId": "dde52e5e-fea3-4cae-85af-cfa3327720bd"
      },
      "source": [
        "lgb_accuracy = evaluation_metric(lgb_preds, y_test_bow, \"LGBM Classifier\")\n",
        "f1_lgb = f1_score(y_test_bow, lgb_preds, average='micro')\n",
        "f1_lgb = round(f1_lgb,2)\n",
        "f1_lgb"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model:  LGBM Classifier\n",
            "\n",
            "Accuracy:  0.8268168765186658\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.97      0.96      1765\n",
            "           1       0.92      0.90      0.91      1822\n",
            "           2       0.88      0.86      0.87      1830\n",
            "           3       0.68      0.64      0.66      1811\n",
            "           4       0.71      0.77      0.74      1826\n",
            "\n",
            "    accuracy                           0.83      9054\n",
            "   macro avg       0.83      0.83      0.83      9054\n",
            "weighted avg       0.83      0.83      0.83      9054\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x432 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.83"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhdXpTBaqRbb"
      },
      "source": [
        "###Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6jf6rkaqaKS"
      },
      "source": [
        "rf_model = RandomForestClassifier()\n",
        "rf_model.fit(X_train_bow, y_train_bow)\n",
        "rf_preds = rf_model.predict(X_test_bow)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "id": "5nmDYgu3qcKr",
        "outputId": "99990231-b71f-4e83-9ae9-a554e3fbb7ad"
      },
      "source": [
        "rf_accuracy = evaluation_metric(rf_preds, y_test_bow, \"RF Classifier\")\n",
        "f1_rf = f1_score(y_test_bow, rf_preds, average='micro')\n",
        "f1_rf = round(f1_rf, 2)\n",
        "f1_rf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model:  RF Classifier\n",
            "\n",
            "Accuracy:  0.8363154406891982\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.97      0.96      1765\n",
            "           1       0.95      0.92      0.94      1822\n",
            "           2       0.90      0.89      0.90      1830\n",
            "           3       0.71      0.63      0.66      1811\n",
            "           4       0.68      0.77      0.72      1826\n",
            "\n",
            "    accuracy                           0.84      9054\n",
            "   macro avg       0.84      0.84      0.84      9054\n",
            "weighted avg       0.84      0.84      0.84      9054\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x432 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.84"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZt8RZpLqelM"
      },
      "source": [
        "###Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NmmbEYeGqmX1",
        "outputId": "0affe1db-bcf4-4670-e7de-9b48716d45c3"
      },
      "source": [
        "lr_model = LogisticRegression()\n",
        "lr_model.fit(X_train_bow, y_train_bow)\n",
        "lr_preds = lr_model.predict(X_test_bow)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning:\n",
            "\n",
            "lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "id": "s9w8P6aoqsvP",
        "outputId": "bde651b2-e942-4519-d232-83d66099beef"
      },
      "source": [
        "lr_accuracy = evaluation_metric(lr_preds, y_test_bow, \"LR Classifier\")\n",
        "f1_lr = f1_score(y_test_bow, lr_preds, average='micro')\n",
        "f1_lr = round(f1_lr, 2)\n",
        "f1_lr"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model:  LR Classifier\n",
            "\n",
            "Accuracy:  0.6954937044400266\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.87      0.84      1765\n",
            "           1       0.69      0.69      0.69      1822\n",
            "           2       0.67      0.66      0.67      1830\n",
            "           3       0.57      0.52      0.55      1811\n",
            "           4       0.71      0.74      0.72      1826\n",
            "\n",
            "    accuracy                           0.70      9054\n",
            "   macro avg       0.69      0.70      0.69      9054\n",
            "weighted avg       0.69      0.70      0.69      9054\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x432 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPA_S-_PCLhg"
      },
      "source": [
        "#QMKDC - QMKDCSGD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgGOLcfLBzB_"
      },
      "source": [
        "from numpy import loadtxt\n",
        "\n",
        "X_train = loadtxt('/content/drive/MyDrive/Tesis_Maestria/Experiment_BOW_L1_L2_QMKDC_QMKDCSGD/Dataset/X_train.csv', delimiter=',')\n",
        "X_test = loadtxt('/content/drive/MyDrive/Tesis_Maestria/Experiment_BOW_L1_L2_QMKDC_QMKDCSGD/Dataset/X_test.csv', delimiter=',')\n",
        "y_train = loadtxt('/content/drive/MyDrive/Tesis_Maestria/Experiment_BOW_L1_L2_QMKDC_QMKDCSGD/Dataset/y_train.csv', delimiter=',')\n",
        "y_test = loadtxt('/content/drive/MyDrive/Tesis_Maestria/Experiment_BOW_L1_L2_QMKDC_QMKDCSGD/Dataset/y_test.csv', delimiter=',')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdAmcFKVCzox",
        "outputId": "c01bab2b-3d69-48e5-be96-700507b84400"
      },
      "source": [
        "print('X_train', X_train.shape)\n",
        "print('X_test', X_test.shape)\n",
        "print('Y_train', y_train.shape)\n",
        "print('Y_test', y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train (36216, 1000)\n",
            "X_test (9054, 1000)\n",
            "Y_train (36216,)\n",
            "Y_test (9054,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RvMgxlJ_Dl5W",
        "outputId": "54a51453-ca31-43f8-f19a-d99e37603a23"
      },
      "source": [
        "print('X_train', X_train.dtype)\n",
        "print('X_test', X_test.dtype)\n",
        "print('Y_train', y_train.dtype)\n",
        "print('Y_test', y_test.dtype)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train float64\n",
            "X_test float64\n",
            "Y_train float64\n",
            "Y_test float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JY_0bYs-_o4V"
      },
      "source": [
        "##QMKDClassifier BOW"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpQ4zA9eAD4f"
      },
      "source": [
        "### Clone QMC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojA2RwDYBx6y",
        "outputId": "01ce326f-ffda-475a-c98b-3cf4364121d1"
      },
      "source": [
        "# Install qmc if running in Google Colab\n",
        "\n",
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    !rm -R qmc qmc1\n",
        "    !git clone https://github.com/fagonzalezo/qmc.git\n",
        "    !mv qmc qmc1\n",
        "    !mv qmc1/qmc .\n",
        "else:\n",
        "    import sys\n",
        "    sys.path.insert(0, \"../\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'qmc'...\n",
            "remote: Enumerating objects: 215, done.\u001b[K\n",
            "remote: Counting objects: 100% (215/215), done.\u001b[K\n",
            "remote: Compressing objects: 100% (151/151), done.\u001b[K\n",
            "remote: Total 215 (delta 94), reused 162 (delta 62), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (215/215), 16.93 MiB | 14.92 MiB/s, done.\n",
            "Resolving deltas: 100% (94/94), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmJG1PvLDRcO"
      },
      "source": [
        "###Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0QdOnYeCmBh"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import make_blobs, make_moons, make_circles\n",
        "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import qmc.tf.layers as layers\n",
        "import qmc.tf.models as models\n",
        "from time import time\n",
        "from time import time\n",
        "import numpy as np\n",
        "import gc\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.figsize'] = (8,5)\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import layers as layers2\n",
        "from tensorflow.keras import Model as ModelTF"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4G3-MriEJXa"
      },
      "source": [
        "###Conversion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bF_3oD30Dcfz"
      },
      "source": [
        "y_train = y_train.astype('int64')\n",
        "y_test = y_test.astype('int64')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8corVyD03KPY"
      },
      "source": [
        "input_dim = X_train.shape[1]\n",
        "num_classes = np.unique(y_train).shape[0]\n",
        "gamma = 0.2\n",
        "component_dim = 1000\n",
        "random_state=0\n",
        "num_eig= 500\n",
        "batch_size= 256\n",
        "epochs = 200\n",
        "lr = 0.005"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdr_oY88EMuW"
      },
      "source": [
        "###QMKDClassifier "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbKkVETk3pvu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6Ir1-AruG6k",
        "outputId": "ce5ddd1a-a047-4c68-e43f-58a2eef756f6"
      },
      "source": [
        "fm_x = layers.QFeatureMapRFF(input_dim=1000, dim=1000, gamma=0.2, random_state=None)\n",
        "qmkdc = models.QMKDClassifier(fm_x=fm_x, dim_x=1000 , num_classes=5)\n",
        "qmkdc.compile()\n",
        "qmkdc.fit(X_train, y_train, epochs=1)\n",
        "out = qmkdc.predict(X_test)\n",
        "y_pred = np.argmax(out, axis = 1).reshape(np.argmax(out, axis = 1).shape[0],1)\n",
        "Result = accuracy_score(y_test, y_pred)\n",
        "Acc.append(Result)\n",
        "\n",
        "print('Accuracy_score', Acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1132/1132 [==============================] - 10s 9ms/step\n",
            "Accuracy_score [0.20532361387232162, 0.20952065385464988, 0.21868787276341947, 0.2675060746631323, 0.22884912745747735, 0.22111773801634638, 0.21957146012812018, 0.20797437596642368, 0.20918930859288712, 0.2132759001546278, 0.2791031588248288, 0.23448199690744423, 0.23205213165451735, 0.22796554009277667, 0.2047713717693837, 0.20587585597525956, 0.21614755908990502, 0.2784404683013033, 0.256129887342611, 0.2468522200132538, 0.24431190633973934, 0.2132759001546278, 0.21912966644576984, 0.22641926220455047, 0.2721449083278109, 0.25745526838966204, 0.25955378838082616, 0.2514910536779324, 0.20808482438701126, 0.20952065385464988, 0.21570576540755468, 0.2914733819306384, 0.28948531036006186, 0.28407333775127014, 0.2772255356748399, 0.2078639275458361, 0.21051468963993814, 0.21714159487519327, 0.3080406450187762, 0.30837199028053897, 0.28120167881599295, 0.2852882703777336, 0.20742213386348576, 0.2094102054340623, 0.2216699801192843, 0.31157499447757897, 0.3045062955599735, 0.285177821957146, 0.2829688535453943, 0.3045062955599735, 0.2798762977689419, 0.334216920698034, 0.25270598630439584, 0.3351005080627347]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2N7AE0PCCs67",
        "outputId": "eefa5434-a9bc-4720-acc7-027493fa9a58"
      },
      "source": [
        "dimv = [50, 100, 150, 200, 400, 800, 1000]\n",
        "gama = [0.00001, 0.0001, 0.001, 0.1, 1, 10, 100]\n",
        "Acc = [] \n",
        "\n",
        "for d in dimv:\n",
        "  for g in gama:\n",
        "    fm_x = layers.QFeatureMapRFF(input_dim=1000, dim=d, gamma=g, random_state=17)\n",
        "    qmkdc = models.QMKDClassifier(fm_x=fm_x, dim_x=d , num_classes=4)\n",
        "    qmkdc.compile()\n",
        "    qmkdc.fit(X_train, y_train, epochs=1)\n",
        "    out = qmkdc.predict(X_test)\n",
        "    y_pred = np.argmax(out, axis = 1).reshape(np.argmax(out, axis = 1).shape[0],1)\n",
        "    Result = accuracy_score(y_test, y_pred)\n",
        "    Acc.append(Result)\n",
        "\n",
        "print('Accuracy_score', Acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1132/1132 [==============================] - 1s 962us/step\n",
            "1132/1132 [==============================] - 1s 973us/step\n",
            "1132/1132 [==============================] - 1s 964us/step\n",
            "1132/1132 [==============================] - 1s 947us/step\n",
            "1132/1132 [==============================] - 1s 953us/step\n",
            "1132/1132 [==============================] - 1s 966us/step\n",
            "1132/1132 [==============================] - 1s 955us/step\n",
            "1132/1132 [==============================] - 1s 953us/step\n",
            "1132/1132 [==============================] - 1s 945us/step\n",
            "1132/1132 [==============================] - 1s 929us/step\n",
            "1132/1132 [==============================] - 1s 937us/step\n",
            "1132/1132 [==============================] - 1s 924us/step\n",
            "1132/1132 [==============================] - 1s 913us/step\n",
            "1132/1132 [==============================] - 1s 944us/step\n",
            "1132/1132 [==============================] - 1s 965us/step\n",
            "1132/1132 [==============================] - 1s 976us/step\n",
            "1132/1132 [==============================] - 1s 1ms/step\n",
            "1132/1132 [==============================] - 1s 1ms/step\n",
            "1132/1132 [==============================] - 1s 974us/step\n",
            "1132/1132 [==============================] - 1s 957us/step\n",
            "1132/1132 [==============================] - 1s 984us/step\n",
            "1132/1132 [==============================] - 1s 958us/step\n",
            "1132/1132 [==============================] - 1s 935us/step\n",
            "1132/1132 [==============================] - 1s 944us/step\n",
            "1132/1132 [==============================] - 1s 945us/step\n",
            "1132/1132 [==============================] - 1s 927us/step\n",
            "1132/1132 [==============================] - 1s 970us/step\n",
            "1132/1132 [==============================] - 1s 953us/step\n",
            "1132/1132 [==============================] - 2s 1ms/step\n",
            "1132/1132 [==============================] - 2s 1ms/step\n",
            "1132/1132 [==============================] - 2s 1ms/step\n",
            "1132/1132 [==============================] - 2s 1ms/step\n",
            "1132/1132 [==============================] - 2s 1ms/step\n",
            "1132/1132 [==============================] - 2s 1ms/step\n",
            "1132/1132 [==============================] - 2s 1ms/step\n",
            "1132/1132 [==============================] - 5s 5ms/step\n",
            "1132/1132 [==============================] - 5s 5ms/step\n",
            "1132/1132 [==============================] - 5s 5ms/step\n",
            "1132/1132 [==============================] - 5s 5ms/step\n",
            "1132/1132 [==============================] - 5s 5ms/step\n",
            "1132/1132 [==============================] - 5s 5ms/step\n",
            "1132/1132 [==============================] - 5s 5ms/step\n",
            "1132/1132 [==============================] - 8s 7ms/step\n",
            "1132/1132 [==============================] - 8s 7ms/step\n",
            "1132/1132 [==============================] - 8s 7ms/step\n",
            "1132/1132 [==============================] - 8s 7ms/step\n",
            "1132/1132 [==============================] - 8s 7ms/step\n",
            "1132/1132 [==============================] - 8s 7ms/step\n",
            "1132/1132 [==============================] - 8s 7ms/step\n",
            "Accuracy_score [0.20532361387232162, 0.20952065385464988, 0.21868787276341947, 0.2675060746631323, 0.22884912745747735, 0.22111773801634638, 0.21957146012812018, 0.20797437596642368, 0.20918930859288712, 0.2132759001546278, 0.2791031588248288, 0.23448199690744423, 0.23205213165451735, 0.2278550916721891, 0.2047713717693837, 0.20587585597525956, 0.21614755908990502, 0.2784404683013033, 0.256129887342611, 0.2468522200132538, 0.24431190633973934, 0.2132759001546278, 0.21912966644576984, 0.22641926220455047, 0.2721449083278109, 0.25745526838966204, 0.25955378838082616, 0.2514910536779324, 0.20808482438701126, 0.20952065385464988, 0.21570576540755468, 0.2914733819306384, 0.28948531036006186, 0.28407333775127014, 0.2772255356748399, 0.2078639275458361, 0.21051468963993814, 0.21714159487519327, 0.3080406450187762, 0.30837199028053897, 0.28120167881599295, 0.2852882703777336, 0.20742213386348576, 0.2094102054340623, 0.2216699801192843, 0.31157499447757897, 0.3045062955599735, 0.285177821957146, 0.2829688535453943]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLOynFGyxcj5",
        "outputId": "fbb114df-8c79-42d0-c692-e5652de16f97"
      },
      "source": [
        "dimv = [50, 100, 150, 200, 400, 800, 1000]\n",
        "gama = [0.01, 0.02, 0.04, 0.06, 0.08, 0.09, 0.1, 0.2, 0.3, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 1.2, 1.4, 1.6]\n",
        "Acc = [] \n",
        "\n",
        "for d in dimv:\n",
        "  for g in gama:\n",
        "    fm_x = layers.QFeatureMapRFF(input_dim=1000, dim=d, gamma=g, random_state=17)\n",
        "    qmkdc = models.QMKDClassifier(fm_x=fm_x, dim_x=d , num_classes=4)\n",
        "    qmkdc.compile()\n",
        "    qmkdc.fit(X_train, y_train, epochs=1)\n",
        "    out = qmkdc.predict(X_test)\n",
        "    y_pred = np.argmax(out, axis = 1).reshape(np.argmax(out, axis = 1).shape[0],1)\n",
        "    Result = accuracy_score(y_test, y_pred)\n",
        "    Acc.append(Result)\n",
        "\n",
        "print('Accuracy_score', Acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1132/1132 [==============================] - 1s 1ms/step\n",
            "1132/1132 [==============================] - 1s 1ms/step\n",
            "1132/1132 [==============================] - 1s 983us/step\n",
            "1132/1132 [==============================] - 1s 1ms/step\n",
            "1132/1132 [==============================] - 1s 989us/step\n",
            "1132/1132 [==============================] - 1s 975us/step\n",
            "1132/1132 [==============================] - 1s 972us/step\n",
            "1132/1132 [==============================] - 1s 994us/step\n",
            "1132/1132 [==============================] - 1s 992us/step\n",
            "1132/1132 [==============================] - 1s 982us/step\n",
            "1132/1132 [==============================] - 1s 957us/step\n",
            "1132/1132 [==============================] - 1s 976us/step\n",
            "1132/1132 [==============================] - 1s 971us/step\n",
            "1132/1132 [==============================] - 1s 996us/step\n",
            "1132/1132 [==============================] - 1s 992us/step\n",
            "1132/1132 [==============================] - 1s 998us/step\n",
            "1132/1132 [==============================] - 1s 980us/step\n",
            "1132/1132 [==============================] - 1s 1ms/step\n",
            "1132/1132 [==============================] - 1s 986us/step\n",
            "1132/1132 [==============================] - 1s 991us/step\n",
            "1132/1132 [==============================] - 1s 923us/step\n",
            "1132/1132 [==============================] - 1s 951us/step\n",
            "1132/1132 [==============================] - 1s 969us/step\n",
            "1132/1132 [==============================] - 1s 952us/step\n",
            "1132/1132 [==============================] - 1s 953us/step\n",
            "1132/1132 [==============================] - 1s 955us/step\n",
            "1132/1132 [==============================] - 1s 971us/step\n",
            "1132/1132 [==============================] - 1s 933us/step\n",
            "1132/1132 [==============================] - 1s 965us/step\n",
            "1132/1132 [==============================] - 1s 953us/step\n",
            "1132/1132 [==============================] - 1s 943us/step\n",
            "1132/1132 [==============================] - 1s 934us/step\n",
            "1132/1132 [==============================] - 1s 951us/step\n",
            "1132/1132 [==============================] - 1s 947us/step\n",
            "1132/1132 [==============================] - 1s 953us/step\n",
            "1132/1132 [==============================] - 1s 954us/step\n",
            "1132/1132 [==============================] - 1s 957us/step\n",
            "1132/1132 [==============================] - 1s 989us/step\n",
            "1132/1132 [==============================] - 1s 978us/step\n",
            "1132/1132 [==============================] - 1s 1ms/step\n",
            "1132/1132 [==============================] - 1s 972us/step\n",
            "1132/1132 [==============================] - 1s 987us/step\n",
            "1132/1132 [==============================] - 1s 995us/step\n",
            "1132/1132 [==============================] - 1s 1ms/step\n",
            "1132/1132 [==============================] - 1s 1ms/step\n",
            "1132/1132 [==============================] - 1s 981us/step\n",
            "1132/1132 [==============================] - 1s 973us/step\n",
            "1132/1132 [==============================] - 1s 987us/step\n",
            "1132/1132 [==============================] - 1s 971us/step\n",
            "1132/1132 [==============================] - 1s 977us/step\n",
            "1132/1132 [==============================] - 1s 976us/step\n",
            "1132/1132 [==============================] - 1s 990us/step\n",
            "1132/1132 [==============================] - 1s 1ms/step\n",
            "1132/1132 [==============================] - 1s 1ms/step\n",
            "1132/1132 [==============================] - 1s 993us/step\n",
            "1132/1132 [==============================] - 1s 989us/step\n",
            "1132/1132 [==============================] - 1s 976us/step\n",
            "1132/1132 [==============================] - 1s 970us/step\n",
            "1132/1132 [==============================] - 1s 974us/step\n",
            "1132/1132 [==============================] - 1s 1ms/step\n",
            "1132/1132 [==============================] - 1s 996us/step\n",
            "1132/1132 [==============================] - 1s 1ms/step\n",
            "1132/1132 [==============================] - 1s 974us/step\n",
            "1132/1132 [==============================] - 1s 965us/step\n",
            "1132/1132 [==============================] - 1s 953us/step\n",
            "1132/1132 [==============================] - 1s 956us/step\n",
            "1132/1132 [==============================] - 1s 954us/step\n",
            "1132/1132 [==============================] - 1s 961us/step\n",
            "1132/1132 [==============================] - 1s 954us/step\n",
            "1132/1132 [==============================] - 1s 971us/step\n",
            "1132/1132 [==============================] - 1s 963us/step\n",
            "1132/1132 [==============================] - 1s 969us/step\n",
            "1132/1132 [==============================] - 2s 1ms/step\n",
            "1132/1132 [==============================] - 2s 1ms/step\n",
            "1132/1132 [==============================] - 2s 1ms/step\n",
            "1132/1132 [==============================] - 2s 1ms/step\n",
            "1132/1132 [==============================] - 2s 1ms/step\n",
            "1132/1132 [==============================] - 2s 1ms/step\n",
            "1132/1132 [==============================] - 2s 1ms/step\n",
            "1132/1132 [==============================] - 2s 1ms/step\n",
            "1132/1132 [==============================] - 2s 1ms/step\n",
            "1132/1132 [==============================] - 2s 1ms/step\n",
            "1132/1132 [==============================] - 2s 1ms/step\n",
            "1132/1132 [==============================] - 2s 1ms/step\n",
            "1132/1132 [==============================] - 2s 1ms/step\n",
            "1132/1132 [==============================] - 2s 1ms/step\n",
            "1132/1132 [==============================] - 2s 1ms/step\n",
            "1132/1132 [==============================] - 2s 1ms/step\n",
            "1132/1132 [==============================] - 2s 1ms/step\n",
            "1132/1132 [==============================] - 2s 1ms/step\n",
            "1132/1132 [==============================] - 5s 5ms/step\n",
            "1132/1132 [==============================] - 5s 5ms/step\n",
            "1132/1132 [==============================] - 5s 5ms/step\n",
            "1132/1132 [==============================] - 5s 5ms/step\n",
            "1132/1132 [==============================] - 5s 5ms/step\n",
            "1132/1132 [==============================] - 5s 5ms/step\n",
            "1132/1132 [==============================] - 5s 5ms/step\n",
            "1132/1132 [==============================] - 5s 5ms/step\n",
            "1132/1132 [==============================] - 5s 5ms/step\n",
            "1132/1132 [==============================] - 5s 5ms/step\n",
            "1132/1132 [==============================] - 5s 5ms/step\n",
            "1132/1132 [==============================] - 5s 5ms/step\n",
            "1132/1132 [==============================] - 5s 5ms/step\n",
            "1132/1132 [==============================] - 5s 5ms/step\n",
            "1132/1132 [==============================] - 5s 5ms/step\n",
            "1132/1132 [==============================] - 5s 5ms/step\n",
            "1132/1132 [==============================] - 5s 5ms/step\n",
            "1132/1132 [==============================] - 5s 5ms/step\n",
            "1132/1132 [==============================] - 8s 7ms/step\n",
            "1132/1132 [==============================] - 8s 7ms/step\n",
            "1132/1132 [==============================] - 8s 7ms/step\n",
            "1132/1132 [==============================] - 8s 7ms/step\n",
            "1132/1132 [==============================] - 8s 7ms/step\n",
            "1132/1132 [==============================] - 8s 7ms/step\n",
            "1132/1132 [==============================] - 8s 7ms/step\n",
            "1132/1132 [==============================] - 8s 7ms/step\n",
            "1132/1132 [==============================] - 8s 7ms/step\n",
            "1132/1132 [==============================] - 8s 7ms/step\n",
            "1132/1132 [==============================] - 8s 7ms/step\n",
            "1132/1132 [==============================] - 8s 7ms/step\n",
            "1132/1132 [==============================] - 8s 7ms/step\n",
            "1132/1132 [==============================] - 8s 7ms/step\n",
            "1132/1132 [==============================] - 8s 7ms/step\n",
            "1132/1132 [==============================] - 8s 7ms/step\n",
            "1132/1132 [==============================] - 8s 7ms/step\n",
            "1132/1132 [==============================] - 8s 7ms/step\n",
            "Accuracy_score [0.2415506958250497, 0.25480450629555995, 0.2645239673072675, 0.26827921360724544, 0.26662248729843163, 0.26673293571901924, 0.2675060746631323, 0.2577866136514248, 0.25171195051910755, 0.23293571901921803, 0.23415065164568147, 0.2301745085045284, 0.22255356748398497, 0.22200132538104705, 0.22884912745747735, 0.22520432957808703, 0.22288491274574773, 0.22332670642809807, 0.2423238347691628, 0.256129887342611, 0.2686105588690082, 0.27634194831013914, 0.27799867461895295, 0.28009719461011706, 0.2791031588248288, 0.27026728517782195, 0.2618732052131654, 0.2538104705102717, 0.2460790810691407, 0.2499447757897062, 0.24861939474265518, 0.24088800530152418, 0.23448199690744423, 0.23669096531919592, 0.2369118621603711, 0.23746410426330902, 0.2468522200132538, 0.25590899050143584, 0.26573889993373095, 0.27545836094543846, 0.2777777777777778, 0.27899271040424123, 0.2784404683013033, 0.2804285398718798, 0.27081952728075986, 0.26662248729843163, 0.2598851336425889, 0.25579854208084823, 0.25491495471614756, 0.2506074663132317, 0.256129887342611, 0.25590899050143584, 0.25115970841616964, 0.2508283631544069, 0.23989396951623593, 0.25491495471614756, 0.27347028937486195, 0.2773359840954274, 0.2772255356748399, 0.27026728517782195, 0.2721449083278109, 0.28009719461011706, 0.280649436713055, 0.2775568809366026, 0.26629114203666887, 0.26419262204550475, 0.2645239673072675, 0.2639717252043296, 0.25745526838966204, 0.26408217362491715, 0.2535895736690965, 0.25480450629555995, 0.23746410426330902, 0.25182239893969516, 0.2686105588690082, 0.2823061630218688, 0.2884912745747736, 0.2916942787718136, 0.2914733819306384, 0.3045062955599735, 0.30483764082173626, 0.3036227081952728, 0.30030925557764526, 0.30329136293351006, 0.29235696929533905, 0.29699580296001765, 0.28948531036006186, 0.2880494808924232, 0.2803180914512923, 0.2741329798983875, 0.24409100949856416, 0.2613209631102275, 0.2772255356748399, 0.2900375524629998, 0.30538988292467417, 0.3045062955599735, 0.3080406450187762, 0.3195272807598851, 0.3322288491274575, 0.3184227965540093, 0.32317207863927544, 0.31621382814225757, 0.3183123481334217, 0.3122376850011045, 0.30837199028053897, 0.29180472719240114, 0.29489728296885354, 0.29567042191296666, 0.25115970841616964, 0.26618069361608127, 0.2846255798542081, 0.2984316324276563, 0.3085928871217142, 0.30848243870112657, 0.31157499447757897, 0.3302407775568809, 0.3343273691186216, 0.3225093881157499, 0.3281422575657168, 0.3225093881157499, 0.32118400706869893, 0.3133421692069803, 0.3045062955599735, 0.29953611663353213, 0.2960017671747294, 0.2935719019218025]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXl9V92zORKR",
        "outputId": "2664693b-b542-4044-a770-a5841d6d57c1"
      },
      "source": [
        "dimv = [200, 400, 500]\n",
        "gama = [0.2, 1, 2]\n",
        "Acc = [] \n",
        "\n",
        "for d in dimv:\n",
        "  for g in gama:\n",
        "    fm_x = layers.QFeatureMapRFF(input_dim=1000, dim=d, gamma=g, random_state=17)\n",
        "    qmkdc = models.QMKDClassifier(fm_x=fm_x, dim_x=d , num_classes=4)\n",
        "    qmkdc.compile()\n",
        "    qmkdc.fit(X_train, y_train, epochs=1)\n",
        "    out = qmkdc.predict(X_test)\n",
        "    y_pred = np.argmax(out, axis = 1).reshape(np.argmax(out, axis = 1).shape[0],1)\n",
        "    Result = accuracy_score(y_test, y_pred)\n",
        "    Acc.append(Result)\n",
        "\n",
        "print('Accuracy_score', Acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1132/1132 [==============================] - 1s 1ms/step\n",
            "1132/1132 [==============================] - 1s 1ms/step\n",
            "1132/1132 [==============================] - 1s 1ms/step\n",
            "1132/1132 [==============================] - 2s 1ms/step\n",
            "1132/1132 [==============================] - 2s 1ms/step\n",
            "1132/1132 [==============================] - 2s 1ms/step\n",
            "1132/1132 [==============================] - 2s 2ms/step\n",
            "1132/1132 [==============================] - 2s 2ms/step\n",
            "1132/1132 [==============================] - 2s 2ms/step\n",
            "Accuracy_score [0.28009719461011706, 0.25745526838966204, 0.2591119946984758, 0.3045062955599735, 0.28948531036006186, 0.2772255356748399, 0.3132317207863928, 0.2860614093218467, 0.2844046830130329]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OaOmJoEkOyD5",
        "outputId": "164c0782-c9ef-48b3-c0c8-71db409ad31c"
      },
      "source": [
        "dimv = [500]\n",
        "gama = [0.0002, 0.002, 0.02, 0.2]\n",
        "Acc = [] \n",
        "\n",
        "for d in dimv:\n",
        "  for g in gama:\n",
        "    fm_x = layers.QFeatureMapRFF(input_dim=1000, dim=d, gamma=g, random_state=17)\n",
        "    qmkdc = models.QMKDClassifier(fm_x=fm_x, dim_x=d , num_classes=4)\n",
        "    qmkdc.compile()\n",
        "    qmkdc.fit(X_train, y_train, epochs=1)\n",
        "    out = qmkdc.predict(X_test)\n",
        "    y_pred = np.argmax(out, axis = 1).reshape(np.argmax(out, axis = 1).shape[0],1)\n",
        "    Result = accuracy_score(y_test, y_pred)\n",
        "    Acc.append(Result)\n",
        "\n",
        "print('Accuracy_score', Acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1132/1132 [==============================] - 2s 2ms/step\n",
            "1132/1132 [==============================] - 2s 2ms/step\n",
            "1132/1132 [==============================] - 2s 2ms/step\n",
            "1132/1132 [==============================] - 2s 2ms/step\n",
            "Accuracy_score [0.20764303070466092, 0.21681024961343054, 0.24674177159266622, 0.3132317207863928]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdLpUiMjPZnj",
        "outputId": "4f09bade-3fe6-4c55-e199-f3bc4680411f"
      },
      "source": [
        "dimv = [500]\n",
        "gama = [0.1, 0.2, 0.3, 0.4]\n",
        "Acc = [] \n",
        "\n",
        "for d in dimv:\n",
        "  for g in gama:\n",
        "    fm_x = layers.QFeatureMapRFF(input_dim=1000, dim=d, gamma=g, random_state=17)\n",
        "    qmkdc = models.QMKDClassifier(fm_x=fm_x, dim_x=d , num_classes=4)\n",
        "    qmkdc.compile()\n",
        "    qmkdc.fit(X_train, y_train, epochs=1)\n",
        "    out = qmkdc.predict(X_test)\n",
        "    y_pred = np.argmax(out, axis = 1).reshape(np.argmax(out, axis = 1).shape[0],1)\n",
        "    Result = accuracy_score(y_test, y_pred)\n",
        "    Acc.append(Result)\n",
        "\n",
        "print('Accuracy_score', Acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1132/1132 [==============================] - 2s 2ms/step\n",
            "1132/1132 [==============================] - 2s 2ms/step\n",
            "1132/1132 [==============================] - 2s 2ms/step\n",
            "1132/1132 [==============================] - 2s 2ms/step\n",
            "Accuracy_score [0.2916942787718136, 0.3132317207863928, 0.31809145129224653, 0.31256903026286725]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjYwCAEh_zRK",
        "outputId": "9747faf3-ef4b-4648-e3e1-b79768aae3ac"
      },
      "source": [
        "dimv = [1000, 2000, 3000]\n",
        "gama = [0.01, 0.02, 0.04, 0.06, 0.08, 0.09, 0.1, 0.2, 0.3, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 1.2, 1.4, 1.6]\n",
        "Acc = [] \n",
        "\n",
        "for d in dimv:\n",
        "  for g in gama:\n",
        "    fm_x = layers.QFeatureMapRFF(input_dim=1000, dim=d, gamma=g, random_state=17)\n",
        "    qmkdc = models.QMKDClassifier(fm_x=fm_x, dim_x=d , num_classes=4)\n",
        "    qmkdc.compile()\n",
        "    qmkdc.fit(X_train, y_train, epochs=1)\n",
        "    out = qmkdc.predict(X_test)\n",
        "    y_pred = np.argmax(out, axis = 1).reshape(np.argmax(out, axis = 1).shape[0],1)\n",
        "    Result = accuracy_score(y_test, y_pred)\n",
        "    Acc.append(Result)\n",
        "\n",
        "print('Accuracy_score', Acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1132/1132 [==============================] - 8s 7ms/step\n",
            "1132/1132 [==============================] - 8s 7ms/step\n",
            "1132/1132 [==============================] - 8s 7ms/step\n",
            "1132/1132 [==============================] - 8s 7ms/step\n",
            "1132/1132 [==============================] - 8s 7ms/step\n",
            "1132/1132 [==============================] - 8s 7ms/step\n",
            "1132/1132 [==============================] - 8s 7ms/step\n",
            "1132/1132 [==============================] - 8s 7ms/step\n",
            "1132/1132 [==============================] - 8s 7ms/step\n",
            "1132/1132 [==============================] - 8s 7ms/step\n",
            "1132/1132 [==============================] - 8s 7ms/step\n",
            "1132/1132 [==============================] - 8s 7ms/step\n",
            "1132/1132 [==============================] - 8s 7ms/step\n",
            "1132/1132 [==============================] - 8s 7ms/step\n",
            "1132/1132 [==============================] - 8s 7ms/step\n",
            "1132/1132 [==============================] - 8s 7ms/step\n",
            "1132/1132 [==============================] - 8s 7ms/step\n",
            "1132/1132 [==============================] - 8s 7ms/step\n",
            "1132/1132 [==============================] - 34s 30ms/step\n",
            "1132/1132 [==============================] - 34s 30ms/step\n",
            "1132/1132 [==============================] - 34s 30ms/step\n",
            "1132/1132 [==============================] - 34s 30ms/step\n",
            "1132/1132 [==============================] - 34s 30ms/step\n",
            "1132/1132 [==============================] - 34s 30ms/step\n",
            "1132/1132 [==============================] - 34s 30ms/step\n",
            "1132/1132 [==============================] - 34s 30ms/step\n",
            "1132/1132 [==============================] - 34s 30ms/step\n",
            "1132/1132 [==============================] - 34s 30ms/step\n",
            "1132/1132 [==============================] - 34s 30ms/step\n",
            "1132/1132 [==============================] - 34s 30ms/step\n",
            "1132/1132 [==============================] - 34s 30ms/step\n",
            "1132/1132 [==============================] - 34s 30ms/step\n",
            "1132/1132 [==============================] - 34s 30ms/step\n",
            "1132/1132 [==============================] - 34s 30ms/step\n",
            "1132/1132 [==============================] - 34s 30ms/step\n",
            "1132/1132 [==============================] - 34s 30ms/step\n",
            "1132/1132 [==============================] - 76s 67ms/step\n",
            "1132/1132 [==============================] - 76s 67ms/step\n",
            "1132/1132 [==============================] - 76s 67ms/step\n",
            "1132/1132 [==============================] - 76s 67ms/step\n",
            "1132/1132 [==============================] - 76s 67ms/step\n",
            "1132/1132 [==============================] - 76s 67ms/step\n",
            "1132/1132 [==============================] - 76s 67ms/step\n",
            "1132/1132 [==============================] - 76s 67ms/step\n",
            "1132/1132 [==============================] - 76s 67ms/step\n",
            "1132/1132 [==============================] - 76s 67ms/step\n",
            "1132/1132 [==============================] - 76s 67ms/step\n",
            "1132/1132 [==============================] - 76s 67ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7U2sbE0KFg3F"
      },
      "source": [
        "###QMKDCSGD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNKdYUi1NbgL"
      },
      "source": [
        "### QMKDCSG on differents set of lr"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XvOSrv7WYgiX",
        "outputId": "6ebc2941-9048-49fc-b97e-45d869a780af"
      },
      "source": [
        "#Best lr set_lr_best = [0.0005, 0.0025, 0.005]\r\n",
        "set_lr_a = [0.005]\r\n",
        "set_lr_b = [0.00005, 0.0005, 0.5, 10]\r\n",
        "set_lr_c = [0.000005, 0.00005, 1, 10]\r\n",
        "set_lr_d = [0.0005, 0.005, 0.05, 0.5, 1, 10]\r\n",
        "\r\n",
        "for lr in set_lr_a:\r\n",
        "  exp_time = time()\r\n",
        "  qmkdc1_dig = models.QMKDClassifierSGD(input_dim=1000, dim_x=component_dim, num_classes=num_classes, num_eig=num_eig, gamma=0.2, random_state=None)\r\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\r\n",
        "  qmkdc1_dig.compile(optimizer, loss=tf.keras.losses.CategoricalCrossentropy(), metrics=['accuracy'])\r\n",
        "  y_train_bin = tf.reshape(tf.keras.backend.one_hot(y_train, num_classes), (-1,num_classes))\r\n",
        "  y_test_bin = tf.reshape(tf.keras.backend.one_hot(y_test, num_classes), (-1,num_classes))\r\n",
        "  qmkdc1_dig.fit(X_train, y_train_bin, epochs=100, verbose=2, validation_data=(X_test, y_test_bin))\r\n",
        "  out = qmkdc1_dig.predict(X_test)\r\n",
        "  print(f'lr = {lr}')\r\n",
        "  print(f'acc = {accuracy_score(y_test, np.argmax(out, axis=1))}')\r\n",
        "  print(\"--------time QMKDCsgd---------------\")\r\n",
        "  print(time() - exp_time)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "1132/1132 - 8s - loss: 0.8779 - accuracy: 0.6452 - val_loss: 0.7417 - val_accuracy: 0.7143\n",
            "Epoch 2/100\n",
            "1132/1132 - 8s - loss: 0.5889 - accuracy: 0.7827 - val_loss: 0.5392 - val_accuracy: 0.8049\n",
            "Epoch 3/100\n",
            "1132/1132 - 8s - loss: 0.3664 - accuracy: 0.8793 - val_loss: 0.4329 - val_accuracy: 0.8480\n",
            "Epoch 4/100\n",
            "1132/1132 - 8s - loss: 0.2365 - accuracy: 0.9319 - val_loss: 0.4071 - val_accuracy: 0.8611\n",
            "Epoch 5/100\n",
            "1132/1132 - 8s - loss: 0.1508 - accuracy: 0.9678 - val_loss: 0.3931 - val_accuracy: 0.8772\n",
            "Epoch 6/100\n",
            "1132/1132 - 8s - loss: 0.0927 - accuracy: 0.9884 - val_loss: 0.4146 - val_accuracy: 0.8772\n",
            "Epoch 7/100\n",
            "1132/1132 - 7s - loss: 0.0562 - accuracy: 0.9974 - val_loss: 0.4173 - val_accuracy: 0.8781\n",
            "Epoch 8/100\n",
            "1132/1132 - 7s - loss: 0.0375 - accuracy: 0.9995 - val_loss: 0.4201 - val_accuracy: 0.8819\n",
            "Epoch 9/100\n",
            "1132/1132 - 7s - loss: 0.0279 - accuracy: 0.9999 - val_loss: 0.4139 - val_accuracy: 0.8824\n",
            "Epoch 10/100\n",
            "1132/1132 - 7s - loss: 0.0229 - accuracy: 0.9999 - val_loss: 0.4147 - val_accuracy: 0.8810\n",
            "Epoch 11/100\n",
            "1132/1132 - 7s - loss: 0.0197 - accuracy: 1.0000 - val_loss: 0.4110 - val_accuracy: 0.8830\n",
            "Epoch 12/100\n",
            "1132/1132 - 7s - loss: 0.0175 - accuracy: 1.0000 - val_loss: 0.4084 - val_accuracy: 0.8845\n",
            "Epoch 13/100\n",
            "1132/1132 - 7s - loss: 0.0153 - accuracy: 1.0000 - val_loss: 0.4064 - val_accuracy: 0.8833\n",
            "Epoch 14/100\n",
            "1132/1132 - 7s - loss: 0.0140 - accuracy: 1.0000 - val_loss: 0.3956 - val_accuracy: 0.8828\n",
            "Epoch 15/100\n",
            "1132/1132 - 7s - loss: 0.0126 - accuracy: 1.0000 - val_loss: 0.4014 - val_accuracy: 0.8848\n",
            "Epoch 16/100\n",
            "1132/1132 - 7s - loss: 0.0119 - accuracy: 1.0000 - val_loss: 0.4022 - val_accuracy: 0.8827\n",
            "Epoch 17/100\n",
            "1132/1132 - 7s - loss: 0.0112 - accuracy: 1.0000 - val_loss: 0.4019 - val_accuracy: 0.8834\n",
            "Epoch 18/100\n",
            "1132/1132 - 7s - loss: 0.0097 - accuracy: 1.0000 - val_loss: 0.3940 - val_accuracy: 0.8844\n",
            "Epoch 19/100\n",
            "1132/1132 - 7s - loss: 0.0091 - accuracy: 1.0000 - val_loss: 0.3955 - val_accuracy: 0.8856\n",
            "Epoch 20/100\n",
            "1132/1132 - 7s - loss: 0.0087 - accuracy: 1.0000 - val_loss: 0.3900 - val_accuracy: 0.8833\n",
            "Epoch 21/100\n",
            "1132/1132 - 8s - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.3783 - val_accuracy: 0.8856\n",
            "Epoch 22/100\n",
            "1132/1132 - 7s - loss: 0.0080 - accuracy: 1.0000 - val_loss: 0.3866 - val_accuracy: 0.8859\n",
            "Epoch 23/100\n",
            "1132/1132 - 7s - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.3862 - val_accuracy: 0.8877\n",
            "Epoch 24/100\n",
            "1132/1132 - 7s - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.3911 - val_accuracy: 0.8845\n",
            "Epoch 25/100\n",
            "1132/1132 - 7s - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.3868 - val_accuracy: 0.8873\n",
            "Epoch 26/100\n",
            "1132/1132 - 7s - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.3744 - val_accuracy: 0.8875\n",
            "Epoch 27/100\n",
            "1132/1132 - 8s - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.3814 - val_accuracy: 0.8867\n",
            "Epoch 28/100\n",
            "1132/1132 - 8s - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.3746 - val_accuracy: 0.8887\n",
            "Epoch 29/100\n",
            "1132/1132 - 7s - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.3800 - val_accuracy: 0.8870\n",
            "Epoch 30/100\n",
            "1132/1132 - 7s - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.3735 - val_accuracy: 0.8869\n",
            "Epoch 31/100\n",
            "1132/1132 - 7s - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.3704 - val_accuracy: 0.8883\n",
            "Epoch 32/100\n",
            "1132/1132 - 7s - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.3702 - val_accuracy: 0.8883\n",
            "Epoch 33/100\n",
            "1132/1132 - 7s - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.3747 - val_accuracy: 0.8870\n",
            "Epoch 34/100\n",
            "1132/1132 - 8s - loss: 0.0050 - accuracy: 1.0000 - val_loss: 0.3694 - val_accuracy: 0.8881\n",
            "Epoch 35/100\n",
            "1132/1132 - 8s - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.3637 - val_accuracy: 0.8888\n",
            "Epoch 36/100\n",
            "1132/1132 - 7s - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.3686 - val_accuracy: 0.8886\n",
            "Epoch 37/100\n",
            "1132/1132 - 7s - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.3664 - val_accuracy: 0.8887\n",
            "Epoch 38/100\n",
            "1132/1132 - 7s - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.3659 - val_accuracy: 0.8893\n",
            "Epoch 39/100\n",
            "1132/1132 - 7s - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.3673 - val_accuracy: 0.8883\n",
            "Epoch 40/100\n",
            "1132/1132 - 7s - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.3643 - val_accuracy: 0.8889\n",
            "Epoch 41/100\n",
            "1132/1132 - 7s - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.3625 - val_accuracy: 0.8888\n",
            "Epoch 42/100\n",
            "1132/1132 - 7s - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.3623 - val_accuracy: 0.8886\n",
            "Epoch 43/100\n",
            "1132/1132 - 7s - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.3596 - val_accuracy: 0.8901\n",
            "Epoch 44/100\n",
            "1132/1132 - 7s - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.3630 - val_accuracy: 0.8902\n",
            "Epoch 45/100\n",
            "1132/1132 - 8s - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.3565 - val_accuracy: 0.8875\n",
            "Epoch 46/100\n",
            "1132/1132 - 7s - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.3566 - val_accuracy: 0.8894\n",
            "Epoch 47/100\n",
            "1132/1132 - 7s - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.3604 - val_accuracy: 0.8880\n",
            "Epoch 48/100\n",
            "1132/1132 - 7s - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.3595 - val_accuracy: 0.8888\n",
            "Epoch 49/100\n",
            "1132/1132 - 7s - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.3641 - val_accuracy: 0.8881\n",
            "Epoch 50/100\n",
            "1132/1132 - 7s - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.3608 - val_accuracy: 0.8884\n",
            "Epoch 51/100\n",
            "1132/1132 - 7s - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.3572 - val_accuracy: 0.8887\n",
            "Epoch 52/100\n",
            "1132/1132 - 7s - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.3562 - val_accuracy: 0.8903\n",
            "Epoch 53/100\n",
            "1132/1132 - 7s - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.3564 - val_accuracy: 0.8878\n",
            "Epoch 54/100\n",
            "1132/1132 - 7s - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.3531 - val_accuracy: 0.8898\n",
            "Epoch 55/100\n",
            "1132/1132 - 8s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.3547 - val_accuracy: 0.8889\n",
            "Epoch 56/100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-360da26309d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0my_train_bin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0my_test_bin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m   \u001b[0mqmkdc1_dig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_bin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_bin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m   \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqmkdc1_dig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'lr = {lr}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1131\u001b[0m               \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m               \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1133\u001b[0;31m               return_dict=True)\n\u001b[0m\u001b[1;32m   1134\u001b[0m           \u001b[0mval_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[1;32m   1382\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1383\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_test_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    460\u001b[0m     \"\"\"\n\u001b[1;32m    461\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_test_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTEST\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    287\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unrecognized hook: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    302\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;34m\"\"\"Helper function for `on_*_batch_end` methods.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m     \u001b[0mhook_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'on_{mode}_batch_end'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCZNQudraV8m",
        "outputId": "550ad423-076f-4655-b212-f933c2deb1e9"
      },
      "source": [
        "#Best lr set_lr_best = [0.0005, 0.0025, 0.005]\r\n",
        "set_lr_a = [0.005]\r\n",
        "set_lr_b = [0.00005, 0.0005, 0.5, 10]\r\n",
        "set_lr_c = [0.000005, 0.00005, 1, 10]\r\n",
        "set_lr_d = [0.0005, 0.005, 0.05, 0.5, 1, 10]\r\n",
        "\r\n",
        "for lr in set_lr_a:\r\n",
        "  exp_time = time()\r\n",
        "  qmkdc1_dig = models.QMKDClassifierSGD(input_dim=1000, dim_x=component_dim, num_classes=num_classes, num_eig=num_eig, gamma=2**-9, random_state=None)\r\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\r\n",
        "  qmkdc1_dig.compile(optimizer, loss=tf.keras.losses.CategoricalCrossentropy(), metrics=['accuracy'])\r\n",
        "  y_train_bin = tf.reshape(tf.keras.backend.one_hot(y_train, num_classes), (-1,num_classes))\r\n",
        "  y_test_bin = tf.reshape(tf.keras.backend.one_hot(y_test, num_classes), (-1,num_classes))\r\n",
        "  qmkdc1_dig.fit(X_train, y_train_bin, epochs=100, verbose=2, validation_data=(X_test, y_test_bin))\r\n",
        "  out = qmkdc1_dig.predict(X_test)\r\n",
        "  print(f'lr = {lr}')\r\n",
        "  print(f'acc = {accuracy_score(y_test, np.argmax(out, axis=1))}')\r\n",
        "  print(\"--------time QMKDCsgd---------------\")\r\n",
        "  print(time() - exp_time)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "1132/1132 - 8s - loss: 0.9207 - accuracy: 0.6257 - val_loss: 0.8338 - val_accuracy: 0.6795\n",
            "Epoch 2/100\n",
            "1132/1132 - 7s - loss: 0.7668 - accuracy: 0.7008 - val_loss: 0.7697 - val_accuracy: 0.6933\n",
            "Epoch 3/100\n",
            "1132/1132 - 8s - loss: 0.6875 - accuracy: 0.7273 - val_loss: 0.6922 - val_accuracy: 0.7266\n",
            "Epoch 4/100\n",
            "1132/1132 - 8s - loss: 0.5902 - accuracy: 0.7655 - val_loss: 0.6233 - val_accuracy: 0.7569\n",
            "Epoch 5/100\n",
            "1132/1132 - 7s - loss: 0.5006 - accuracy: 0.8077 - val_loss: 0.5544 - val_accuracy: 0.7926\n",
            "Epoch 6/100\n",
            "1132/1132 - 7s - loss: 0.4116 - accuracy: 0.8497 - val_loss: 0.4965 - val_accuracy: 0.8220\n",
            "Epoch 7/100\n",
            "1132/1132 - 7s - loss: 0.3410 - accuracy: 0.8817 - val_loss: 0.4663 - val_accuracy: 0.8416\n",
            "Epoch 8/100\n",
            "1132/1132 - 7s - loss: 0.2902 - accuracy: 0.9059 - val_loss: 0.4600 - val_accuracy: 0.8488\n",
            "Epoch 9/100\n",
            "1132/1132 - 7s - loss: 0.2499 - accuracy: 0.9228 - val_loss: 0.4364 - val_accuracy: 0.8514\n",
            "Epoch 10/100\n",
            "1132/1132 - 7s - loss: 0.2154 - accuracy: 0.9379 - val_loss: 0.4401 - val_accuracy: 0.8585\n",
            "Epoch 11/100\n",
            "1132/1132 - 7s - loss: 0.1879 - accuracy: 0.9506 - val_loss: 0.4438 - val_accuracy: 0.8595\n",
            "Epoch 12/100\n",
            "1132/1132 - 7s - loss: 0.1602 - accuracy: 0.9632 - val_loss: 0.4489 - val_accuracy: 0.8583\n",
            "Epoch 13/100\n",
            "1132/1132 - 7s - loss: 0.1359 - accuracy: 0.9747 - val_loss: 0.4484 - val_accuracy: 0.8630\n",
            "Epoch 14/100\n",
            "1132/1132 - 7s - loss: 0.1154 - accuracy: 0.9831 - val_loss: 0.4675 - val_accuracy: 0.8635\n",
            "Epoch 15/100\n",
            "1132/1132 - 7s - loss: 0.0972 - accuracy: 0.9893 - val_loss: 0.4709 - val_accuracy: 0.8636\n",
            "Epoch 16/100\n",
            "1132/1132 - 7s - loss: 0.0815 - accuracy: 0.9944 - val_loss: 0.4776 - val_accuracy: 0.8655\n",
            "Epoch 17/100\n",
            "1132/1132 - 7s - loss: 0.0707 - accuracy: 0.9968 - val_loss: 0.4864 - val_accuracy: 0.8669\n",
            "Epoch 18/100\n",
            "1132/1132 - 7s - loss: 0.0604 - accuracy: 0.9986 - val_loss: 0.4856 - val_accuracy: 0.8649\n",
            "Epoch 19/100\n",
            "1132/1132 - 7s - loss: 0.0532 - accuracy: 0.9994 - val_loss: 0.4813 - val_accuracy: 0.8674\n",
            "Epoch 20/100\n",
            "1132/1132 - 7s - loss: 0.0479 - accuracy: 0.9996 - val_loss: 0.4740 - val_accuracy: 0.8676\n",
            "Epoch 21/100\n",
            "1132/1132 - 7s - loss: 0.0424 - accuracy: 0.9998 - val_loss: 0.4786 - val_accuracy: 0.8681\n",
            "Epoch 22/100\n",
            "1132/1132 - 7s - loss: 0.0385 - accuracy: 0.9998 - val_loss: 0.4752 - val_accuracy: 0.8688\n",
            "Epoch 23/100\n",
            "1132/1132 - 7s - loss: 0.0353 - accuracy: 0.9999 - val_loss: 0.4776 - val_accuracy: 0.8702\n",
            "Epoch 24/100\n",
            "1132/1132 - 7s - loss: 0.0326 - accuracy: 0.9999 - val_loss: 0.4781 - val_accuracy: 0.8701\n",
            "Epoch 25/100\n",
            "1132/1132 - 7s - loss: 0.0303 - accuracy: 0.9999 - val_loss: 0.4752 - val_accuracy: 0.8685\n",
            "Epoch 26/100\n",
            "1132/1132 - 7s - loss: 0.0281 - accuracy: 0.9999 - val_loss: 0.4749 - val_accuracy: 0.8697\n",
            "Epoch 27/100\n",
            "1132/1132 - 7s - loss: 0.0262 - accuracy: 1.0000 - val_loss: 0.4768 - val_accuracy: 0.8704\n",
            "Epoch 28/100\n",
            "1132/1132 - 7s - loss: 0.0247 - accuracy: 1.0000 - val_loss: 0.4821 - val_accuracy: 0.8669\n",
            "Epoch 29/100\n",
            "1132/1132 - 7s - loss: 0.0232 - accuracy: 1.0000 - val_loss: 0.4730 - val_accuracy: 0.8697\n",
            "Epoch 30/100\n",
            "1132/1132 - 7s - loss: 0.0218 - accuracy: 1.0000 - val_loss: 0.4853 - val_accuracy: 0.8685\n",
            "Epoch 31/100\n",
            "1132/1132 - 7s - loss: 0.0208 - accuracy: 1.0000 - val_loss: 0.4707 - val_accuracy: 0.8693\n",
            "Epoch 32/100\n",
            "1132/1132 - 7s - loss: 0.0195 - accuracy: 1.0000 - val_loss: 0.4706 - val_accuracy: 0.8691\n",
            "Epoch 33/100\n",
            "1132/1132 - 7s - loss: 0.0188 - accuracy: 1.0000 - val_loss: 0.4708 - val_accuracy: 0.8687\n",
            "Epoch 34/100\n",
            "1132/1132 - 7s - loss: 0.0178 - accuracy: 1.0000 - val_loss: 0.4721 - val_accuracy: 0.8693\n",
            "Epoch 35/100\n",
            "1132/1132 - 7s - loss: 0.0171 - accuracy: 1.0000 - val_loss: 0.4710 - val_accuracy: 0.8691\n",
            "Epoch 36/100\n",
            "1132/1132 - 7s - loss: 0.0165 - accuracy: 1.0000 - val_loss: 0.4697 - val_accuracy: 0.8688\n",
            "Epoch 37/100\n",
            "1132/1132 - 7s - loss: 0.0157 - accuracy: 1.0000 - val_loss: 0.4599 - val_accuracy: 0.8715\n",
            "Epoch 38/100\n",
            "1132/1132 - 7s - loss: 0.0153 - accuracy: 1.0000 - val_loss: 0.4665 - val_accuracy: 0.8699\n",
            "Epoch 39/100\n",
            "1132/1132 - 7s - loss: 0.0147 - accuracy: 1.0000 - val_loss: 0.4731 - val_accuracy: 0.8699\n",
            "Epoch 40/100\n",
            "1132/1132 - 7s - loss: 0.0142 - accuracy: 1.0000 - val_loss: 0.4733 - val_accuracy: 0.8689\n",
            "Epoch 41/100\n",
            "1132/1132 - 7s - loss: 0.0135 - accuracy: 1.0000 - val_loss: 0.4747 - val_accuracy: 0.8693\n",
            "Epoch 42/100\n",
            "1132/1132 - 7s - loss: 0.0133 - accuracy: 1.0000 - val_loss: 0.4645 - val_accuracy: 0.8701\n",
            "Epoch 43/100\n",
            "1132/1132 - 7s - loss: 0.0128 - accuracy: 1.0000 - val_loss: 0.4655 - val_accuracy: 0.8702\n",
            "Epoch 44/100\n",
            "1132/1132 - 7s - loss: 0.0125 - accuracy: 1.0000 - val_loss: 0.4667 - val_accuracy: 0.8683\n",
            "Epoch 45/100\n",
            "1132/1132 - 8s - loss: 0.0121 - accuracy: 1.0000 - val_loss: 0.4667 - val_accuracy: 0.8687\n",
            "Epoch 46/100\n",
            "1132/1132 - 7s - loss: 0.0118 - accuracy: 1.0000 - val_loss: 0.4678 - val_accuracy: 0.8685\n",
            "Epoch 47/100\n",
            "1132/1132 - 7s - loss: 0.0114 - accuracy: 1.0000 - val_loss: 0.4629 - val_accuracy: 0.8690\n",
            "Epoch 48/100\n",
            "1132/1132 - 7s - loss: 0.0112 - accuracy: 1.0000 - val_loss: 0.4708 - val_accuracy: 0.8685\n",
            "Epoch 49/100\n",
            "1132/1132 - 7s - loss: 0.0109 - accuracy: 1.0000 - val_loss: 0.4709 - val_accuracy: 0.8692\n",
            "Epoch 50/100\n",
            "1132/1132 - 7s - loss: 0.0107 - accuracy: 1.0000 - val_loss: 0.4682 - val_accuracy: 0.8674\n",
            "Epoch 51/100\n",
            "1132/1132 - 7s - loss: 0.0104 - accuracy: 1.0000 - val_loss: 0.4628 - val_accuracy: 0.8688\n",
            "Epoch 52/100\n",
            "1132/1132 - 7s - loss: 0.0102 - accuracy: 1.0000 - val_loss: 0.4662 - val_accuracy: 0.8697\n",
            "Epoch 53/100\n",
            "1132/1132 - 7s - loss: 0.0100 - accuracy: 1.0000 - val_loss: 0.4662 - val_accuracy: 0.8698\n",
            "Epoch 54/100\n",
            "1132/1132 - 7s - loss: 0.0098 - accuracy: 1.0000 - val_loss: 0.4640 - val_accuracy: 0.8713\n",
            "Epoch 55/100\n",
            "1132/1132 - 7s - loss: 0.0096 - accuracy: 1.0000 - val_loss: 0.4639 - val_accuracy: 0.8687\n",
            "Epoch 56/100\n",
            "1132/1132 - 7s - loss: 0.0093 - accuracy: 1.0000 - val_loss: 0.4584 - val_accuracy: 0.8697\n",
            "Epoch 57/100\n",
            "1132/1132 - 7s - loss: 0.0093 - accuracy: 1.0000 - val_loss: 0.4666 - val_accuracy: 0.8700\n",
            "Epoch 58/100\n",
            "1132/1132 - 7s - loss: 0.0091 - accuracy: 1.0000 - val_loss: 0.4659 - val_accuracy: 0.8696\n",
            "Epoch 59/100\n",
            "1132/1132 - 7s - loss: 0.0089 - accuracy: 1.0000 - val_loss: 0.4622 - val_accuracy: 0.8682\n",
            "Epoch 60/100\n",
            "1132/1132 - 7s - loss: 0.0087 - accuracy: 1.0000 - val_loss: 0.4679 - val_accuracy: 0.8681\n",
            "Epoch 61/100\n",
            "1132/1132 - 7s - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.4666 - val_accuracy: 0.8682\n",
            "Epoch 62/100\n",
            "1132/1132 - 7s - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.4692 - val_accuracy: 0.8669\n",
            "Epoch 63/100\n",
            "1132/1132 - 7s - loss: 0.0082 - accuracy: 1.0000 - val_loss: 0.4645 - val_accuracy: 0.8678\n",
            "Epoch 64/100\n",
            "1132/1132 - 7s - loss: 0.0081 - accuracy: 1.0000 - val_loss: 0.4666 - val_accuracy: 0.8679\n",
            "Epoch 65/100\n",
            "1132/1132 - 7s - loss: 0.0081 - accuracy: 1.0000 - val_loss: 0.4675 - val_accuracy: 0.8685\n",
            "Epoch 66/100\n",
            "1132/1132 - 7s - loss: 0.0079 - accuracy: 1.0000 - val_loss: 0.4679 - val_accuracy: 0.8685\n",
            "Epoch 67/100\n",
            "1132/1132 - 7s - loss: 0.0079 - accuracy: 1.0000 - val_loss: 0.4681 - val_accuracy: 0.8675\n",
            "Epoch 68/100\n",
            "1132/1132 - 7s - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.4643 - val_accuracy: 0.8702\n",
            "Epoch 69/100\n",
            "1132/1132 - 7s - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.4673 - val_accuracy: 0.8666\n",
            "Epoch 70/100\n",
            "1132/1132 - 7s - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.4660 - val_accuracy: 0.8688\n",
            "Epoch 71/100\n",
            "1132/1132 - 7s - loss: 0.0073 - accuracy: 1.0000 - val_loss: 0.4620 - val_accuracy: 0.8687\n",
            "Epoch 72/100\n",
            "1132/1132 - 7s - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.4663 - val_accuracy: 0.8672\n",
            "Epoch 73/100\n",
            "1132/1132 - 7s - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.4591 - val_accuracy: 0.8677\n",
            "Epoch 74/100\n",
            "1132/1132 - 7s - loss: 0.0071 - accuracy: 1.0000 - val_loss: 0.4679 - val_accuracy: 0.8680\n",
            "Epoch 75/100\n",
            "1132/1132 - 7s - loss: 0.0070 - accuracy: 1.0000 - val_loss: 0.4658 - val_accuracy: 0.8689\n",
            "Epoch 76/100\n",
            "1132/1132 - 7s - loss: 0.0070 - accuracy: 1.0000 - val_loss: 0.4644 - val_accuracy: 0.8678\n",
            "Epoch 77/100\n",
            "1132/1132 - 7s - loss: 0.0069 - accuracy: 1.0000 - val_loss: 0.4618 - val_accuracy: 0.8689\n",
            "Epoch 78/100\n",
            "1132/1132 - 7s - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.4588 - val_accuracy: 0.8696\n",
            "Epoch 79/100\n",
            "1132/1132 - 7s - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.4675 - val_accuracy: 0.8666\n",
            "Epoch 80/100\n",
            "1132/1132 - 7s - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.4624 - val_accuracy: 0.8671\n",
            "Epoch 81/100\n",
            "1132/1132 - 7s - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.4650 - val_accuracy: 0.8665\n",
            "Epoch 82/100\n",
            "1132/1132 - 7s - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.4674 - val_accuracy: 0.8679\n",
            "Epoch 83/100\n",
            "1132/1132 - 7s - loss: 0.0064 - accuracy: 1.0000 - val_loss: 0.4644 - val_accuracy: 0.8666\n",
            "Epoch 84/100\n",
            "1132/1132 - 7s - loss: 0.0064 - accuracy: 1.0000 - val_loss: 0.4660 - val_accuracy: 0.8676\n",
            "Epoch 85/100\n",
            "1132/1132 - 7s - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.4619 - val_accuracy: 0.8671\n",
            "Epoch 86/100\n",
            "1132/1132 - 7s - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.4661 - val_accuracy: 0.8665\n",
            "Epoch 87/100\n",
            "1132/1132 - 8s - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.4671 - val_accuracy: 0.8674\n",
            "Epoch 88/100\n",
            "1132/1132 - 8s - loss: 0.0060 - accuracy: 1.0000 - val_loss: 0.4602 - val_accuracy: 0.8659\n",
            "Epoch 89/100\n",
            "1132/1132 - 7s - loss: 0.0060 - accuracy: 1.0000 - val_loss: 0.4683 - val_accuracy: 0.8667\n",
            "Epoch 90/100\n",
            "1132/1132 - 7s - loss: 0.0060 - accuracy: 1.0000 - val_loss: 0.4665 - val_accuracy: 0.8686\n",
            "Epoch 91/100\n",
            "1132/1132 - 7s - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.4608 - val_accuracy: 0.8677\n",
            "Epoch 92/100\n",
            "1132/1132 - 7s - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.4615 - val_accuracy: 0.8677\n",
            "Epoch 93/100\n",
            "1132/1132 - 7s - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.4627 - val_accuracy: 0.8660\n",
            "Epoch 94/100\n",
            "1132/1132 - 7s - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.4640 - val_accuracy: 0.8661\n",
            "Epoch 95/100\n",
            "1132/1132 - 7s - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.4679 - val_accuracy: 0.8662\n",
            "Epoch 96/100\n",
            "1132/1132 - 7s - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.4628 - val_accuracy: 0.8666\n",
            "Epoch 97/100\n",
            "1132/1132 - 7s - loss: 0.0055 - accuracy: 1.0000 - val_loss: 0.4597 - val_accuracy: 0.8662\n",
            "Epoch 98/100\n",
            "1132/1132 - 7s - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.4617 - val_accuracy: 0.8682\n",
            "Epoch 99/100\n",
            "1132/1132 - 7s - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.4657 - val_accuracy: 0.8654\n",
            "Epoch 100/100\n",
            "1132/1132 - 7s - loss: 0.0055 - accuracy: 1.0000 - val_loss: 0.4606 - val_accuracy: 0.8670\n",
            "lr = 0.005\n",
            "acc = 0.867020101612547\n",
            "--------time QMKDCsgd---------------\n",
            "742.2460424900055\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 631
        },
        "id": "XVEtGDoaaG5S",
        "outputId": "a659f23e-8b33-4226-91d0-5adffbfd51b1"
      },
      "source": [
        "#Best lr set_lr_best = [0.0005, 0.0025, 0.005]\r\n",
        "set_lr_a = [0.005]\r\n",
        "set_lr_b = [0.00005, 0.0005, 0.5, 10]\r\n",
        "set_lr_c = [0.000005, 0.00005, 1, 10]\r\n",
        "set_lr_d = [0.0005, 0.005, 0.05, 0.5, 1, 10]\r\n",
        "\r\n",
        "for lr in set_lr_a:\r\n",
        "  exp_time = time()\r\n",
        "  qmkdc1_dig = models.QMKDClassifierSGD(input_dim=1000, dim_x=component_dim, num_classes=num_classes, num_eig=num_eig, gamma=2**-9, random_state=None)\r\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\r\n",
        "  qmkdc1_dig.compile(optimizer, loss=tf.keras.losses.CategoricalCrossentropy(), metrics=['accuracy'])\r\n",
        "  y_train_bin = tf.reshape(tf.keras.backend.one_hot(y_train, num_classes), (-1,num_classes))\r\n",
        "  y_test_bin = tf.reshape(tf.keras.backend.one_hot(y_test, num_classes), (-1,num_classes))\r\n",
        "  qmkdc1_dig.fit(X_train, y_train_bin, epochs=100, verbose=2, validation_data=(X_test, y_test_bin))\r\n",
        "  out = qmkdc1_dig.predict(X_test)\r\n",
        "  print(f'lr = {lr}')\r\n",
        "  print(f'acc = {accuracy_score(y_test, np.argmax(out, axis=1))}')\r\n",
        "  print(\"--------time QMKDCsgd---------------\")\r\n",
        "  print(time() - exp_time)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "1132/1132 - 8s - loss: 0.9278 - accuracy: 0.6240 - val_loss: 0.8300 - val_accuracy: 0.6779\n",
            "Epoch 2/100\n",
            "1132/1132 - 8s - loss: 0.7662 - accuracy: 0.7020 - val_loss: 0.7618 - val_accuracy: 0.7043\n",
            "Epoch 3/100\n",
            "1132/1132 - 8s - loss: 0.6734 - accuracy: 0.7309 - val_loss: 0.6728 - val_accuracy: 0.7268\n",
            "Epoch 4/100\n",
            "1132/1132 - 8s - loss: 0.5751 - accuracy: 0.7742 - val_loss: 0.6059 - val_accuracy: 0.7668\n",
            "Epoch 5/100\n",
            "1132/1132 - 8s - loss: 0.4834 - accuracy: 0.8175 - val_loss: 0.5415 - val_accuracy: 0.7964\n",
            "Epoch 6/100\n",
            "1132/1132 - 8s - loss: 0.4036 - accuracy: 0.8561 - val_loss: 0.5001 - val_accuracy: 0.8234\n",
            "Epoch 7/100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-f182bb2629b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0my_train_bin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0my_test_bin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m   \u001b[0mqmkdc1_dig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_bin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_bin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m   \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqmkdc1_dig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'lr = {lr}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSlp5_-zON9y",
        "outputId": "9bf673aa-4608-425f-e24b-4989e9d7383c"
      },
      "source": [
        "#Best lr set_lr_best = [0.0005, 0.0025, 0.005]\r\n",
        "set_lr_a = [0.005]\r\n",
        "set_lr_b = [0.00005, 0.0005, 0.5, 10]\r\n",
        "set_lr_c = [0.000005, 0.00005, 1, 10]\r\n",
        "set_lr_d = [0.0005, 0.005, 0.05, 0.5, 1, 10]\r\n",
        "\r\n",
        "for lr in set_lr_a:\r\n",
        "  exp_time = time()\r\n",
        "  qmkdc1_dig = models.QMKDClassifierSGD(input_dim=1000, dim_x=component_dim, num_classes=num_classes, num_eig=num_eig, gamma=2**-9, random_state=None)\r\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\r\n",
        "  qmkdc1_dig.compile(optimizer, loss=tf.keras.losses.CategoricalCrossentropy(), metrics=['accuracy'])\r\n",
        "  y_train_bin = tf.reshape(tf.keras.backend.one_hot(y_train, num_classes), (-1,num_classes))\r\n",
        "  y_test_bin = tf.reshape(tf.keras.backend.one_hot(y_test, num_classes), (-1,num_classes))\r\n",
        "  qmkdc1_dig.fit(X_train, y_train_bin, epochs=100, verbose=2, validation_data=(X_test, y_test_bin))\r\n",
        "  out = qmkdc1_dig.predict(X_test)\r\n",
        "  print(f'lr = {lr}')\r\n",
        "  print(f'acc = {accuracy_score(y_test, np.argmax(out, axis=1))}')\r\n",
        "  print(\"--------time QMKDCsgd---------------\")\r\n",
        "  print(time() - exp_time)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "1132/1132 - 8s - loss: 0.9232 - accuracy: 0.6236 - val_loss: 0.8163 - val_accuracy: 0.6791\n",
            "Epoch 2/100\n",
            "1132/1132 - 7s - loss: 0.7614 - accuracy: 0.6997 - val_loss: 0.7573 - val_accuracy: 0.7012\n",
            "Epoch 3/100\n",
            "1132/1132 - 7s - loss: 0.6722 - accuracy: 0.7313 - val_loss: 0.6841 - val_accuracy: 0.7335\n",
            "Epoch 4/100\n",
            "1132/1132 - 7s - loss: 0.5745 - accuracy: 0.7739 - val_loss: 0.6078 - val_accuracy: 0.7678\n",
            "Epoch 5/100\n",
            "1132/1132 - 7s - loss: 0.4832 - accuracy: 0.8175 - val_loss: 0.5438 - val_accuracy: 0.8028\n",
            "Epoch 6/100\n",
            "1132/1132 - 7s - loss: 0.3998 - accuracy: 0.8565 - val_loss: 0.4944 - val_accuracy: 0.8212\n",
            "Epoch 7/100\n",
            "1132/1132 - 7s - loss: 0.3384 - accuracy: 0.8830 - val_loss: 0.4738 - val_accuracy: 0.8328\n",
            "Epoch 8/100\n",
            "1132/1132 - 7s - loss: 0.2907 - accuracy: 0.9034 - val_loss: 0.4480 - val_accuracy: 0.8435\n",
            "Epoch 9/100\n",
            "1132/1132 - 7s - loss: 0.2518 - accuracy: 0.9216 - val_loss: 0.4456 - val_accuracy: 0.8521\n",
            "Epoch 10/100\n",
            "1132/1132 - 7s - loss: 0.2201 - accuracy: 0.9355 - val_loss: 0.4379 - val_accuracy: 0.8545\n",
            "Epoch 11/100\n",
            "1132/1132 - 7s - loss: 0.1924 - accuracy: 0.9489 - val_loss: 0.4455 - val_accuracy: 0.8563\n",
            "Epoch 12/100\n",
            "1132/1132 - 7s - loss: 0.1659 - accuracy: 0.9607 - val_loss: 0.4701 - val_accuracy: 0.8577\n",
            "Epoch 13/100\n",
            "1132/1132 - 7s - loss: 0.1432 - accuracy: 0.9707 - val_loss: 0.4764 - val_accuracy: 0.8548\n",
            "Epoch 14/100\n",
            "1132/1132 - 7s - loss: 0.1226 - accuracy: 0.9811 - val_loss: 0.4893 - val_accuracy: 0.8581\n",
            "Epoch 15/100\n",
            "1132/1132 - 7s - loss: 0.1053 - accuracy: 0.9874 - val_loss: 0.4847 - val_accuracy: 0.8585\n",
            "Epoch 16/100\n",
            "1132/1132 - 7s - loss: 0.0908 - accuracy: 0.9927 - val_loss: 0.4907 - val_accuracy: 0.8601\n",
            "Epoch 17/100\n",
            "1132/1132 - 7s - loss: 0.0786 - accuracy: 0.9958 - val_loss: 0.4862 - val_accuracy: 0.8644\n",
            "Epoch 18/100\n",
            "1132/1132 - 7s - loss: 0.0693 - accuracy: 0.9975 - val_loss: 0.4831 - val_accuracy: 0.8619\n",
            "Epoch 19/100\n",
            "1132/1132 - 7s - loss: 0.0614 - accuracy: 0.9988 - val_loss: 0.4931 - val_accuracy: 0.8655\n",
            "Epoch 20/100\n",
            "1132/1132 - 7s - loss: 0.0554 - accuracy: 0.9991 - val_loss: 0.4870 - val_accuracy: 0.8650\n",
            "Epoch 21/100\n",
            "1132/1132 - 7s - loss: 0.0494 - accuracy: 0.9995 - val_loss: 0.4789 - val_accuracy: 0.8656\n",
            "Epoch 22/100\n",
            "1132/1132 - 7s - loss: 0.0457 - accuracy: 0.9997 - val_loss: 0.4932 - val_accuracy: 0.8639\n",
            "Epoch 23/100\n",
            "1132/1132 - 7s - loss: 0.0417 - accuracy: 0.9998 - val_loss: 0.4851 - val_accuracy: 0.8653\n",
            "Epoch 24/100\n",
            "1132/1132 - 7s - loss: 0.0386 - accuracy: 0.9998 - val_loss: 0.4911 - val_accuracy: 0.8623\n",
            "Epoch 25/100\n",
            "1132/1132 - 7s - loss: 0.0360 - accuracy: 0.9999 - val_loss: 0.4827 - val_accuracy: 0.8665\n",
            "Epoch 26/100\n",
            "1132/1132 - 7s - loss: 0.0332 - accuracy: 0.9999 - val_loss: 0.4807 - val_accuracy: 0.8644\n",
            "Epoch 27/100\n",
            "1132/1132 - 7s - loss: 0.0318 - accuracy: 0.9999 - val_loss: 0.4848 - val_accuracy: 0.8659\n",
            "Epoch 28/100\n",
            "1132/1132 - 7s - loss: 0.0290 - accuracy: 1.0000 - val_loss: 0.4831 - val_accuracy: 0.8671\n",
            "Epoch 29/100\n",
            "1132/1132 - 7s - loss: 0.0274 - accuracy: 1.0000 - val_loss: 0.4903 - val_accuracy: 0.8669\n",
            "Epoch 30/100\n",
            "1132/1132 - 7s - loss: 0.0261 - accuracy: 1.0000 - val_loss: 0.4826 - val_accuracy: 0.8686\n",
            "Epoch 31/100\n",
            "1132/1132 - 7s - loss: 0.0245 - accuracy: 1.0000 - val_loss: 0.4807 - val_accuracy: 0.8651\n",
            "Epoch 32/100\n",
            "1132/1132 - 7s - loss: 0.0230 - accuracy: 1.0000 - val_loss: 0.4835 - val_accuracy: 0.8657\n",
            "Epoch 33/100\n",
            "1132/1132 - 7s - loss: 0.0221 - accuracy: 1.0000 - val_loss: 0.4775 - val_accuracy: 0.8678\n",
            "Epoch 34/100\n",
            "1132/1132 - 7s - loss: 0.0210 - accuracy: 1.0000 - val_loss: 0.4801 - val_accuracy: 0.8651\n",
            "Epoch 35/100\n",
            "1132/1132 - 7s - loss: 0.0201 - accuracy: 1.0000 - val_loss: 0.4786 - val_accuracy: 0.8657\n",
            "Epoch 36/100\n",
            "1132/1132 - 8s - loss: 0.0191 - accuracy: 1.0000 - val_loss: 0.4798 - val_accuracy: 0.8670\n",
            "Epoch 37/100\n",
            "1132/1132 - 8s - loss: 0.0183 - accuracy: 1.0000 - val_loss: 0.4833 - val_accuracy: 0.8676\n",
            "Epoch 38/100\n",
            "1132/1132 - 7s - loss: 0.0176 - accuracy: 1.0000 - val_loss: 0.4889 - val_accuracy: 0.8656\n",
            "Epoch 39/100\n",
            "1132/1132 - 7s - loss: 0.0170 - accuracy: 1.0000 - val_loss: 0.4814 - val_accuracy: 0.8674\n",
            "Epoch 40/100\n",
            "1132/1132 - 7s - loss: 0.0162 - accuracy: 1.0000 - val_loss: 0.4785 - val_accuracy: 0.8686\n",
            "Epoch 41/100\n",
            "1132/1132 - 7s - loss: 0.0159 - accuracy: 1.0000 - val_loss: 0.4809 - val_accuracy: 0.8667\n",
            "Epoch 42/100\n",
            "1132/1132 - 7s - loss: 0.0159 - accuracy: 0.9999 - val_loss: 0.4780 - val_accuracy: 0.8675\n",
            "Epoch 43/100\n",
            "1132/1132 - 7s - loss: 0.0145 - accuracy: 1.0000 - val_loss: 0.4854 - val_accuracy: 0.8665\n",
            "Epoch 44/100\n",
            "1132/1132 - 7s - loss: 0.0139 - accuracy: 1.0000 - val_loss: 0.4844 - val_accuracy: 0.8670\n",
            "Epoch 45/100\n",
            "1132/1132 - 7s - loss: 0.0136 - accuracy: 1.0000 - val_loss: 0.4878 - val_accuracy: 0.8644\n",
            "Epoch 46/100\n",
            "1132/1132 - 7s - loss: 0.0136 - accuracy: 1.0000 - val_loss: 0.4810 - val_accuracy: 0.8644\n",
            "Epoch 47/100\n",
            "1132/1132 - 7s - loss: 0.0129 - accuracy: 1.0000 - val_loss: 0.4789 - val_accuracy: 0.8662\n",
            "Epoch 48/100\n",
            "1132/1132 - 7s - loss: 0.0124 - accuracy: 1.0000 - val_loss: 0.4930 - val_accuracy: 0.8654\n",
            "Epoch 49/100\n",
            "1132/1132 - 7s - loss: 0.0122 - accuracy: 1.0000 - val_loss: 0.4854 - val_accuracy: 0.8666\n",
            "Epoch 50/100\n",
            "1132/1132 - 7s - loss: 0.0120 - accuracy: 1.0000 - val_loss: 0.4831 - val_accuracy: 0.8661\n",
            "Epoch 51/100\n",
            "1132/1132 - 7s - loss: 0.0117 - accuracy: 1.0000 - val_loss: 0.4832 - val_accuracy: 0.8665\n",
            "Epoch 52/100\n",
            "1132/1132 - 7s - loss: 0.0114 - accuracy: 1.0000 - val_loss: 0.4840 - val_accuracy: 0.8632\n",
            "Epoch 53/100\n",
            "1132/1132 - 7s - loss: 0.0112 - accuracy: 1.0000 - val_loss: 0.4797 - val_accuracy: 0.8647\n",
            "Epoch 54/100\n",
            "1132/1132 - 7s - loss: 0.0108 - accuracy: 1.0000 - val_loss: 0.4796 - val_accuracy: 0.8659\n",
            "Epoch 55/100\n",
            "1132/1132 - 7s - loss: 0.0105 - accuracy: 1.0000 - val_loss: 0.4887 - val_accuracy: 0.8654\n",
            "Epoch 56/100\n",
            "1132/1132 - 7s - loss: 0.0104 - accuracy: 1.0000 - val_loss: 0.4843 - val_accuracy: 0.8665\n",
            "Epoch 57/100\n",
            "1132/1132 - 7s - loss: 0.0104 - accuracy: 1.0000 - val_loss: 0.4788 - val_accuracy: 0.8658\n",
            "Epoch 58/100\n",
            "1132/1132 - 7s - loss: 0.0100 - accuracy: 1.0000 - val_loss: 0.4821 - val_accuracy: 0.8651\n",
            "Epoch 59/100\n",
            "1132/1132 - 7s - loss: 0.0097 - accuracy: 1.0000 - val_loss: 0.4861 - val_accuracy: 0.8655\n",
            "Epoch 60/100\n",
            "1132/1132 - 7s - loss: 0.0096 - accuracy: 1.0000 - val_loss: 0.4829 - val_accuracy: 0.8659\n",
            "Epoch 61/100\n",
            "1132/1132 - 7s - loss: 0.0094 - accuracy: 1.0000 - val_loss: 0.4871 - val_accuracy: 0.8662\n",
            "Epoch 62/100\n",
            "1132/1132 - 7s - loss: 0.0093 - accuracy: 1.0000 - val_loss: 0.4918 - val_accuracy: 0.8655\n",
            "Epoch 63/100\n",
            "1132/1132 - 7s - loss: 0.0098 - accuracy: 1.0000 - val_loss: 0.4789 - val_accuracy: 0.8658\n",
            "Epoch 64/100\n",
            "1132/1132 - 7s - loss: 0.0088 - accuracy: 1.0000 - val_loss: 0.4922 - val_accuracy: 0.8680\n",
            "Epoch 65/100\n",
            "1132/1132 - 7s - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.4878 - val_accuracy: 0.8658\n",
            "Epoch 66/100\n",
            "1132/1132 - 7s - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.4966 - val_accuracy: 0.8661\n",
            "Epoch 67/100\n",
            "1132/1132 - 7s - loss: 0.0087 - accuracy: 1.0000 - val_loss: 0.4871 - val_accuracy: 0.8658\n",
            "Epoch 68/100\n",
            "1132/1132 - 7s - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.4833 - val_accuracy: 0.8646\n",
            "Epoch 69/100\n",
            "1132/1132 - 7s - loss: 0.0083 - accuracy: 1.0000 - val_loss: 0.4878 - val_accuracy: 0.8662\n",
            "Epoch 70/100\n",
            "1132/1132 - 7s - loss: 0.0082 - accuracy: 1.0000 - val_loss: 0.4830 - val_accuracy: 0.8672\n",
            "Epoch 71/100\n",
            "1132/1132 - 7s - loss: 0.0082 - accuracy: 1.0000 - val_loss: 0.4920 - val_accuracy: 0.8638\n",
            "Epoch 72/100\n",
            "1132/1132 - 7s - loss: 0.0079 - accuracy: 1.0000 - val_loss: 0.4898 - val_accuracy: 0.8666\n",
            "Epoch 73/100\n",
            "1132/1132 - 7s - loss: 0.0079 - accuracy: 1.0000 - val_loss: 0.4893 - val_accuracy: 0.8674\n",
            "Epoch 74/100\n",
            "1132/1132 - 7s - loss: 0.0078 - accuracy: 1.0000 - val_loss: 0.4844 - val_accuracy: 0.8662\n",
            "Epoch 75/100\n",
            "1132/1132 - 7s - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.4867 - val_accuracy: 0.8672\n",
            "Epoch 76/100\n",
            "1132/1132 - 7s - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.4909 - val_accuracy: 0.8672\n",
            "Epoch 77/100\n",
            "1132/1132 - 7s - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.4888 - val_accuracy: 0.8670\n",
            "Epoch 78/100\n",
            "1132/1132 - 7s - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.4905 - val_accuracy: 0.8659\n",
            "Epoch 79/100\n",
            "1132/1132 - 8s - loss: 0.0073 - accuracy: 1.0000 - val_loss: 0.4879 - val_accuracy: 0.8666\n",
            "Epoch 80/100\n",
            "1132/1132 - 7s - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.4929 - val_accuracy: 0.8651\n",
            "Epoch 81/100\n",
            "1132/1132 - 7s - loss: 0.0071 - accuracy: 1.0000 - val_loss: 0.4900 - val_accuracy: 0.8653\n",
            "Epoch 82/100\n",
            "1132/1132 - 7s - loss: 0.0070 - accuracy: 1.0000 - val_loss: 0.4941 - val_accuracy: 0.8645\n",
            "Epoch 83/100\n",
            "1132/1132 - 7s - loss: 0.0070 - accuracy: 1.0000 - val_loss: 0.4956 - val_accuracy: 0.8645\n",
            "Epoch 84/100\n",
            "1132/1132 - 7s - loss: 0.0069 - accuracy: 1.0000 - val_loss: 0.4944 - val_accuracy: 0.8660\n",
            "Epoch 85/100\n",
            "1132/1132 - 7s - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.4854 - val_accuracy: 0.8654\n",
            "Epoch 86/100\n",
            "1132/1132 - 7s - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.5005 - val_accuracy: 0.8653\n",
            "Epoch 87/100\n",
            "1132/1132 - 7s - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.4924 - val_accuracy: 0.8634\n",
            "Epoch 88/100\n",
            "1132/1132 - 7s - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.4895 - val_accuracy: 0.8648\n",
            "Epoch 89/100\n",
            "1132/1132 - 7s - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.4897 - val_accuracy: 0.8659\n",
            "Epoch 90/100\n",
            "1132/1132 - 7s - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.4872 - val_accuracy: 0.8664\n",
            "Epoch 91/100\n",
            "1132/1132 - 7s - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.5009 - val_accuracy: 0.8630\n",
            "Epoch 92/100\n",
            "1132/1132 - 7s - loss: 0.0064 - accuracy: 1.0000 - val_loss: 0.4914 - val_accuracy: 0.8650\n",
            "Epoch 93/100\n",
            "1132/1132 - 7s - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.4879 - val_accuracy: 0.8646\n",
            "Epoch 94/100\n",
            "1132/1132 - 7s - loss: 0.0064 - accuracy: 1.0000 - val_loss: 0.4846 - val_accuracy: 0.8648\n",
            "Epoch 95/100\n",
            "1132/1132 - 7s - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.4886 - val_accuracy: 0.8658\n",
            "Epoch 96/100\n",
            "1132/1132 - 7s - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.4988 - val_accuracy: 0.8644\n",
            "Epoch 97/100\n",
            "1132/1132 - 7s - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.4976 - val_accuracy: 0.8629\n",
            "Epoch 98/100\n",
            "1132/1132 - 7s - loss: 0.0060 - accuracy: 1.0000 - val_loss: 0.4936 - val_accuracy: 0.8651\n",
            "Epoch 99/100\n",
            "1132/1132 - 7s - loss: 0.0060 - accuracy: 1.0000 - val_loss: 0.4907 - val_accuracy: 0.8643\n",
            "Epoch 100/100\n",
            "1132/1132 - 7s - loss: 0.0060 - accuracy: 1.0000 - val_loss: 0.4871 - val_accuracy: 0.8636\n",
            "lr = 0.005\n",
            "acc = 0.8635962005743318\n",
            "--------time QMKDCsgd---------------\n",
            "736.7115707397461\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgK3rDRKP38W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39db4e95-a901-4092-8a6f-c2e094a53297"
      },
      "source": [
        "#Best lr set_lr_best = [0.0005, 0.0025, 0.005]\n",
        "set_lr_a = [0.005, 0.0005, 0.00005, 0.000005, 0.0000005]\n",
        "set_lr_b = [0.00005, 0.0005, 0.5, 10]\n",
        "set_lr_c = [0.000005, 0.00005, 1, 10]\n",
        "set_lr_d = [0.0005, 0.005, 0.05, 0.5, 1, 10]\n",
        "\n",
        "for lr in set_lr_a:\n",
        "  exp_time = time()\n",
        "  qmkdc1_dig = models.QMKDClassifierSGD(input_dim=1000, dim_x=component_dim, num_classes=num_classes, num_eig=num_eig, gamma=0.2, random_state=None)\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "  qmkdc1_dig.compile(optimizer, loss=tf.keras.losses.CategoricalCrossentropy())\n",
        "  y_train_bin = tf.reshape(tf.keras.backend.one_hot(y_train, num_classes), (-1,num_classes))\n",
        "  qmkdc1_dig.fit(X_train, y_train_bin, epochs=200)\n",
        "  out = qmkdc1_dig.predict(X_test)\n",
        "  print(f'lr = {lr}')\n",
        "  print(f'acc = {accuracy_score(y_test, np.argmax(out, axis=1))}')\n",
        "  print(\"--------time QMKDCsgd---------------\")\n",
        "  print(time() - exp_time)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.8777\n",
            "Epoch 2/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.6053\n",
            "Epoch 3/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.3810\n",
            "Epoch 4/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.2395\n",
            "Epoch 5/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.1567\n",
            "Epoch 6/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0961\n",
            "Epoch 7/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0606\n",
            "Epoch 8/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0395\n",
            "Epoch 9/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0292\n",
            "Epoch 10/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0236\n",
            "Epoch 11/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0204\n",
            "Epoch 12/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0179\n",
            "Epoch 13/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0160\n",
            "Epoch 14/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0143\n",
            "Epoch 15/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0129\n",
            "Epoch 16/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0116\n",
            "Epoch 17/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0109\n",
            "Epoch 18/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0101\n",
            "Epoch 19/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0097\n",
            "Epoch 20/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0089\n",
            "Epoch 21/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0082\n",
            "Epoch 22/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0079\n",
            "Epoch 23/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0076\n",
            "Epoch 24/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0072\n",
            "Epoch 25/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0071\n",
            "Epoch 26/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0065\n",
            "Epoch 27/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0063\n",
            "Epoch 28/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0061\n",
            "Epoch 29/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0058\n",
            "Epoch 30/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0057\n",
            "Epoch 31/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0054\n",
            "Epoch 32/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0052\n",
            "Epoch 33/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0052\n",
            "Epoch 34/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0050\n",
            "Epoch 35/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0048\n",
            "Epoch 36/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0047\n",
            "Epoch 37/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0047\n",
            "Epoch 38/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0044\n",
            "Epoch 39/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0044\n",
            "Epoch 40/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0043\n",
            "Epoch 41/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0042\n",
            "Epoch 42/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0040\n",
            "Epoch 43/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0041\n",
            "Epoch 44/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0039\n",
            "Epoch 45/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0038\n",
            "Epoch 46/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0038\n",
            "Epoch 47/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0036\n",
            "Epoch 48/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0036\n",
            "Epoch 49/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0036\n",
            "Epoch 50/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0035\n",
            "Epoch 51/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0034\n",
            "Epoch 52/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0034\n",
            "Epoch 53/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0033\n",
            "Epoch 54/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0032\n",
            "Epoch 55/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0032\n",
            "Epoch 56/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0032\n",
            "Epoch 57/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0031\n",
            "Epoch 58/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0031\n",
            "Epoch 59/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0030\n",
            "Epoch 60/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0030\n",
            "Epoch 61/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0029\n",
            "Epoch 62/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0029\n",
            "Epoch 63/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0029\n",
            "Epoch 64/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0028\n",
            "Epoch 65/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0028\n",
            "Epoch 66/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0028\n",
            "Epoch 67/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0027\n",
            "Epoch 68/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0027\n",
            "Epoch 69/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0027\n",
            "Epoch 70/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0026\n",
            "Epoch 71/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0026\n",
            "Epoch 72/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0026\n",
            "Epoch 73/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0026\n",
            "Epoch 74/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0026\n",
            "Epoch 75/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0025\n",
            "Epoch 76/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0025\n",
            "Epoch 77/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0025\n",
            "Epoch 78/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0024\n",
            "Epoch 79/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0024\n",
            "Epoch 80/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0024\n",
            "Epoch 81/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0024\n",
            "Epoch 82/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0023\n",
            "Epoch 83/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0023\n",
            "Epoch 84/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0023\n",
            "Epoch 85/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0023\n",
            "Epoch 86/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0023\n",
            "Epoch 87/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0023\n",
            "Epoch 88/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0022\n",
            "Epoch 89/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0022\n",
            "Epoch 90/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0022\n",
            "Epoch 91/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0022\n",
            "Epoch 92/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0022\n",
            "Epoch 93/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0021\n",
            "Epoch 94/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0021\n",
            "Epoch 95/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0021\n",
            "Epoch 96/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0021\n",
            "Epoch 97/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0021\n",
            "Epoch 98/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0021\n",
            "Epoch 99/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0020\n",
            "Epoch 100/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0020\n",
            "Epoch 101/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0020\n",
            "Epoch 102/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0020\n",
            "Epoch 103/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0020\n",
            "Epoch 104/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0019\n",
            "Epoch 105/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0020\n",
            "Epoch 106/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0020\n",
            "Epoch 107/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0019\n",
            "Epoch 108/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0019\n",
            "Epoch 109/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0019\n",
            "Epoch 110/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0019\n",
            "Epoch 111/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0019\n",
            "Epoch 112/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0019\n",
            "Epoch 113/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0019\n",
            "Epoch 114/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0019\n",
            "Epoch 115/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0018\n",
            "Epoch 116/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0018\n",
            "Epoch 117/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0018\n",
            "Epoch 118/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0018\n",
            "Epoch 119/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0018\n",
            "Epoch 120/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0018\n",
            "Epoch 121/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0018\n",
            "Epoch 122/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0018\n",
            "Epoch 123/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0018\n",
            "Epoch 124/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0018\n",
            "Epoch 125/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0017\n",
            "Epoch 126/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0018\n",
            "Epoch 127/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0017\n",
            "Epoch 128/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0017\n",
            "Epoch 129/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0017\n",
            "Epoch 130/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0017\n",
            "Epoch 131/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0017\n",
            "Epoch 132/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0017\n",
            "Epoch 133/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0017\n",
            "Epoch 134/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0017\n",
            "Epoch 135/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0017\n",
            "Epoch 136/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0017\n",
            "Epoch 137/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0016\n",
            "Epoch 138/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0017\n",
            "Epoch 139/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0016\n",
            "Epoch 140/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0016\n",
            "Epoch 141/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0016\n",
            "Epoch 142/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0016\n",
            "Epoch 143/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0016\n",
            "Epoch 144/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0016\n",
            "Epoch 145/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0016\n",
            "Epoch 146/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0016\n",
            "Epoch 147/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0016\n",
            "Epoch 148/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0016\n",
            "Epoch 149/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0016\n",
            "Epoch 150/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0016\n",
            "Epoch 151/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0015\n",
            "Epoch 152/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0016\n",
            "Epoch 153/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0015\n",
            "Epoch 154/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0015\n",
            "Epoch 155/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0015\n",
            "Epoch 156/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0015\n",
            "Epoch 157/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0015\n",
            "Epoch 158/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0015\n",
            "Epoch 159/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0015\n",
            "Epoch 160/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0015\n",
            "Epoch 161/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0015\n",
            "Epoch 162/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0015\n",
            "Epoch 163/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0015\n",
            "Epoch 164/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0015\n",
            "Epoch 165/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0015\n",
            "Epoch 166/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0015\n",
            "Epoch 167/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0015\n",
            "Epoch 168/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0015\n",
            "Epoch 169/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0015\n",
            "Epoch 170/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0015\n",
            "Epoch 171/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0014\n",
            "Epoch 172/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0014\n",
            "Epoch 173/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0014\n",
            "Epoch 174/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0015\n",
            "Epoch 175/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0014\n",
            "Epoch 176/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0014\n",
            "Epoch 177/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0014\n",
            "Epoch 178/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0014\n",
            "Epoch 179/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0014\n",
            "Epoch 180/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0014\n",
            "Epoch 181/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0014\n",
            "Epoch 182/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0014\n",
            "Epoch 183/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0014\n",
            "Epoch 184/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0014\n",
            "Epoch 185/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0014\n",
            "Epoch 186/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0014\n",
            "Epoch 187/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0014\n",
            "Epoch 188/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0014\n",
            "Epoch 189/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0014\n",
            "Epoch 190/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0014\n",
            "Epoch 191/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0014\n",
            "Epoch 192/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0013\n",
            "Epoch 193/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0013\n",
            "Epoch 194/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0013\n",
            "Epoch 195/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0014\n",
            "Epoch 196/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0013\n",
            "Epoch 197/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0013\n",
            "Epoch 198/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0013\n",
            "Epoch 199/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0013\n",
            "Epoch 200/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0013\n",
            "lr = 0.005\n",
            "acc = 0.8926441351888668\n",
            "--------time QMKDCsgd---------------\n",
            "1404.6034326553345\n",
            "Epoch 1/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.0150\n",
            "Epoch 2/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.8047\n",
            "Epoch 3/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7176\n",
            "Epoch 4/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.6365\n",
            "Epoch 5/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.5539\n",
            "Epoch 6/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.4792\n",
            "Epoch 7/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.4141\n",
            "Epoch 8/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.3603\n",
            "Epoch 9/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.3174\n",
            "Epoch 10/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.2781\n",
            "Epoch 11/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.2455\n",
            "Epoch 12/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.2161\n",
            "Epoch 13/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.1877\n",
            "Epoch 14/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.1640\n",
            "Epoch 15/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.1424\n",
            "Epoch 16/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.1228\n",
            "Epoch 17/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.1070\n",
            "Epoch 18/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0911\n",
            "Epoch 19/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0784\n",
            "Epoch 20/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0679\n",
            "Epoch 21/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0583\n",
            "Epoch 22/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0499\n",
            "Epoch 23/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0432\n",
            "Epoch 24/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0386\n",
            "Epoch 25/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0333\n",
            "Epoch 26/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0291\n",
            "Epoch 27/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0260\n",
            "Epoch 28/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0232\n",
            "Epoch 29/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0210\n",
            "Epoch 30/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0193\n",
            "Epoch 31/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0174\n",
            "Epoch 32/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0158\n",
            "Epoch 33/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0146\n",
            "Epoch 34/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0134\n",
            "Epoch 35/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0127\n",
            "Epoch 36/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0118\n",
            "Epoch 37/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0109\n",
            "Epoch 38/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0103\n",
            "Epoch 39/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0096\n",
            "Epoch 40/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0092\n",
            "Epoch 41/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0085\n",
            "Epoch 42/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0083\n",
            "Epoch 43/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0079\n",
            "Epoch 44/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0075\n",
            "Epoch 45/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0071\n",
            "Epoch 46/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0068\n",
            "Epoch 47/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0065\n",
            "Epoch 48/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0063\n",
            "Epoch 49/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0060\n",
            "Epoch 50/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0058\n",
            "Epoch 51/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0057\n",
            "Epoch 52/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0051\n",
            "Epoch 53/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0052\n",
            "Epoch 54/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0050\n",
            "Epoch 55/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0049\n",
            "Epoch 56/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0046\n",
            "Epoch 57/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0045\n",
            "Epoch 58/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0042\n",
            "Epoch 59/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0042\n",
            "Epoch 60/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0042\n",
            "Epoch 61/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0039\n",
            "Epoch 62/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0038\n",
            "Epoch 63/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0037\n",
            "Epoch 64/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0037\n",
            "Epoch 65/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0036\n",
            "Epoch 66/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0035\n",
            "Epoch 67/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0033\n",
            "Epoch 68/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0033\n",
            "Epoch 69/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0033\n",
            "Epoch 70/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0032\n",
            "Epoch 71/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0031\n",
            "Epoch 72/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0029\n",
            "Epoch 73/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0029\n",
            "Epoch 74/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0029\n",
            "Epoch 75/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0029\n",
            "Epoch 76/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0027\n",
            "Epoch 77/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0027\n",
            "Epoch 78/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0027\n",
            "Epoch 79/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0025\n",
            "Epoch 80/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0025\n",
            "Epoch 81/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0025\n",
            "Epoch 82/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0024\n",
            "Epoch 83/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0024\n",
            "Epoch 84/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0023\n",
            "Epoch 85/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0023\n",
            "Epoch 86/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0023\n",
            "Epoch 87/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0022\n",
            "Epoch 88/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0022\n",
            "Epoch 89/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0022\n",
            "Epoch 90/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0021\n",
            "Epoch 91/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0021\n",
            "Epoch 92/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0021\n",
            "Epoch 93/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0019\n",
            "Epoch 94/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0020\n",
            "Epoch 95/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0020\n",
            "Epoch 96/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0020\n",
            "Epoch 97/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0019\n",
            "Epoch 98/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0019\n",
            "Epoch 99/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0019\n",
            "Epoch 100/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0017\n",
            "Epoch 101/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0018\n",
            "Epoch 102/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0018\n",
            "Epoch 103/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0018\n",
            "Epoch 104/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0017\n",
            "Epoch 105/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0017\n",
            "Epoch 106/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0017\n",
            "Epoch 107/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0016\n",
            "Epoch 108/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0017\n",
            "Epoch 109/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0016\n",
            "Epoch 110/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0015\n",
            "Epoch 111/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0016\n",
            "Epoch 112/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0016\n",
            "Epoch 113/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0015\n",
            "Epoch 114/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0015\n",
            "Epoch 115/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0015\n",
            "Epoch 116/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0015\n",
            "Epoch 117/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0015\n",
            "Epoch 118/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0014\n",
            "Epoch 119/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0014\n",
            "Epoch 120/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0014\n",
            "Epoch 121/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0014\n",
            "Epoch 122/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0014\n",
            "Epoch 123/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0014\n",
            "Epoch 124/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0014\n",
            "Epoch 125/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0013\n",
            "Epoch 126/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0013\n",
            "Epoch 127/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0013\n",
            "Epoch 128/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0013\n",
            "Epoch 129/200\n",
            "1132/1132 [==============================] - 7s 7ms/step - loss: 0.0013\n",
            "Epoch 130/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0013\n",
            "Epoch 131/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0012\n",
            "Epoch 132/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0012\n",
            "Epoch 133/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0013\n",
            "Epoch 134/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0012\n",
            "Epoch 135/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0012\n",
            "Epoch 136/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0012\n",
            "Epoch 137/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0012\n",
            "Epoch 138/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0012\n",
            "Epoch 139/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0011\n",
            "Epoch 140/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0011\n",
            "Epoch 141/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0012\n",
            "Epoch 142/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0012\n",
            "Epoch 143/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0011\n",
            "Epoch 144/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0011\n",
            "Epoch 145/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0012\n",
            "Epoch 146/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0011\n",
            "Epoch 147/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0010\n",
            "Epoch 148/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0010\n",
            "Epoch 149/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0011\n",
            "Epoch 150/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0011\n",
            "Epoch 151/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0011\n",
            "Epoch 152/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0010\n",
            "Epoch 153/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0011\n",
            "Epoch 154/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0010\n",
            "Epoch 155/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0010\n",
            "Epoch 156/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 9.8752e-04\n",
            "Epoch 157/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 9.9152e-04\n",
            "Epoch 158/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 9.9941e-04\n",
            "Epoch 159/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 9.7013e-04\n",
            "Epoch 160/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 9.3706e-04\n",
            "Epoch 161/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0010\n",
            "Epoch 162/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 9.8396e-04\n",
            "Epoch 163/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 9.4733e-04\n",
            "Epoch 164/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 9.8326e-04\n",
            "Epoch 165/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 9.1314e-04\n",
            "Epoch 166/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 9.4457e-04\n",
            "Epoch 167/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 9.0959e-04\n",
            "Epoch 168/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 9.3996e-04\n",
            "Epoch 169/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 9.2054e-04\n",
            "Epoch 170/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 8.8165e-04\n",
            "Epoch 171/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 8.8119e-04\n",
            "Epoch 172/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 9.0788e-04\n",
            "Epoch 173/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 9.0721e-04\n",
            "Epoch 174/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 8.8920e-04\n",
            "Epoch 175/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 8.5543e-04\n",
            "Epoch 176/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 8.6894e-04\n",
            "Epoch 177/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 8.7022e-04\n",
            "Epoch 178/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 8.5571e-04\n",
            "Epoch 179/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 8.4906e-04\n",
            "Epoch 180/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 8.7257e-04\n",
            "Epoch 181/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 8.2404e-04\n",
            "Epoch 182/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 8.0579e-04\n",
            "Epoch 183/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 8.5283e-04\n",
            "Epoch 184/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 8.2644e-04\n",
            "Epoch 185/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 8.1236e-04\n",
            "Epoch 186/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 8.1592e-04\n",
            "Epoch 187/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 7.9032e-04\n",
            "Epoch 188/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 7.8766e-04\n",
            "Epoch 189/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 8.2997e-04\n",
            "Epoch 190/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 7.7095e-04\n",
            "Epoch 191/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 7.6328e-04\n",
            "Epoch 192/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 7.7446e-04\n",
            "Epoch 193/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 7.6909e-04\n",
            "Epoch 194/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 7.6648e-04\n",
            "Epoch 195/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 7.6600e-04\n",
            "Epoch 196/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 7.7952e-04\n",
            "Epoch 197/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 7.3183e-04\n",
            "Epoch 198/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 7.3771e-04\n",
            "Epoch 199/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 7.3494e-04\n",
            "Epoch 200/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 7.4020e-04\n",
            "lr = 0.0005\n",
            "acc = 0.8870112657388999\n",
            "--------time QMKDCsgd---------------\n",
            "1426.3151910305023\n",
            "Epoch 1/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.3338\n",
            "Epoch 2/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.1054\n",
            "Epoch 3/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.0149\n",
            "Epoch 4/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.9608\n",
            "Epoch 5/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.9225\n",
            "Epoch 6/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.8929\n",
            "Epoch 7/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.8680\n",
            "Epoch 8/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.8471\n",
            "Epoch 9/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.8281\n",
            "Epoch 10/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.8111\n",
            "Epoch 11/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7955\n",
            "Epoch 12/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7811\n",
            "Epoch 13/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7676\n",
            "Epoch 14/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7551\n",
            "Epoch 15/200\n",
            "1132/1132 [==============================] - 7s 7ms/step - loss: 0.7429\n",
            "Epoch 16/200\n",
            "1132/1132 [==============================] - 7s 7ms/step - loss: 0.7309\n",
            "Epoch 17/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7197\n",
            "Epoch 18/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7094\n",
            "Epoch 19/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.6986\n",
            "Epoch 20/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.6878\n",
            "Epoch 21/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.6780\n",
            "Epoch 22/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.6678\n",
            "Epoch 23/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.6576\n",
            "Epoch 24/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.6480\n",
            "Epoch 25/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.6379\n",
            "Epoch 26/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.6282\n",
            "Epoch 27/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.6185\n",
            "Epoch 28/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.6089\n",
            "Epoch 29/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.5998\n",
            "Epoch 30/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.5902\n",
            "Epoch 31/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.5806\n",
            "Epoch 32/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.5715\n",
            "Epoch 33/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.5618\n",
            "Epoch 34/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.5527\n",
            "Epoch 35/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.5434\n",
            "Epoch 36/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.5345\n",
            "Epoch 37/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.5253\n",
            "Epoch 38/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.5166\n",
            "Epoch 39/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.5078\n",
            "Epoch 40/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.4993\n",
            "Epoch 41/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.4906\n",
            "Epoch 42/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.4824\n",
            "Epoch 43/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.4739\n",
            "Epoch 44/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.4657\n",
            "Epoch 45/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.4576\n",
            "Epoch 46/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.4496\n",
            "Epoch 47/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.4421\n",
            "Epoch 48/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.4341\n",
            "Epoch 49/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.4267\n",
            "Epoch 50/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.4191\n",
            "Epoch 51/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.4123\n",
            "Epoch 52/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.4053\n",
            "Epoch 53/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.3980\n",
            "Epoch 54/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.3914\n",
            "Epoch 55/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.3844\n",
            "Epoch 56/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.3781\n",
            "Epoch 57/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.3718\n",
            "Epoch 58/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.3656\n",
            "Epoch 59/200\n",
            "1132/1132 [==============================] - 7s 7ms/step - loss: 0.3587\n",
            "Epoch 60/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.3533\n",
            "Epoch 61/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.3469\n",
            "Epoch 62/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.3413\n",
            "Epoch 63/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.3353\n",
            "Epoch 64/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.3293\n",
            "Epoch 65/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.3237\n",
            "Epoch 66/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.3183\n",
            "Epoch 67/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.3131\n",
            "Epoch 68/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.3075\n",
            "Epoch 69/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.3027\n",
            "Epoch 70/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.2973\n",
            "Epoch 71/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.2919\n",
            "Epoch 72/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.2869\n",
            "Epoch 73/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.2821\n",
            "Epoch 74/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.2773\n",
            "Epoch 75/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.2720\n",
            "Epoch 76/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.2674\n",
            "Epoch 77/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.2624\n",
            "Epoch 78/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.2580\n",
            "Epoch 79/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.2534\n",
            "Epoch 80/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.2489\n",
            "Epoch 81/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.2448\n",
            "Epoch 82/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.2405\n",
            "Epoch 83/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.2362\n",
            "Epoch 84/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.2319\n",
            "Epoch 85/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.2276\n",
            "Epoch 86/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.2237\n",
            "Epoch 87/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.2196\n",
            "Epoch 88/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.2156\n",
            "Epoch 89/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.2112\n",
            "Epoch 90/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.2078\n",
            "Epoch 91/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.2038\n",
            "Epoch 92/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.1998\n",
            "Epoch 93/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.1966\n",
            "Epoch 94/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.1926\n",
            "Epoch 95/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.1888\n",
            "Epoch 96/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.1854\n",
            "Epoch 97/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.1818\n",
            "Epoch 98/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.1783\n",
            "Epoch 99/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.1745\n",
            "Epoch 100/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.1712\n",
            "Epoch 101/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.1679\n",
            "Epoch 102/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.1643\n",
            "Epoch 103/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.1611\n",
            "Epoch 104/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.1580\n",
            "Epoch 105/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.1548\n",
            "Epoch 106/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.1513\n",
            "Epoch 107/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.1484\n",
            "Epoch 108/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.1453\n",
            "Epoch 109/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.1421\n",
            "Epoch 110/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.1392\n",
            "Epoch 111/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.1362\n",
            "Epoch 112/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.1337\n",
            "Epoch 113/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.1308\n",
            "Epoch 114/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.1279\n",
            "Epoch 115/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.1254\n",
            "Epoch 116/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.1231\n",
            "Epoch 117/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.1202\n",
            "Epoch 118/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.1178\n",
            "Epoch 119/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.1152\n",
            "Epoch 120/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.1127\n",
            "Epoch 121/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.1101\n",
            "Epoch 122/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.1080\n",
            "Epoch 123/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.1059\n",
            "Epoch 124/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.1032\n",
            "Epoch 125/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.1012\n",
            "Epoch 126/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0989\n",
            "Epoch 127/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0968\n",
            "Epoch 128/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0949\n",
            "Epoch 129/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0926\n",
            "Epoch 130/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0906\n",
            "Epoch 131/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0886\n",
            "Epoch 132/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0868\n",
            "Epoch 133/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0849\n",
            "Epoch 134/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0828\n",
            "Epoch 135/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0810\n",
            "Epoch 136/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0793\n",
            "Epoch 137/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0777\n",
            "Epoch 138/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0760\n",
            "Epoch 139/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0744\n",
            "Epoch 140/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0725\n",
            "Epoch 141/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0711\n",
            "Epoch 142/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0698\n",
            "Epoch 143/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0681\n",
            "Epoch 144/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0666\n",
            "Epoch 145/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0651\n",
            "Epoch 146/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0638\n",
            "Epoch 147/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0622\n",
            "Epoch 148/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0611\n",
            "Epoch 149/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0601\n",
            "Epoch 150/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0584\n",
            "Epoch 151/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0572\n",
            "Epoch 152/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0558\n",
            "Epoch 153/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0548\n",
            "Epoch 154/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0537\n",
            "Epoch 155/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0524\n",
            "Epoch 156/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0515\n",
            "Epoch 157/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0503\n",
            "Epoch 158/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0492\n",
            "Epoch 159/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0483\n",
            "Epoch 160/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0471\n",
            "Epoch 161/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0463\n",
            "Epoch 162/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0453\n",
            "Epoch 163/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0444\n",
            "Epoch 164/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0436\n",
            "Epoch 165/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0427\n",
            "Epoch 166/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0416\n",
            "Epoch 167/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0408\n",
            "Epoch 168/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0400\n",
            "Epoch 169/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0392\n",
            "Epoch 170/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0385\n",
            "Epoch 171/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0377\n",
            "Epoch 172/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0371\n",
            "Epoch 173/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0363\n",
            "Epoch 174/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0356\n",
            "Epoch 175/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0348\n",
            "Epoch 176/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0342\n",
            "Epoch 177/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0336\n",
            "Epoch 178/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0329\n",
            "Epoch 179/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0322\n",
            "Epoch 180/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0316\n",
            "Epoch 181/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0310\n",
            "Epoch 182/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0304\n",
            "Epoch 183/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0299\n",
            "Epoch 184/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0293\n",
            "Epoch 185/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0288\n",
            "Epoch 186/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0282\n",
            "Epoch 187/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0278\n",
            "Epoch 188/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0272\n",
            "Epoch 189/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0267\n",
            "Epoch 190/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0261\n",
            "Epoch 191/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0257\n",
            "Epoch 192/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0253\n",
            "Epoch 193/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0249\n",
            "Epoch 194/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0243\n",
            "Epoch 195/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0239\n",
            "Epoch 196/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0236\n",
            "Epoch 197/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0231\n",
            "Epoch 198/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0226\n",
            "Epoch 199/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0223\n",
            "Epoch 200/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.0220\n",
            "lr = 5e-05\n",
            "acc = 0.8680141373978352\n",
            "--------time QMKDCsgd---------------\n",
            "1389.9682803153992\n",
            "Epoch 1/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.5533\n",
            "Epoch 2/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.4609\n",
            "Epoch 3/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.3852\n",
            "Epoch 4/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.3330\n",
            "Epoch 5/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.2959\n",
            "Epoch 6/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.2651\n",
            "Epoch 7/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.2381\n",
            "Epoch 8/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.2141\n",
            "Epoch 9/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.1927\n",
            "Epoch 10/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.1732\n",
            "Epoch 11/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.1558\n",
            "Epoch 12/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.1397\n",
            "Epoch 13/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.1251\n",
            "Epoch 14/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.1116\n",
            "Epoch 15/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.0993\n",
            "Epoch 16/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.0879\n",
            "Epoch 17/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.0771\n",
            "Epoch 18/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.0671\n",
            "Epoch 19/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.0578\n",
            "Epoch 20/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.0490\n",
            "Epoch 21/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.0408\n",
            "Epoch 22/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.0330\n",
            "Epoch 23/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.0256\n",
            "Epoch 24/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.0186\n",
            "Epoch 25/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.0119\n",
            "Epoch 26/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.0056\n",
            "Epoch 27/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.9995\n",
            "Epoch 28/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.9937\n",
            "Epoch 29/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.9880\n",
            "Epoch 30/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.9828\n",
            "Epoch 31/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.9777\n",
            "Epoch 32/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.9727\n",
            "Epoch 33/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.9680\n",
            "Epoch 34/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.9634\n",
            "Epoch 35/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.9590\n",
            "Epoch 36/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.9546\n",
            "Epoch 37/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.9504\n",
            "Epoch 38/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.9464\n",
            "Epoch 39/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.9426\n",
            "Epoch 40/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.9387\n",
            "Epoch 41/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.9351\n",
            "Epoch 42/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.9315\n",
            "Epoch 43/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.9280\n",
            "Epoch 44/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.9246\n",
            "Epoch 45/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.9213\n",
            "Epoch 46/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.9182\n",
            "Epoch 47/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.9148\n",
            "Epoch 48/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.9119\n",
            "Epoch 49/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.9089\n",
            "Epoch 50/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.9059\n",
            "Epoch 51/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.9030\n",
            "Epoch 52/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.9002\n",
            "Epoch 53/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.8974\n",
            "Epoch 54/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.8947\n",
            "Epoch 55/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.8919\n",
            "Epoch 56/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.8894\n",
            "Epoch 57/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.8867\n",
            "Epoch 58/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.8842\n",
            "Epoch 59/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.8817\n",
            "Epoch 60/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.8793\n",
            "Epoch 61/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.8768\n",
            "Epoch 62/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.8745\n",
            "Epoch 63/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.8722\n",
            "Epoch 64/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.8699\n",
            "Epoch 65/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.8675\n",
            "Epoch 66/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.8654\n",
            "Epoch 67/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.8632\n",
            "Epoch 68/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.8610\n",
            "Epoch 69/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.8588\n",
            "Epoch 70/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.8566\n",
            "Epoch 71/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.8547\n",
            "Epoch 72/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.8526\n",
            "Epoch 73/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.8506\n",
            "Epoch 74/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.8485\n",
            "Epoch 75/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.8465\n",
            "Epoch 76/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.8446\n",
            "Epoch 77/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.8426\n",
            "Epoch 78/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.8407\n",
            "Epoch 79/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.8387\n",
            "Epoch 80/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.8369\n",
            "Epoch 81/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.8350\n",
            "Epoch 82/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.8332\n",
            "Epoch 83/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.8313\n",
            "Epoch 84/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.8297\n",
            "Epoch 85/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.8279\n",
            "Epoch 86/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.8261\n",
            "Epoch 87/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.8242\n",
            "Epoch 88/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.8226\n",
            "Epoch 89/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.8209\n",
            "Epoch 90/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.8191\n",
            "Epoch 91/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.8175\n",
            "Epoch 92/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.8158\n",
            "Epoch 93/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.8142\n",
            "Epoch 94/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.8125\n",
            "Epoch 95/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.8109\n",
            "Epoch 96/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.8093\n",
            "Epoch 97/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.8077\n",
            "Epoch 98/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.8061\n",
            "Epoch 99/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.8046\n",
            "Epoch 100/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.8030\n",
            "Epoch 101/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.8015\n",
            "Epoch 102/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7999\n",
            "Epoch 103/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7984\n",
            "Epoch 104/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7969\n",
            "Epoch 105/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7954\n",
            "Epoch 106/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7939\n",
            "Epoch 107/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7924\n",
            "Epoch 108/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7909\n",
            "Epoch 109/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7894\n",
            "Epoch 110/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7880\n",
            "Epoch 111/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7865\n",
            "Epoch 112/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7852\n",
            "Epoch 113/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7838\n",
            "Epoch 114/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7823\n",
            "Epoch 115/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7810\n",
            "Epoch 116/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7796\n",
            "Epoch 117/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7782\n",
            "Epoch 118/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7768\n",
            "Epoch 119/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7755\n",
            "Epoch 120/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7741\n",
            "Epoch 121/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7727\n",
            "Epoch 122/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7714\n",
            "Epoch 123/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7701\n",
            "Epoch 124/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7687\n",
            "Epoch 125/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7675\n",
            "Epoch 126/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7661\n",
            "Epoch 127/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7648\n",
            "Epoch 128/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7635\n",
            "Epoch 129/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7622\n",
            "Epoch 130/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7610\n",
            "Epoch 131/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7597\n",
            "Epoch 132/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7584\n",
            "Epoch 133/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7572\n",
            "Epoch 134/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7559\n",
            "Epoch 135/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7547\n",
            "Epoch 136/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7534\n",
            "Epoch 137/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7521\n",
            "Epoch 138/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7510\n",
            "Epoch 139/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7496\n",
            "Epoch 140/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7484\n",
            "Epoch 141/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7474\n",
            "Epoch 142/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7461\n",
            "Epoch 143/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7448\n",
            "Epoch 144/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7436\n",
            "Epoch 145/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7425\n",
            "Epoch 146/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7412\n",
            "Epoch 147/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7401\n",
            "Epoch 148/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7390\n",
            "Epoch 149/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7377\n",
            "Epoch 150/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7366\n",
            "Epoch 151/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7354\n",
            "Epoch 152/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7342\n",
            "Epoch 153/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7331\n",
            "Epoch 154/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7319\n",
            "Epoch 155/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7308\n",
            "Epoch 156/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7297\n",
            "Epoch 157/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7285\n",
            "Epoch 158/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7274\n",
            "Epoch 159/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7263\n",
            "Epoch 160/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7251\n",
            "Epoch 161/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7240\n",
            "Epoch 162/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7229\n",
            "Epoch 163/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7218\n",
            "Epoch 164/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7206\n",
            "Epoch 165/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7195\n",
            "Epoch 166/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7185\n",
            "Epoch 167/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7174\n",
            "Epoch 168/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7162\n",
            "Epoch 169/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7152\n",
            "Epoch 170/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7140\n",
            "Epoch 171/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7130\n",
            "Epoch 172/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7119\n",
            "Epoch 173/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7109\n",
            "Epoch 174/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7097\n",
            "Epoch 175/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7086\n",
            "Epoch 176/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7076\n",
            "Epoch 177/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7065\n",
            "Epoch 178/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7054\n",
            "Epoch 179/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7044\n",
            "Epoch 180/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7033\n",
            "Epoch 181/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7022\n",
            "Epoch 182/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7012\n",
            "Epoch 183/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.7002\n",
            "Epoch 184/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.6991\n",
            "Epoch 185/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.6980\n",
            "Epoch 186/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.6970\n",
            "Epoch 187/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.6960\n",
            "Epoch 188/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.6949\n",
            "Epoch 189/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.6938\n",
            "Epoch 190/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.6929\n",
            "Epoch 191/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.6918\n",
            "Epoch 192/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.6908\n",
            "Epoch 193/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.6898\n",
            "Epoch 194/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.6887\n",
            "Epoch 195/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.6878\n",
            "Epoch 196/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.6867\n",
            "Epoch 197/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.6856\n",
            "Epoch 198/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.6846\n",
            "Epoch 199/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.6836\n",
            "Epoch 200/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 0.6826\n",
            "lr = 5e-06\n",
            "acc = 0.7266401590457257\n",
            "--------time QMKDCsgd---------------\n",
            "1392.1024475097656\n",
            "Epoch 1/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.6034\n",
            "Epoch 2/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.5910\n",
            "Epoch 3/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.5792\n",
            "Epoch 4/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.5679\n",
            "Epoch 5/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.5572\n",
            "Epoch 6/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.5470\n",
            "Epoch 7/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.5372\n",
            "Epoch 8/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.5277\n",
            "Epoch 9/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.5186\n",
            "Epoch 10/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.5098\n",
            "Epoch 11/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.5012\n",
            "Epoch 12/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.4928\n",
            "Epoch 13/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.4846\n",
            "Epoch 14/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.4765\n",
            "Epoch 15/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.4686\n",
            "Epoch 16/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.4609\n",
            "Epoch 17/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.4534\n",
            "Epoch 18/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.4461\n",
            "Epoch 19/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.4390\n",
            "Epoch 20/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.4321\n",
            "Epoch 21/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.4254\n",
            "Epoch 22/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.4190\n",
            "Epoch 23/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.4128\n",
            "Epoch 24/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.4068\n",
            "Epoch 25/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.4011\n",
            "Epoch 26/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.3956\n",
            "Epoch 27/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.3903\n",
            "Epoch 28/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.3853\n",
            "Epoch 29/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.3804\n",
            "Epoch 30/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.3758\n",
            "Epoch 31/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.3714\n",
            "Epoch 32/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.3671\n",
            "Epoch 33/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.3629\n",
            "Epoch 34/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.3589\n",
            "Epoch 35/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.3549\n",
            "Epoch 36/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.3511\n",
            "Epoch 37/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.3474\n",
            "Epoch 38/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.3437\n",
            "Epoch 39/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.3401\n",
            "Epoch 40/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.3365\n",
            "Epoch 41/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.3330\n",
            "Epoch 42/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.3296\n",
            "Epoch 43/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.3262\n",
            "Epoch 44/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.3228\n",
            "Epoch 45/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.3195\n",
            "Epoch 46/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.3163\n",
            "Epoch 47/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.3130\n",
            "Epoch 48/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.3098\n",
            "Epoch 49/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.3067\n",
            "Epoch 50/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.3035\n",
            "Epoch 51/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.3004\n",
            "Epoch 52/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.2974\n",
            "Epoch 53/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.2944\n",
            "Epoch 54/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.2914\n",
            "Epoch 55/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.2884\n",
            "Epoch 56/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.2855\n",
            "Epoch 57/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.2826\n",
            "Epoch 58/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.2797\n",
            "Epoch 59/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.2768\n",
            "Epoch 60/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.2740\n",
            "Epoch 61/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.2713\n",
            "Epoch 62/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.2685\n",
            "Epoch 63/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.2658\n",
            "Epoch 64/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.2631\n",
            "Epoch 65/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.2605\n",
            "Epoch 66/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.2578\n",
            "Epoch 67/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.2552\n",
            "Epoch 68/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.2526\n",
            "Epoch 69/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.2501\n",
            "Epoch 70/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.2476\n",
            "Epoch 71/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.2451\n",
            "Epoch 72/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.2426\n",
            "Epoch 73/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.2402\n",
            "Epoch 74/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.2378\n",
            "Epoch 75/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.2354\n",
            "Epoch 76/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.2330\n",
            "Epoch 77/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.2307\n",
            "Epoch 78/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.2283\n",
            "Epoch 79/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.2261\n",
            "Epoch 80/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.2238\n",
            "Epoch 81/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.2215\n",
            "Epoch 82/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.2193\n",
            "Epoch 83/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.2171\n",
            "Epoch 84/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.2150\n",
            "Epoch 85/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.2128\n",
            "Epoch 86/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.2107\n",
            "Epoch 87/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.2086\n",
            "Epoch 88/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.2065\n",
            "Epoch 89/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.2044\n",
            "Epoch 90/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.2024\n",
            "Epoch 91/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.2004\n",
            "Epoch 92/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.1984\n",
            "Epoch 93/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.1964\n",
            "Epoch 94/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.1944\n",
            "Epoch 95/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.1925\n",
            "Epoch 96/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.1905\n",
            "Epoch 97/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.1886\n",
            "Epoch 98/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.1867\n",
            "Epoch 99/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.1849\n",
            "Epoch 100/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.1830\n",
            "Epoch 101/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.1812\n",
            "Epoch 102/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.1794\n",
            "Epoch 103/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.1776\n",
            "Epoch 104/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.1758\n",
            "Epoch 105/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.1740\n",
            "Epoch 106/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.1723\n",
            "Epoch 107/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.1706\n",
            "Epoch 108/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.1689\n",
            "Epoch 109/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.1671\n",
            "Epoch 110/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.1655\n",
            "Epoch 111/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.1638\n",
            "Epoch 112/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.1621\n",
            "Epoch 113/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.1605\n",
            "Epoch 114/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.1589\n",
            "Epoch 115/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.1573\n",
            "Epoch 116/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.1557\n",
            "Epoch 117/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.1541\n",
            "Epoch 118/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.1525\n",
            "Epoch 119/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.1510\n",
            "Epoch 120/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.1494\n",
            "Epoch 121/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.1479\n",
            "Epoch 122/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.1464\n",
            "Epoch 123/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.1449\n",
            "Epoch 124/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.1434\n",
            "Epoch 125/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.1420\n",
            "Epoch 126/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.1405\n",
            "Epoch 127/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.1391\n",
            "Epoch 128/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.1376\n",
            "Epoch 129/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.1362\n",
            "Epoch 130/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.1348\n",
            "Epoch 131/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.1334\n",
            "Epoch 132/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.1320\n",
            "Epoch 133/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.1306\n",
            "Epoch 134/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.1293\n",
            "Epoch 135/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.1279\n",
            "Epoch 136/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.1266\n",
            "Epoch 137/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.1252\n",
            "Epoch 138/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.1239\n",
            "Epoch 139/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.1226\n",
            "Epoch 140/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.1213\n",
            "Epoch 141/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.1200\n",
            "Epoch 142/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.1187\n",
            "Epoch 143/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.1175\n",
            "Epoch 144/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.1162\n",
            "Epoch 145/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.1150\n",
            "Epoch 146/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.1137\n",
            "Epoch 147/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.1125\n",
            "Epoch 148/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.1113\n",
            "Epoch 149/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.1101\n",
            "Epoch 150/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.1089\n",
            "Epoch 151/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.1077\n",
            "Epoch 152/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.1065\n",
            "Epoch 153/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.1053\n",
            "Epoch 154/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.1041\n",
            "Epoch 155/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.1030\n",
            "Epoch 156/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.1018\n",
            "Epoch 157/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.1007\n",
            "Epoch 158/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.0996\n",
            "Epoch 159/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.0984\n",
            "Epoch 160/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.0973\n",
            "Epoch 161/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.0962\n",
            "Epoch 162/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.0951\n",
            "Epoch 163/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.0940\n",
            "Epoch 164/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.0929\n",
            "Epoch 165/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.0918\n",
            "Epoch 166/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.0908\n",
            "Epoch 167/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.0897\n",
            "Epoch 168/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.0887\n",
            "Epoch 169/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.0876\n",
            "Epoch 170/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.0866\n",
            "Epoch 171/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.0855\n",
            "Epoch 172/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.0845\n",
            "Epoch 173/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.0835\n",
            "Epoch 174/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.0825\n",
            "Epoch 175/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.0815\n",
            "Epoch 176/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.0805\n",
            "Epoch 177/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.0795\n",
            "Epoch 178/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.0785\n",
            "Epoch 179/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.0775\n",
            "Epoch 180/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.0765\n",
            "Epoch 181/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.0756\n",
            "Epoch 182/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.0746\n",
            "Epoch 183/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.0737\n",
            "Epoch 184/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.0727\n",
            "Epoch 185/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.0718\n",
            "Epoch 186/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.0708\n",
            "Epoch 187/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.0699\n",
            "Epoch 188/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.0690\n",
            "Epoch 189/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.0680\n",
            "Epoch 190/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.0671\n",
            "Epoch 191/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.0662\n",
            "Epoch 192/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.0653\n",
            "Epoch 193/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.0644\n",
            "Epoch 194/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.0635\n",
            "Epoch 195/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.0626\n",
            "Epoch 196/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.0617\n",
            "Epoch 197/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.0609\n",
            "Epoch 198/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.0600\n",
            "Epoch 199/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.0591\n",
            "Epoch 200/200\n",
            "1132/1132 [==============================] - 7s 6ms/step - loss: 1.0582\n",
            "lr = 5e-07\n",
            "acc = 0.5906781533024078\n",
            "--------time QMKDCsgd---------------\n",
            "1413.7022478580475\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "id": "gWbNksuBa401",
        "outputId": "a2a72c9b-cc2b-41c6-a516-622d2a26affd"
      },
      "source": [
        "#Best lr set_lr_best = [0.0005, 0.0025, 0.005]\n",
        "set_lr_a = [0.005, 0.0005, 0.00005, 0.000005, 0.0000005]\n",
        "set_lr_b = [0.0005, 0.005, 0.05, 0.5, 1, 10]\n",
        "\n",
        "for lr in set_lr_b:\n",
        "  exp_time = time()\n",
        "  qmkdc1_dig = models.QMKDClassifierSGD(input_dim=1000, dim_x=component_dim, num_classes=num_classes, num_eig=num_eig, gamma=0.2, random_state=None)\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "  qmkdc1_dig.compile(optimizer, loss=tf.keras.losses.CategoricalCrossentropy())\n",
        "  y_train_bin = tf.reshape(tf.keras.backend.one_hot(y_train, num_classes), (-1,num_classes))\n",
        "  qmkdc1_dig.fit(X_train, y_train_bin, epochs=200)\n",
        "  out = qmkdc1_dig.predict(X_test)\n",
        "  print(f'lr = {lr}')\n",
        "  print(f'acc = {accuracy_score(y_test, np.argmax(out, axis=1))}')\n",
        "  print(\"--------time QMKDCsgd---------------\")\n",
        "  print(time() - exp_time)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-0fc841977cc3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mset_lr_b\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mexp_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0mqmkdc1_dig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQMKDClassifierSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_x\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcomponent_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_eig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_eig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'time' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjskPpGWLMAv",
        "outputId": "c6263ad7-e3fa-4901-a13e-74afed0646fd"
      },
      "source": [
        "#Best lr set_lr_best = [0.0005, 0.0025, 0.005]\n",
        "set_eig_a = [1, 5, 10, 20, 25, 100, 250, 500]\n",
        "\n",
        "for eigr in set_eig_a:\n",
        "  exp_time = time()\n",
        "  qmkdc1_dig = models.QMKDClassifierSGD(input_dim=1000, dim_x=500, num_classes=4, num_eig=eigr, gamma=0.2, random_state=None)\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
        "  qmkdc1_dig.compile(optimizer, loss=tf.keras.losses.CategoricalCrossentropy())\n",
        "  y_train_bin = tf.reshape(tf.keras.backend.one_hot(y_train, 4), (-1,4))\n",
        "  qmkdc1_dig.fit(X_train, y_train_bin, epochs=10)\n",
        "  out = qmkdc1_dig.predict(X_test)\n",
        "  print(f'eig = {eigr}')\n",
        "  print(f'acc = {accuracy_score(y_test, np.argmax(out, axis=1))}')\n",
        "  print(\"--------time QMKDCsgd---------------\")\n",
        "  print(time() - exp_time)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 9.5417e-08\n",
            "Epoch 2/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 9.5417e-08\n",
            "Epoch 3/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 9.5417e-08\n",
            "Epoch 4/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 9.5417e-08\n",
            "Epoch 5/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 9.5417e-08\n",
            "Epoch 6/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 9.5417e-08\n",
            "Epoch 7/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 9.5417e-08\n",
            "Epoch 8/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 9.5417e-08\n",
            "Epoch 9/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 9.5417e-08\n",
            "Epoch 10/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 9.5417e-08\n",
            "eig = 1\n",
            "acc = 0.1949414623370886\n",
            "--------time QMKDCsgd---------------\n",
            "44.52291703224182\n",
            "Epoch 1/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.4528\n",
            "Epoch 2/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.1534\n",
            "Epoch 3/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.0431\n",
            "Epoch 4/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.9302\n",
            "Epoch 5/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8229\n",
            "Epoch 6/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7418\n",
            "Epoch 7/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6979\n",
            "Epoch 8/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6110\n",
            "Epoch 9/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5322\n",
            "Epoch 10/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4759\n",
            "eig = 5\n",
            "acc = 0.3126794786834548\n",
            "--------time QMKDCsgd---------------\n",
            "47.725860357284546\n",
            "Epoch 1/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.2074\n",
            "Epoch 2/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.9289\n",
            "Epoch 3/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7982\n",
            "Epoch 4/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7029\n",
            "Epoch 5/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6236\n",
            "Epoch 6/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5549\n",
            "Epoch 7/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4955\n",
            "Epoch 8/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.4427\n",
            "Epoch 9/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4104\n",
            "Epoch 10/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.3963\n",
            "eig = 10\n",
            "acc = 0.3441572785509167\n",
            "--------time QMKDCsgd---------------\n",
            "47.492961406707764\n",
            "Epoch 1/10\n",
            "1132/1132 [==============================] - 5s 5ms/step - loss: 1.0879\n",
            "Epoch 2/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.8553\n",
            "Epoch 3/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7379\n",
            "Epoch 4/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6494\n",
            "Epoch 5/10\n",
            "1132/1132 [==============================] - 5s 5ms/step - loss: 0.5733\n",
            "Epoch 6/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5074\n",
            "Epoch 7/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4521\n",
            "Epoch 8/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4046\n",
            "Epoch 9/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.3613\n",
            "Epoch 10/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.3248\n",
            "eig = 20\n",
            "acc = 0.3704440026507621\n",
            "--------time QMKDCsgd---------------\n",
            "50.51721215248108\n",
            "Epoch 1/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.0819\n",
            "Epoch 2/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.8572\n",
            "Epoch 3/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7417\n",
            "Epoch 4/10\n",
            "1132/1132 [==============================] - 5s 5ms/step - loss: 0.6519\n",
            "Epoch 5/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5764\n",
            "Epoch 6/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5121\n",
            "Epoch 7/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4545\n",
            "Epoch 8/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4006\n",
            "Epoch 9/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.3615\n",
            "Epoch 10/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.3274\n",
            "eig = 25\n",
            "acc = 0.35365584272144907\n",
            "--------time QMKDCsgd---------------\n",
            "49.741976499557495\n",
            "Epoch 1/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.0424\n",
            "Epoch 2/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8617\n",
            "Epoch 3/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7495\n",
            "Epoch 4/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6578\n",
            "Epoch 5/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5761\n",
            "Epoch 6/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5048\n",
            "Epoch 7/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4410\n",
            "Epoch 8/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.3894\n",
            "Epoch 9/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.3497\n",
            "Epoch 10/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.3164\n",
            "eig = 100\n",
            "acc = 0.361939474265518\n",
            "--------time QMKDCsgd---------------\n",
            "48.45318627357483\n",
            "Epoch 1/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.0312\n",
            "Epoch 2/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8629\n",
            "Epoch 3/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7450\n",
            "Epoch 4/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6421\n",
            "Epoch 5/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5508\n",
            "Epoch 6/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.4723\n",
            "Epoch 7/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.4084\n",
            "Epoch 8/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.3571\n",
            "Epoch 9/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.3154\n",
            "Epoch 10/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.2873\n",
            "eig = 250\n",
            "acc = 0.3662469626684338\n",
            "--------time QMKDCsgd---------------\n",
            "46.7802574634552\n",
            "Epoch 1/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.0302\n",
            "Epoch 2/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.8647\n",
            "Epoch 3/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7447\n",
            "Epoch 4/10\n",
            "1132/1132 [==============================] - 5s 5ms/step - loss: 0.6405\n",
            "Epoch 5/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5471\n",
            "Epoch 6/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4672\n",
            "Epoch 7/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4017\n",
            "Epoch 8/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.3476\n",
            "Epoch 9/10\n",
            "1132/1132 [==============================] - 5s 5ms/step - loss: 0.3086\n",
            "Epoch 10/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.2760\n",
            "eig = 500\n",
            "acc = 0.363264855312569\n",
            "--------time QMKDCsgd---------------\n",
            "50.55093836784363\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfeU_05CNN9U",
        "outputId": "e8805083-25af-4225-fc54-618174a931a5"
      },
      "source": [
        "#Best lr set_lr_best = [0.0005, 0.0025, 0.005]\n",
        "set_lr_a = [0.005, 0.0005, 0.00005, 0.000005, 0.0000005]\n",
        "set_lr_b = [0.00005, 0.0005, 0.5, 10]\n",
        "set_lr_c = [0.000005, 0.00005, 1, 10]\n",
        "set_lr_d = [0.0005, 0.005, 0.05, 0.5, 1, 10]\n",
        "\n",
        "for lr in set_lr_d:\n",
        "  exp_time = time()\n",
        "  qmkdc1_dig = models.QMKDClassifierSGD(input_dim=1000, dim_x=500, num_classes=4, num_eig=50, gamma=0.2, random_state=None)\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "  qmkdc1_dig.compile(optimizer, loss=tf.keras.losses.CategoricalCrossentropy())\n",
        "  y_train_bin = tf.reshape(tf.keras.backend.one_hot(y_train, 4), (-1,4))\n",
        "  qmkdc1_dig.fit(X_train, y_train_bin, epochs=10)\n",
        "  out = qmkdc1_dig.predict(X_test)\n",
        "  print(f'lr = {lr}')\n",
        "  print(f'acc = {accuracy_score(y_test, np.argmax(out, axis=1))}')\n",
        "  print(\"--------time QMKDCsgd---------------\")\n",
        "  print(time() - exp_time)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.0515\n",
            "Epoch 2/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.8582\n",
            "Epoch 3/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7443\n",
            "Epoch 4/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6521\n",
            "Epoch 5/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5713\n",
            "Epoch 6/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5000\n",
            "Epoch 7/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4426\n",
            "Epoch 8/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.3935\n",
            "Epoch 9/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.3543\n",
            "Epoch 10/10\n",
            "1132/1132 [==============================] - 5s 5ms/step - loss: 0.3210\n",
            "lr = 0.0005\n",
            "acc = 0.3600618511155291\n",
            "--------time QMKDCsgd---------------\n",
            "48.04964995384216\n",
            "Epoch 1/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.0233\n",
            "Epoch 2/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.9391\n",
            "Epoch 3/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.9028\n",
            "Epoch 4/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.8848\n",
            "Epoch 5/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.8602\n",
            "Epoch 6/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.8547\n",
            "Epoch 7/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8423\n",
            "Epoch 8/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8340\n",
            "Epoch 9/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8354\n",
            "Epoch 10/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8213\n",
            "lr = 0.005\n",
            "acc = 0.3554230174508505\n",
            "--------time QMKDCsgd---------------\n",
            "46.33331274986267\n",
            "Epoch 1/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.1583\n",
            "Epoch 2/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.1694\n",
            "Epoch 3/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.1686\n",
            "Epoch 4/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.1703\n",
            "Epoch 5/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.1715\n",
            "Epoch 6/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.1637\n",
            "Epoch 7/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.1677\n",
            "Epoch 8/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.1676\n",
            "Epoch 9/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.1682\n",
            "Epoch 10/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.1687\n",
            "lr = 0.05\n",
            "acc = 0.2009056770488182\n",
            "--------time QMKDCsgd---------------\n",
            "48.690396308898926\n",
            "Epoch 1/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.1616\n",
            "Epoch 2/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.1566\n",
            "Epoch 3/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.1587\n",
            "Epoch 4/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.1531\n",
            "Epoch 5/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.1560\n",
            "Epoch 6/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.1562\n",
            "Epoch 7/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.1563\n",
            "Epoch 8/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.1565\n",
            "Epoch 9/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.1525\n",
            "Epoch 10/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.1544\n",
            "lr = 0.5\n",
            "acc = 0.20311464546056993\n",
            "--------time QMKDCsgd---------------\n",
            "47.577587366104126\n",
            "Epoch 1/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.1705\n",
            "Epoch 2/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.1672\n",
            "Epoch 3/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.1664\n",
            "Epoch 4/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.1671\n",
            "Epoch 5/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.1669\n",
            "Epoch 6/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.1644\n",
            "Epoch 7/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.1643\n",
            "Epoch 8/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.1673\n",
            "Epoch 9/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.1667\n",
            "Epoch 10/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.1645\n",
            "lr = 1\n",
            "acc = 0.20101612546940578\n",
            "--------time QMKDCsgd---------------\n",
            "48.11946368217468\n",
            "Epoch 1/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.1729\n",
            "Epoch 2/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.1630\n",
            "Epoch 3/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.1657\n",
            "Epoch 4/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.1675\n",
            "Epoch 5/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.1691\n",
            "Epoch 6/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.1691\n",
            "Epoch 7/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.1716\n",
            "Epoch 8/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.1689\n",
            "Epoch 9/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.1689\n",
            "Epoch 10/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.1666\n",
            "lr = 10\n",
            "acc = 0.20455047492820852\n",
            "--------time QMKDCsgd---------------\n",
            "47.95047688484192\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zz3MZGUjILnH",
        "outputId": "23bec91f-08a3-40ff-855f-0beec20c7cf8"
      },
      "source": [
        "#Best lr set_lr_best = [0.0005, 0.0025, 0.005]\n",
        "set_lr_a = [0.005, 0.0005, 0.00005, 0.000005, 0.0000005]\n",
        "set_lr_b = [0.00005, 0.0005, 0.5, 10]\n",
        "set_lr_c = [0.000005, 0.00005, 1, 10]\n",
        "set_lr_d = [0.0005, 0.005, 0.05, 0.5, 1, 10]\n",
        "\n",
        "for lr in set_lr_a:\n",
        "  exp_time = time()\n",
        "  qmkdc1_dig = models.QMKDClassifierSGD(input_dim=1000, dim_x=1000, num_classes=4, num_eig=50, gamma=100, random_state=17)\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "  qmkdc1_dig.compile(optimizer, loss=tf.keras.losses.CategoricalCrossentropy())\n",
        "  y_train_bin = tf.reshape(tf.keras.backend.one_hot(y_train, 4), (-1,4))\n",
        "  qmkdc1_dig.fit(X_train, y_train_bin, epochs=10)\n",
        "  out = qmkdc1_dig.predict(X_test)\n",
        "  print(f'lr = {lr}')\n",
        "  print(f'acc = {accuracy_score(y_test, np.argmax(out, axis=1))}')\n",
        "  print(\"--------time QMKDCsgd---------------\")\n",
        "  print(time() - exp_time)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.1106\n",
            "Epoch 2/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.9980\n",
            "Epoch 3/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.9507\n",
            "Epoch 4/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.9274\n",
            "Epoch 5/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.9067\n",
            "Epoch 6/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8922\n",
            "Epoch 7/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8824\n",
            "Epoch 8/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.8706\n",
            "Epoch 9/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.8634\n",
            "Epoch 10/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8655\n",
            "lr = 0.005\n",
            "acc = 0.2696045946542964\n",
            "--------time QMKDCsgd---------------\n",
            "46.9223415851593\n",
            "Epoch 1/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.1149\n",
            "Epoch 2/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.8736\n",
            "Epoch 3/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7491\n",
            "Epoch 4/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6554\n",
            "Epoch 5/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5720\n",
            "Epoch 6/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4995\n",
            "Epoch 7/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4386\n",
            "Epoch 8/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.3933\n",
            "Epoch 9/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.3613\n",
            "Epoch 10/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.3202\n",
            "lr = 0.0005\n",
            "acc = 0.27711508725425227\n",
            "--------time QMKDCsgd---------------\n",
            "46.82047772407532\n",
            "Epoch 1/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.1389\n",
            "Epoch 2/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.0908\n",
            "Epoch 3/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.0518\n",
            "Epoch 4/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.0168\n",
            "Epoch 5/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.9841\n",
            "Epoch 6/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.9530\n",
            "Epoch 7/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.9236\n",
            "Epoch 8/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.8953\n",
            "Epoch 9/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.8684\n",
            "Epoch 10/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.8428\n",
            "lr = 5e-05\n",
            "acc = 0.2476253589573669\n",
            "--------time QMKDCsgd---------------\n",
            "47.957865715026855\n",
            "Epoch 1/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.1476\n",
            "Epoch 2/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.1415\n",
            "Epoch 3/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.1357\n",
            "Epoch 4/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.1302\n",
            "Epoch 5/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.1248\n",
            "Epoch 6/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.1196\n",
            "Epoch 7/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.1146\n",
            "Epoch 8/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.1096\n",
            "Epoch 9/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.1048\n",
            "Epoch 10/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.1001\n",
            "lr = 5e-06\n",
            "acc = 0.20963110227523746\n",
            "--------time QMKDCsgd---------------\n",
            "46.93932604789734\n",
            "Epoch 1/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.1430\n",
            "Epoch 2/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.1425\n",
            "Epoch 3/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.1420\n",
            "Epoch 4/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.1415\n",
            "Epoch 5/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.1411\n",
            "Epoch 6/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.1406\n",
            "Epoch 7/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.1401\n",
            "Epoch 8/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.1397\n",
            "Epoch 9/10\n",
            "1132/1132 [==============================] - 5s 5ms/step - loss: 1.1392\n",
            "Epoch 10/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.1387\n",
            "lr = 5e-07\n",
            "acc = 0.2009056770488182\n",
            "--------time QMKDCsgd---------------\n",
            "47.643826484680176\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFbwJatkLwp2",
        "outputId": "f0d35964-d277-4526-abcb-aa1a78de2073"
      },
      "source": [
        "set_lr_a = [0.005, 0.0005, 0.00005, 0.000005, 0.0000005]\n",
        "set_lr_b = [0.00005, 0.0005, 0.005, 0.05, 10]\n",
        "set_lr_c = [0.000005, 0.00005, 1, 10]\n",
        "set_lr_d = [0.0005, 0.005, 0.05, 0.5, 1, 10]\n",
        "\n",
        "for lr in set_lr_b:\n",
        "  exp_time = time()\n",
        "  qmkdc1_dig = models.QMKDClassifierSGD(input_dim=1000, dim_x=1000, num_classes=4, num_eig=50, gamma=100, random_state=17)\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "  qmkdc1_dig.compile(optimizer, loss=tf.keras.losses.CategoricalCrossentropy())\n",
        "  y_train_bin = tf.reshape(tf.keras.backend.one_hot(y_train, 4), (-1,4))\n",
        "  qmkdc1_dig.fit(X_train, y_train_bin, epochs=10)\n",
        "  out = qmkdc1_dig.predict(X_test)\n",
        "  print(f'lr = {lr}')\n",
        "  print(f'acc = {accuracy_score(y_test, np.argmax(out, axis=1))}')\n",
        "  print(\"--------time QMKDCsgd---------------\")\n",
        "  print(time() - exp_time)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.1363\n",
            "Epoch 2/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.0888\n",
            "Epoch 3/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.0504\n",
            "Epoch 4/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.0157\n",
            "Epoch 5/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.9834\n",
            "Epoch 6/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.9528\n",
            "Epoch 7/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.9237\n",
            "Epoch 8/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.8957\n",
            "Epoch 9/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.8690\n",
            "Epoch 10/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.8435\n",
            "lr = 5e-05\n",
            "acc = 0.24596863264855312\n",
            "--------time QMKDCsgd---------------\n",
            "47.36651253700256\n",
            "Epoch 1/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.1137\n",
            "Epoch 2/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8753\n",
            "Epoch 3/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7516\n",
            "Epoch 4/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6563\n",
            "Epoch 5/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5727\n",
            "Epoch 6/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4997\n",
            "Epoch 7/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.4407\n",
            "Epoch 8/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.3867\n",
            "Epoch 9/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.3485\n",
            "Epoch 10/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.3183\n",
            "lr = 0.0005\n",
            "acc = 0.2767837419924895\n",
            "--------time QMKDCsgd---------------\n",
            "46.98797821998596\n",
            "Epoch 1/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.1150\n",
            "Epoch 2/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.9987\n",
            "Epoch 3/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.9486\n",
            "Epoch 4/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.9265\n",
            "Epoch 5/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.9074\n",
            "Epoch 6/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.8905\n",
            "Epoch 7/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.8867\n",
            "Epoch 8/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.8741\n",
            "Epoch 9/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8644\n",
            "Epoch 10/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8645\n",
            "lr = 0.005\n",
            "acc = 0.26982549149547164\n",
            "--------time QMKDCsgd---------------\n",
            "47.63798713684082\n",
            "Epoch 1/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.1647\n",
            "Epoch 2/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.1594\n",
            "Epoch 3/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.1589\n",
            "Epoch 4/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.1586\n",
            "Epoch 5/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.1591\n",
            "Epoch 6/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.1619\n",
            "Epoch 7/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.1584\n",
            "Epoch 8/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.1539\n",
            "Epoch 9/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.1567\n",
            "Epoch 10/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.1582\n",
            "lr = 0.05\n",
            "acc = 0.2100728959575878\n",
            "--------time QMKDCsgd---------------\n",
            "49.193336963653564\n",
            "Epoch 1/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.1921\n",
            "Epoch 2/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.1857\n",
            "Epoch 3/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.1866\n",
            "Epoch 4/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.1885\n",
            "Epoch 5/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.1865\n",
            "Epoch 6/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.1899\n",
            "Epoch 7/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.1856\n",
            "Epoch 8/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.1855\n",
            "Epoch 9/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.1848\n",
            "Epoch 10/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.1887\n",
            "lr = 10\n",
            "acc = 0.19847581179589133\n",
            "--------time QMKDCsgd---------------\n",
            "48.47744584083557\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WiwuXgwDGU1m",
        "outputId": "3ff94cb2-fb7a-43d1-fafe-148d03ea1ffe"
      },
      "source": [
        "'''\n",
        "Modified version of https://scikit-learn.org/stable/auto_examples/miscellaneous/plot_kernel_approximation.html#sphx-glr-auto-examples-miscellaneous-plot-kernel-approximation-py\n",
        "'''\n",
        "# Parameters \n",
        "num_comp = 1000\n",
        "log_plot = True\n",
        "num_exps = 1\n",
        "num_eig = 50\n",
        "num_epochs = 3\n",
        "lr = 0.0025\n",
        "\n",
        "# Interval of gamma points for tuning\n",
        "if log_plot == True:\n",
        "  #sample_sizes = np.logspace(-13, -1, num=13, base=2)\n",
        "  sample_sizes = np.logspace(-10, 11, num=21, base=2)\n",
        "else:\n",
        "  sample_sizes = 0.025 * onp.arange(2, 31)\n",
        "\n",
        "# sample_sizes = np.array([2**(-8)]) # Uncomment this in case you want only one gamma\n",
        "\n",
        "# Create numpy arrays to keep track of data\n",
        "num_points = sample_sizes.shape[0]\n",
        "mixed_scores = np.zeros((num_points, num_exps))\n",
        "qmkdc_sgd_scores = np.zeros((num_points, num_exps))\n",
        "#linear_rff_scores = np.zeros((num_points, num_exps))\n",
        "\n",
        "# num_exps is the number of experiments for each point\n",
        "for i in range(num_exps):\n",
        "  # Start the training on the samples\n",
        "  #exp_time = time()\n",
        "  for j in range(num_points):  \n",
        "      exp_time = time()\n",
        "\n",
        "      #'''\n",
        "      #### Lin SVM #############\n",
        "      # feature_map_fourier = RBFSampler(gamma=sample_sizes[j], random_state=(i+1), n_components=num_comp)\n",
        "      # X_ff_train = feature_map_fourier.fit_transform(X_train)\n",
        "      # X_ff_train = X_ff_train/(np.linalg.norm(X_ff_train,axis=1)).reshape(-1,1)\n",
        "      # X_ff_test = feature_map_fourier.transform(X_test)\n",
        "      # X_ff_test = X_ff_test/(np.linalg.norm(X_ff_test,axis=1)).reshape(-1,1)\n",
        "      # # train linear svm over RFF map\n",
        "      # linear_rff_svm = svm.LinearSVC()\n",
        "      # linear_rff_svm.fit(X_ff_train, y_train.ravel())\n",
        "      # # Predict and save the value\n",
        "      # linear_rff_scores[j,i] = linear_rff_svm.score(X_ff_test, y_test.ravel())\n",
        "      # print(\"--------time LinSVM---------------\")\n",
        "      # print(time() - exp_time)\n",
        "      # exp_time = time()\n",
        "      #'''\n",
        "\n",
        "      #### QMKDCsgd ##############\n",
        "      qmkdc1_dig = models.QMKDClassifierSGD(input_dim=100, dim_x=num_comp, num_classes=4, num_eig=num_eig, gamma=sample_sizes[j], random_state=(i+1))\n",
        "      optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "      qmkdc1_dig.compile(optimizer, loss=tf.keras.losses.CategoricalCrossentropy())\n",
        "      y_train_bin = tf.reshape(tf.keras.backend.one_hot(y_train, 4), (-1,4))\n",
        "      qmkdc1_dig.fit(X_train, y_train_bin, epochs=num_epochs)\n",
        "      # history = qmkdc1_dig.fit(X_train, y_train_bin, epochs=num_epochs)  # Uncomment this if you want to plot loss and acc of last training\n",
        "      out = qmkdc1_dig.predict(X_test)\n",
        "      qmkdc_sgd_scores[j,i] = accuracy_score(y_test, np.argmax(out, axis=1))\n",
        "      del qmkdc1_dig\n",
        "      gc.collect()\n",
        "      print(\"--------time QMKDCsgd---------------\")\n",
        "      print(time() - exp_time)\n",
        "  #print(time() - exp_time)\n",
        "\n",
        "# Save the average accuracies and standard deviations in three lists\n",
        "ave_qmkdc_sgd_scores = qmkdc_sgd_scores.mean(axis=1).tolist()\n",
        "# ave_linear_rff_scores = linear_rff_scores.mean(axis=1).tolist()\n",
        "std_qmkdc_sgd_scores = qmkdc_sgd_scores.std(axis=1).tolist()\n",
        "# std_linear_rff_scores = linear_rff_scores.std(axis=1).tolist()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8590\n",
            "Epoch 2/3\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7627\n",
            "Epoch 3/3\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7490\n",
            "--------time QMKDCsgd---------------\n",
            "15.529392719268799\n",
            "Epoch 1/3\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.8493\n",
            "Epoch 2/3\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7663\n",
            "Epoch 3/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7473\n",
            "--------time QMKDCsgd---------------\n",
            "15.325875759124756\n",
            "Epoch 1/3\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.8554\n",
            "Epoch 2/3\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7637\n",
            "Epoch 3/3\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7476\n",
            "--------time QMKDCsgd---------------\n",
            "15.701651811599731\n",
            "Epoch 1/3\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.8437\n",
            "Epoch 2/3\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7578\n",
            "Epoch 3/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7430\n",
            "--------time QMKDCsgd---------------\n",
            "15.279701471328735\n",
            "Epoch 1/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8436\n",
            "Epoch 2/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7609\n",
            "Epoch 3/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7438\n",
            "--------time QMKDCsgd---------------\n",
            "15.10535717010498\n",
            "Epoch 1/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8261\n",
            "Epoch 2/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7505\n",
            "Epoch 3/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7386\n",
            "--------time QMKDCsgd---------------\n",
            "15.073111057281494\n",
            "Epoch 1/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8218\n",
            "Epoch 2/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7458\n",
            "Epoch 3/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7344\n",
            "--------time QMKDCsgd---------------\n",
            "15.029189586639404\n",
            "Epoch 1/3\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.8049\n",
            "Epoch 2/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7356\n",
            "Epoch 3/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7232\n",
            "--------time QMKDCsgd---------------\n",
            "15.219162702560425\n",
            "Epoch 1/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7871\n",
            "Epoch 2/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7321\n",
            "Epoch 3/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7164\n",
            "--------time QMKDCsgd---------------\n",
            "15.11698579788208\n",
            "Epoch 1/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7735\n",
            "Epoch 2/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7201\n",
            "Epoch 3/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7050\n",
            "--------time QMKDCsgd---------------\n",
            "14.948049545288086\n",
            "Epoch 1/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7617\n",
            "Epoch 2/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7101\n",
            "Epoch 3/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6938\n",
            "--------time QMKDCsgd---------------\n",
            "14.947823524475098\n",
            "Epoch 1/3\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7476\n",
            "Epoch 2/3\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6929\n",
            "Epoch 3/3\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6746\n",
            "--------time QMKDCsgd---------------\n",
            "15.798391580581665\n",
            "Epoch 1/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7306\n",
            "Epoch 2/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6713\n",
            "Epoch 3/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6419\n",
            "--------time QMKDCsgd---------------\n",
            "14.938186883926392\n",
            "Epoch 1/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7020\n",
            "Epoch 2/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6230\n",
            "Epoch 3/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5800\n",
            "--------time QMKDCsgd---------------\n",
            "14.797127723693848\n",
            "Epoch 1/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6683\n",
            "Epoch 2/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5449\n",
            "Epoch 3/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.4885\n",
            "--------time QMKDCsgd---------------\n",
            "14.838965654373169\n",
            "Epoch 1/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6698\n",
            "Epoch 2/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5072\n",
            "Epoch 3/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.4583\n",
            "--------time QMKDCsgd---------------\n",
            "14.8014976978302\n",
            "Epoch 1/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7675\n",
            "Epoch 2/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5733\n",
            "Epoch 3/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5326\n",
            "--------time QMKDCsgd---------------\n",
            "14.728968381881714\n",
            "Epoch 1/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.9020\n",
            "Epoch 2/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6867\n",
            "Epoch 3/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6447\n",
            "--------time QMKDCsgd---------------\n",
            "14.832273006439209\n",
            "Epoch 1/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.9890\n",
            "Epoch 2/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7694\n",
            "Epoch 3/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7240\n",
            "--------time QMKDCsgd---------------\n",
            "14.4526686668396\n",
            "Epoch 1/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.0396\n",
            "Epoch 2/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8182\n",
            "Epoch 3/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7732\n",
            "--------time QMKDCsgd---------------\n",
            "15.007417917251587\n",
            "Epoch 1/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.0758\n",
            "Epoch 2/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8579\n",
            "Epoch 3/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8088\n",
            "--------time QMKDCsgd---------------\n",
            "14.654496192932129\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "id": "AqIHkfZMVALg",
        "outputId": "a7986015-7fd9-4d74-e79e-ce924419ff71"
      },
      "source": [
        "# plot the results:\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.errorbar(sample_sizes, ave_qmkdc_sgd_scores, yerr=std_qmkdc_sgd_scores, label=f\"RFF QMKDClassifierSGD n_comp={num_comp} n_eig={num_eig}\")\n",
        "#plt.errorbar(sample_sizes, ave_linear_rff_scores, yerr=std_linear_rff_scores, label=f\"RFF LinearSVM n_comp={num_comp}\")\n",
        "\n",
        "# legends and labels\n",
        "# plt.title(\"QMDensitySGD, QMKDClassifierSGD and LinSVM with RFF on MNIST\")\n",
        "plt.title(\"QMKDClassifierSGD with RFF with RFF on Trip Advisor Hotel Reviews\")\n",
        "plt.xlim(sample_sizes[0], sample_sizes[-1])\n",
        "plt.ylim(np.min(mixed_scores), 1)\n",
        "plt.xlabel(\"Gamma\")\n",
        "plt.ylabel(\"Classification accuracy\")\n",
        "plt.legend(loc='best')\n",
        "if log_plot == True:\n",
        "  plt.semilogx(basex=2)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfQJjWusVgXg",
        "outputId": "72c00900-6c0b-4799-ef93-567e78da1e39"
      },
      "source": [
        "'''\n",
        "Modified version of https://scikit-learn.org/stable/auto_examples/miscellaneous/plot_kernel_approximation.html#sphx-glr-auto-examples-miscellaneous-plot-kernel-approximation-py\n",
        "'''\n",
        "# Parameters \n",
        "num_comp = 1000\n",
        "log_plot = True\n",
        "num_exps = 1\n",
        "num_eig = 50\n",
        "num_epochs = 3\n",
        "lr = 0.0025\n",
        "\n",
        "# Interval of gamma points for tuning\n",
        "if log_plot == True:\n",
        "  #sample_sizes = np.logspace(-13, -1, num=13, base=2)\n",
        "  sample_sizes = np.logspace(4, 11, num=9, base=2)\n",
        "else:\n",
        "  sample_sizes = 0.025 * onp.arange(2, 31)\n",
        "\n",
        "# sample_sizes = np.array([2**(-8)]) # Uncomment this in case you want only one gamma\n",
        "\n",
        "# Create numpy arrays to keep track of data\n",
        "num_points = sample_sizes.shape[0]\n",
        "mixed_scores = np.zeros((num_points, num_exps))\n",
        "qmkdc_sgd_scores = np.zeros((num_points, num_exps))\n",
        "#linear_rff_scores = np.zeros((num_points, num_exps))\n",
        "\n",
        "# num_exps is the number of experiments for each point\n",
        "for i in range(num_exps):\n",
        "  # Start the training on the samples\n",
        "  #exp_time = time()\n",
        "  for j in range(num_points):  \n",
        "      exp_time = time()\n",
        "\n",
        "      #'''\n",
        "      #### Lin SVM #############\n",
        "      # feature_map_fourier = RBFSampler(gamma=sample_sizes[j], random_state=(i+1), n_components=num_comp)\n",
        "      # X_ff_train = feature_map_fourier.fit_transform(X_train)\n",
        "      # X_ff_train = X_ff_train/(np.linalg.norm(X_ff_train,axis=1)).reshape(-1,1)\n",
        "      # X_ff_test = feature_map_fourier.transform(X_test)\n",
        "      # X_ff_test = X_ff_test/(np.linalg.norm(X_ff_test,axis=1)).reshape(-1,1)\n",
        "      # # train linear svm over RFF map\n",
        "      # linear_rff_svm = svm.LinearSVC()\n",
        "      # linear_rff_svm.fit(X_ff_train, y_train.ravel())\n",
        "      # # Predict and save the value\n",
        "      # linear_rff_scores[j,i] = linear_rff_svm.score(X_ff_test, y_test.ravel())\n",
        "      # print(\"--------time LinSVM---------------\")\n",
        "      # print(time() - exp_time)\n",
        "      # exp_time = time()\n",
        "      #'''\n",
        "\n",
        "      #### QMKDCsgd ##############\n",
        "      qmkdc1_dig = models.QMKDClassifierSGD(input_dim=100, dim_x=num_comp, num_classes=4, num_eig=num_eig, gamma=sample_sizes[j], random_state=(i+1))\n",
        "      optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "      qmkdc1_dig.compile(optimizer, loss=tf.keras.losses.CategoricalCrossentropy())\n",
        "      y_train_bin = tf.reshape(tf.keras.backend.one_hot(y_train, 4), (-1,4))\n",
        "      qmkdc1_dig.fit(X_train, y_train_bin, epochs=num_epochs)\n",
        "      # history = qmkdc1_dig.fit(X_train, y_train_bin, epochs=num_epochs)  # Uncomment this if you want to plot loss and acc of last training\n",
        "      out = qmkdc1_dig.predict(X_test)\n",
        "      qmkdc_sgd_scores[j,i] = accuracy_score(y_test, np.argmax(out, axis=1))\n",
        "      del qmkdc1_dig\n",
        "      gc.collect()\n",
        "      print(\"--------time QMKDCsgd---------------\")\n",
        "      print(time() - exp_time)\n",
        "  #print(time() - exp_time)\n",
        "\n",
        "# Save the average accuracies and standard deviations in three lists\n",
        "ave_qmkdc_sgd_scores = qmkdc_sgd_scores.mean(axis=1).tolist()\n",
        "# ave_linear_rff_scores = linear_rff_scores.mean(axis=1).tolist()\n",
        "std_qmkdc_sgd_scores = qmkdc_sgd_scores.std(axis=1).tolist()\n",
        "# std_linear_rff_scores = linear_rff_scores.std(axis=1).tolist()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6920\n",
            "Epoch 2/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6018\n",
            "Epoch 3/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5491\n",
            "--------time QMKDCsgd---------------\n",
            "14.867408990859985\n",
            "Epoch 1/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6644\n",
            "Epoch 2/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5334\n",
            "Epoch 3/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.4753\n",
            "--------time QMKDCsgd---------------\n",
            "14.824714660644531\n",
            "Epoch 1/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6686\n",
            "Epoch 2/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5058\n",
            "Epoch 3/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.4569\n",
            "--------time QMKDCsgd---------------\n",
            "14.771422386169434\n",
            "Epoch 1/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7452\n",
            "Epoch 2/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5585\n",
            "Epoch 3/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5187\n",
            "--------time QMKDCsgd---------------\n",
            "14.70203423500061\n",
            "Epoch 1/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8578\n",
            "Epoch 2/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6506\n",
            "Epoch 3/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6101\n",
            "--------time QMKDCsgd---------------\n",
            "14.87454867362976\n",
            "Epoch 1/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.9465\n",
            "Epoch 2/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7299\n",
            "Epoch 3/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6870\n",
            "--------time QMKDCsgd---------------\n",
            "14.95890188217163\n",
            "Epoch 1/3\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.0099\n",
            "Epoch 2/3\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7872\n",
            "Epoch 3/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7426\n",
            "--------time QMKDCsgd---------------\n",
            "15.203496217727661\n",
            "Epoch 1/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.0474\n",
            "Epoch 2/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8276\n",
            "Epoch 3/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7809\n",
            "--------time QMKDCsgd---------------\n",
            "15.018136739730835\n",
            "Epoch 1/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.0790\n",
            "Epoch 2/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8589\n",
            "Epoch 3/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8103\n",
            "--------time QMKDCsgd---------------\n",
            "14.912609338760376\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "id": "Kip_bslsWKTf",
        "outputId": "4b069ea8-46d5-46ad-bb1a-d4acc4333d7b"
      },
      "source": [
        "# plot the results:\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.errorbar(sample_sizes, ave_qmkdc_sgd_scores, yerr=std_qmkdc_sgd_scores, label=f\"RFF QMKDClassifierSGD n_comp={num_comp} n_eig={num_eig}\")\n",
        "#plt.errorbar(sample_sizes, ave_linear_rff_scores, yerr=std_linear_rff_scores, label=f\"RFF LinearSVM n_comp={num_comp}\")\n",
        "\n",
        "# legends and labels\n",
        "# plt.title(\"QMDensitySGD, QMKDClassifierSGD and LinSVM with RFF on MNIST\")\n",
        "plt.title(\"QMKDClassifierSGD with RFF with RFF on Trip Advisor Hotel Reviews\")\n",
        "plt.xlim(sample_sizes[0], sample_sizes[-1])\n",
        "plt.ylim(np.min(mixed_scores), 1)\n",
        "plt.xlabel(\"Gamma\")\n",
        "plt.ylabel(\"Classification accuracy\")\n",
        "plt.legend(loc='best')\n",
        "if log_plot == True:\n",
        "  plt.semilogx(basex=2)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNOPuY6KXEFW",
        "outputId": "68e83fa7-3d08-461d-a5bb-c35cfcd2e768"
      },
      "source": [
        "'''\n",
        "Modified version of https://scikit-learn.org/stable/auto_examples/miscellaneous/plot_kernel_approximation.html#sphx-glr-auto-examples-miscellaneous-plot-kernel-approximation-py\n",
        "'''\n",
        "# Parameters \n",
        "num_comp = 1000\n",
        "log_plot = True\n",
        "num_exps = 1\n",
        "num_eig = 50\n",
        "num_epochs = 3\n",
        "lr = 0.0025\n",
        "\n",
        "# Interval of gamma points for tuning\n",
        "if log_plot == True:\n",
        "  #sample_sizes = np.logspace(-13, -1, num=13, base=2)\n",
        "  sample_sizes = np.logspace(-2, 4, num=6, base=10)\n",
        "else:\n",
        "  sample_sizes = 0.025 * onp.arange(2, 31)\n",
        "\n",
        "# sample_sizes = np.array([2**(-8)]) # Uncomment this in case you want only one gamma\n",
        "\n",
        "# Create numpy arrays to keep track of data\n",
        "num_points = sample_sizes.shape[0]\n",
        "mixed_scores = np.zeros((num_points, num_exps))\n",
        "qmkdc_sgd_scores = np.zeros((num_points, num_exps))\n",
        "#linear_rff_scores = np.zeros((num_points, num_exps))\n",
        "\n",
        "# num_exps is the number of experiments for each point\n",
        "for i in range(num_exps):\n",
        "  # Start the training on the samples\n",
        "  #exp_time = time()\n",
        "  for j in range(num_points):  \n",
        "      exp_time = time()\n",
        "\n",
        "      #'''\n",
        "      #### Lin SVM #############\n",
        "      # feature_map_fourier = RBFSampler(gamma=sample_sizes[j], random_state=(i+1), n_components=num_comp)\n",
        "      # X_ff_train = feature_map_fourier.fit_transform(X_train)\n",
        "      # X_ff_train = X_ff_train/(np.linalg.norm(X_ff_train,axis=1)).reshape(-1,1)\n",
        "      # X_ff_test = feature_map_fourier.transform(X_test)\n",
        "      # X_ff_test = X_ff_test/(np.linalg.norm(X_ff_test,axis=1)).reshape(-1,1)\n",
        "      # # train linear svm over RFF map\n",
        "      # linear_rff_svm = svm.LinearSVC()\n",
        "      # linear_rff_svm.fit(X_ff_train, y_train.ravel())\n",
        "      # # Predict and save the value\n",
        "      # linear_rff_scores[j,i] = linear_rff_svm.score(X_ff_test, y_test.ravel())\n",
        "      # print(\"--------time LinSVM---------------\")\n",
        "      # print(time() - exp_time)\n",
        "      # exp_time = time()\n",
        "      #'''\n",
        "\n",
        "      #### QMKDCsgd ##############\n",
        "      qmkdc1_dig = models.QMKDClassifierSGD(input_dim=100, dim_x=num_comp, num_classes=4, num_eig=num_eig, gamma=sample_sizes[j], random_state=(i+1))\n",
        "      optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "      qmkdc1_dig.compile(optimizer, loss=tf.keras.losses.CategoricalCrossentropy())\n",
        "      y_train_bin = tf.reshape(tf.keras.backend.one_hot(y_train, 4), (-1,4))\n",
        "      qmkdc1_dig.fit(X_train, y_train_bin, epochs=num_epochs)\n",
        "      # history = qmkdc1_dig.fit(X_train, y_train_bin, epochs=num_epochs)  # Uncomment this if you want to plot loss and acc of last training\n",
        "      out = qmkdc1_dig.predict(X_test)\n",
        "      qmkdc_sgd_scores[j,i] = accuracy_score(y_test, np.argmax(out, axis=1))\n",
        "      del qmkdc1_dig\n",
        "      gc.collect()\n",
        "      print(\"--------time QMKDCsgd---------------\")\n",
        "      print(time() - exp_time)\n",
        "  #print(time() - exp_time)\n",
        "\n",
        "# Save the average accuracies and standard deviations in three lists\n",
        "ave_qmkdc_sgd_scores = qmkdc_sgd_scores.mean(axis=1).tolist()\n",
        "# ave_linear_rff_scores = linear_rff_scores.mean(axis=1).tolist()\n",
        "std_qmkdc_sgd_scores = qmkdc_sgd_scores.std(axis=1).tolist()\n",
        "# std_linear_rff_scores = linear_rff_scores.std(axis=1).tolist()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8441\n",
            "Epoch 2/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7588\n",
            "Epoch 3/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7457\n",
            "--------time QMKDCsgd---------------\n",
            "15.070761442184448\n",
            "Epoch 1/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8034\n",
            "Epoch 2/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7394\n",
            "Epoch 3/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7254\n",
            "--------time QMKDCsgd---------------\n",
            "14.989478826522827\n",
            "Epoch 1/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7506\n",
            "Epoch 2/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7001\n",
            "Epoch 3/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6797\n",
            "--------time QMKDCsgd---------------\n",
            "15.076785326004028\n",
            "Epoch 1/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6554\n",
            "Epoch 2/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5079\n",
            "Epoch 3/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.4556\n",
            "--------time QMKDCsgd---------------\n",
            "14.962951421737671\n",
            "Epoch 1/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.0105\n",
            "Epoch 2/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7891\n",
            "Epoch 3/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7456\n",
            "--------time QMKDCsgd---------------\n",
            "15.056585311889648\n",
            "Epoch 1/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.1135\n",
            "Epoch 2/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8980\n",
            "Epoch 3/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8471\n",
            "--------time QMKDCsgd---------------\n",
            "15.057629346847534\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "id": "zlPFz1-xXid_",
        "outputId": "6ecb5cd2-e217-4bfa-bd9a-8960dbaca17f"
      },
      "source": [
        "# plot the results:\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.errorbar(sample_sizes, ave_qmkdc_sgd_scores, yerr=std_qmkdc_sgd_scores, label=f\"RFF QMKDClassifierSGD n_comp={num_comp} n_eig={num_eig}\")\n",
        "#plt.errorbar(sample_sizes, ave_linear_rff_scores, yerr=std_linear_rff_scores, label=f\"RFF LinearSVM n_comp={num_comp}\")\n",
        "\n",
        "# legends and labels\n",
        "# plt.title(\"QMDensitySGD, QMKDClassifierSGD and LinSVM with RFF on MNIST\")\n",
        "plt.title(\"QMKDClassifierSGD with RFF with RFF on Trip Advisor Hotel Reviews\")\n",
        "plt.xlim(sample_sizes[0], sample_sizes[-1])\n",
        "plt.ylim(np.min(mixed_scores), 1)\n",
        "plt.xlabel(\"Gamma\")\n",
        "plt.ylabel(\"Classification accuracy\")\n",
        "plt.legend(loc='best')\n",
        "if log_plot == True:\n",
        "  plt.semilogx(basex=10)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vwmKi_pYWtM",
        "outputId": "3f646642-015c-4308-90b2-b544a6c2657b"
      },
      "source": [
        "'''\n",
        "Modified version of https://scikit-learn.org/stable/auto_examples/miscellaneous/plot_kernel_approximation.html#sphx-glr-auto-examples-miscellaneous-plot-kernel-approximation-py\n",
        "'''\n",
        "# Parameters \n",
        "num_comp = 1000\n",
        "log_plot = True\n",
        "num_exps = 1\n",
        "num_eig = 50\n",
        "num_epochs = 3\n",
        "lr = 0.0025\n",
        "\n",
        "# Interval of gamma points for tuning\n",
        "if log_plot == True:\n",
        "  #sample_sizes = np.logspace(-13, -1, num=13, base=2)\n",
        "  sample_sizes = np.logspace(5, 9, num=10, base=2)\n",
        "else:\n",
        "  sample_sizes = 0.025 * onp.arange(2, 31)\n",
        "\n",
        "# sample_sizes = np.array([2**(-8)]) # Uncomment this in case you want only one gamma\n",
        "\n",
        "# Create numpy arrays to keep track of data\n",
        "num_points = sample_sizes.shape[0]\n",
        "mixed_scores = np.zeros((num_points, num_exps))\n",
        "qmkdc_sgd_scores = np.zeros((num_points, num_exps))\n",
        "#linear_rff_scores = np.zeros((num_points, num_exps))\n",
        "\n",
        "# num_exps is the number of experiments for each point\n",
        "for i in range(num_exps):\n",
        "  # Start the training on the samples\n",
        "  #exp_time = time()\n",
        "  for j in range(num_points):  \n",
        "      exp_time = time()\n",
        "\n",
        "      #'''\n",
        "      #### Lin SVM #############\n",
        "      # feature_map_fourier = RBFSampler(gamma=sample_sizes[j], random_state=(i+1), n_components=num_comp)\n",
        "      # X_ff_train = feature_map_fourier.fit_transform(X_train)\n",
        "      # X_ff_train = X_ff_train/(np.linalg.norm(X_ff_train,axis=1)).reshape(-1,1)\n",
        "      # X_ff_test = feature_map_fourier.transform(X_test)\n",
        "      # X_ff_test = X_ff_test/(np.linalg.norm(X_ff_test,axis=1)).reshape(-1,1)\n",
        "      # # train linear svm over RFF map\n",
        "      # linear_rff_svm = svm.LinearSVC()\n",
        "      # linear_rff_svm.fit(X_ff_train, y_train.ravel())\n",
        "      # # Predict and save the value\n",
        "      # linear_rff_scores[j,i] = linear_rff_svm.score(X_ff_test, y_test.ravel())\n",
        "      # print(\"--------time LinSVM---------------\")\n",
        "      # print(time() - exp_time)\n",
        "      # exp_time = time()\n",
        "      #'''\n",
        "\n",
        "      #### QMKDCsgd ##############\n",
        "      qmkdc1_dig = models.QMKDClassifierSGD(input_dim=100, dim_x=num_comp, num_classes=4, num_eig=num_eig, gamma=sample_sizes[j], random_state=(i+1))\n",
        "      optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "      qmkdc1_dig.compile(optimizer, loss=tf.keras.losses.CategoricalCrossentropy())\n",
        "      y_train_bin = tf.reshape(tf.keras.backend.one_hot(y_train, 4), (-1,4))\n",
        "      qmkdc1_dig.fit(X_train, y_train_bin, epochs=num_epochs)\n",
        "      # history = qmkdc1_dig.fit(X_train, y_train_bin, epochs=num_epochs)  # Uncomment this if you want to plot loss and acc of last training\n",
        "      out = qmkdc1_dig.predict(X_test)\n",
        "      qmkdc_sgd_scores[j,i] = accuracy_score(y_test, np.argmax(out, axis=1))\n",
        "      del qmkdc1_dig\n",
        "      gc.collect()\n",
        "      print(\"--------time QMKDCsgd---------------\")\n",
        "      print(time() - exp_time)\n",
        "  #print(time() - exp_time)\n",
        "\n",
        "# Save the average accuracies and standard deviations in three lists\n",
        "ave_qmkdc_sgd_scores = qmkdc_sgd_scores.mean(axis=1).tolist()\n",
        "# ave_linear_rff_scores = linear_rff_scores.mean(axis=1).tolist()\n",
        "std_qmkdc_sgd_scores = qmkdc_sgd_scores.std(axis=1).tolist()\n",
        "# std_linear_rff_scores = linear_rff_scores.std(axis=1).tolist()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6621\n",
            "Epoch 2/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5251\n",
            "Epoch 3/3\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4680\n",
            "--------time QMKDCsgd---------------\n",
            "15.114065647125244\n",
            "Epoch 1/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6546\n",
            "Epoch 2/3\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5034\n",
            "Epoch 3/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.4535\n",
            "--------time QMKDCsgd---------------\n",
            "15.109029769897461\n",
            "Epoch 1/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6741\n",
            "Epoch 2/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5089\n",
            "Epoch 3/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.4625\n",
            "--------time QMKDCsgd---------------\n",
            "15.074975490570068\n",
            "Epoch 1/3\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7158\n",
            "Epoch 2/3\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5364\n",
            "Epoch 3/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.4929\n",
            "--------time QMKDCsgd---------------\n",
            "15.085627317428589\n",
            "Epoch 1/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7655\n",
            "Epoch 2/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5725\n",
            "Epoch 3/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5308\n",
            "--------time QMKDCsgd---------------\n",
            "14.806511402130127\n",
            "Epoch 1/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8242\n",
            "Epoch 2/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6212\n",
            "Epoch 3/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5816\n",
            "--------time QMKDCsgd---------------\n",
            "14.721445560455322\n",
            "Epoch 1/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8807\n",
            "Epoch 2/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6688\n",
            "Epoch 3/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6274\n",
            "--------time QMKDCsgd---------------\n",
            "14.46574354171753\n",
            "Epoch 1/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.9256\n",
            "Epoch 2/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7110\n",
            "Epoch 3/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6695\n",
            "--------time QMKDCsgd---------------\n",
            "14.650604486465454\n",
            "Epoch 1/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.9600\n",
            "Epoch 2/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7432\n",
            "Epoch 3/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7001\n",
            "--------time QMKDCsgd---------------\n",
            "14.880597829818726\n",
            "Epoch 1/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.9929\n",
            "Epoch 2/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7723\n",
            "Epoch 3/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7284\n",
            "--------time QMKDCsgd---------------\n",
            "14.851251602172852\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "id": "ZoQTWaOtY9as",
        "outputId": "bcbb6899-8dee-4a3d-f801-99be3f1cc9ce"
      },
      "source": [
        "# plot the results:\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.errorbar(sample_sizes, ave_qmkdc_sgd_scores, yerr=std_qmkdc_sgd_scores, label=f\"RFF QMKDClassifierSGD n_comp={num_comp} n_eig={num_eig}\")\n",
        "#plt.errorbar(sample_sizes, ave_linear_rff_scores, yerr=std_linear_rff_scores, label=f\"RFF LinearSVM n_comp={num_comp}\")\n",
        "\n",
        "# legends and labels\n",
        "# plt.title(\"QMDensitySGD, QMKDClassifierSGD and LinSVM with RFF on MNIST\")\n",
        "plt.title(\"QMKDClassifierSGD with RFF with RFF on Trip Advisor Hotel Reviews\")\n",
        "plt.xlim(sample_sizes[0], sample_sizes[-1])\n",
        "plt.ylim(np.min(mixed_scores), 1)\n",
        "plt.xlabel(\"Gamma\")\n",
        "plt.ylabel(\"Classification accuracy\")\n",
        "plt.legend(loc='best')\n",
        "if log_plot == True:\n",
        "  plt.semilogx(basex=2)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGT42bJoaEHz",
        "outputId": "6b42108d-795e-4fb2-8cef-ac5120254fcc"
      },
      "source": [
        "'''\n",
        "Modified version of https://scikit-learn.org/stable/auto_examples/miscellaneous/plot_kernel_approximation.html#sphx-glr-auto-examples-miscellaneous-plot-kernel-approximation-py\n",
        "'''\n",
        "# Parameters \n",
        "num_comp = 1000\n",
        "log_plot = True\n",
        "num_exps = 1\n",
        "num_eig = 50\n",
        "num_epochs = 3\n",
        "lr = 0.0025\n",
        "\n",
        "# Interval of gamma points for tuning\n",
        "if log_plot == True:\n",
        "  #sample_sizes = np.logspace(-13, -1, num=13, base=2)\n",
        "  sample_sizes = np.logspace(2, 11, num=10, base=2)\n",
        "else:\n",
        "  sample_sizes = 0.025 * onp.arange(2, 31)\n",
        "\n",
        "# sample_sizes = np.array([2**(-8)]) # Uncomment this in case you want only one gamma\n",
        "\n",
        "# Create numpy arrays to keep track of data\n",
        "num_points = sample_sizes.shape[0]\n",
        "mixed_scores = np.zeros((num_points, num_exps))\n",
        "qmkdc_sgd_scores = np.zeros((num_points, num_exps))\n",
        "#linear_rff_scores = np.zeros((num_points, num_exps))\n",
        "\n",
        "# num_exps is the number of experiments for each point\n",
        "for i in range(num_exps):\n",
        "  # Start the training on the samples\n",
        "  #exp_time = time()\n",
        "  for j in range(num_points):  \n",
        "      exp_time = time()\n",
        "\n",
        "      #'''\n",
        "      #### Lin SVM #############\n",
        "      # feature_map_fourier = RBFSampler(gamma=sample_sizes[j], random_state=(i+1), n_components=num_comp)\n",
        "      # X_ff_train = feature_map_fourier.fit_transform(X_train)\n",
        "      # X_ff_train = X_ff_train/(np.linalg.norm(X_ff_train,axis=1)).reshape(-1,1)\n",
        "      # X_ff_test = feature_map_fourier.transform(X_test)\n",
        "      # X_ff_test = X_ff_test/(np.linalg.norm(X_ff_test,axis=1)).reshape(-1,1)\n",
        "      # # train linear svm over RFF map\n",
        "      # linear_rff_svm = svm.LinearSVC()\n",
        "      # linear_rff_svm.fit(X_ff_train, y_train.ravel())\n",
        "      # # Predict and save the value\n",
        "      # linear_rff_scores[j,i] = linear_rff_svm.score(X_ff_test, y_test.ravel())\n",
        "      # print(\"--------time LinSVM---------------\")\n",
        "      # print(time() - exp_time)\n",
        "      # exp_time = time()\n",
        "      #'''\n",
        "\n",
        "      #### QMKDCsgd ##############\n",
        "      qmkdc1_dig = models.QMKDClassifierSGD(input_dim=100, dim_x=num_comp, num_classes=4, num_eig=num_eig, gamma=sample_sizes[j], random_state=(i+1))\n",
        "      optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "      qmkdc1_dig.compile(optimizer, loss=tf.keras.losses.CategoricalCrossentropy())\n",
        "      y_train_bin = tf.reshape(tf.keras.backend.one_hot(y_train, 4), (-1,4))\n",
        "      qmkdc1_dig.fit(X_train, y_train_bin, epochs=num_epochs)\n",
        "      # history = qmkdc1_dig.fit(X_train, y_train_bin, epochs=num_epochs)  # Uncomment this if you want to plot loss and acc of last training\n",
        "      out = qmkdc1_dig.predict(X_test)\n",
        "      qmkdc_sgd_scores[j,i] = accuracy_score(y_test, np.argmax(out, axis=1))\n",
        "      del qmkdc1_dig\n",
        "      gc.collect()\n",
        "      print(\"--------time QMKDCsgd---------------\")\n",
        "      print(time() - exp_time)\n",
        "  #print(time() - exp_time)\n",
        "\n",
        "# Save the average accuracies and standard deviations in three lists\n",
        "ave_qmkdc_sgd_scores = qmkdc_sgd_scores.mean(axis=1).tolist()\n",
        "# ave_linear_rff_scores = linear_rff_scores.mean(axis=1).tolist()\n",
        "std_qmkdc_sgd_scores = qmkdc_sgd_scores.std(axis=1).tolist()\n",
        "# std_linear_rff_scores = linear_rff_scores.std(axis=1).tolist()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7446\n",
            "Epoch 2/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6868\n",
            "Epoch 3/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6639\n",
            "--------time QMKDCsgd---------------\n",
            "15.099749326705933\n",
            "Epoch 1/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7195\n",
            "Epoch 2/3\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6538\n",
            "Epoch 3/3\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6221\n",
            "--------time QMKDCsgd---------------\n",
            "15.254241228103638\n",
            "Epoch 1/3\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6916\n",
            "Epoch 2/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5984\n",
            "Epoch 3/3\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5477\n",
            "--------time QMKDCsgd---------------\n",
            "15.233581066131592\n",
            "Epoch 1/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6611\n",
            "Epoch 2/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5211\n",
            "Epoch 3/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.4656\n",
            "--------time QMKDCsgd---------------\n",
            "14.946702241897583\n",
            "Epoch 1/3\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6796\n",
            "Epoch 2/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5105\n",
            "Epoch 3/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.4673\n",
            "--------time QMKDCsgd---------------\n",
            "15.008647680282593\n",
            "Epoch 1/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7935\n",
            "Epoch 2/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5957\n",
            "Epoch 3/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5550\n",
            "--------time QMKDCsgd---------------\n",
            "14.683684587478638\n",
            "Epoch 1/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.9132\n",
            "Epoch 2/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6973\n",
            "Epoch 3/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6556\n",
            "--------time QMKDCsgd---------------\n",
            "14.505578756332397\n",
            "Epoch 1/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.9901\n",
            "Epoch 2/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7709\n",
            "Epoch 3/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7278\n",
            "--------time QMKDCsgd---------------\n",
            "14.647811651229858\n",
            "Epoch 1/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.0396\n",
            "Epoch 2/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8190\n",
            "Epoch 3/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7746\n",
            "--------time QMKDCsgd---------------\n",
            "14.676089525222778\n",
            "Epoch 1/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.0738\n",
            "Epoch 2/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8545\n",
            "Epoch 3/3\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8077\n",
            "--------time QMKDCsgd---------------\n",
            "14.788108825683594\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "id": "m9xZn3k_aaMk",
        "outputId": "dcf01c19-7c6e-4c76-daff-4633df045fcd"
      },
      "source": [
        "# plot the results:\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.errorbar(sample_sizes, ave_qmkdc_sgd_scores, yerr=std_qmkdc_sgd_scores, label=f\"RFF QMKDClassifierSGD n_comp={num_comp} n_eig={num_eig}\")\n",
        "#plt.errorbar(sample_sizes, ave_linear_rff_scores, yerr=std_linear_rff_scores, label=f\"RFF LinearSVM n_comp={num_comp}\")\n",
        "\n",
        "# legends and labels\n",
        "# plt.title(\"QMDensitySGD, QMKDClassifierSGD and LinSVM with RFF on MNIST\")\n",
        "plt.title(\"QMKDClassifierSGD with RFF with RFF on Trip Advisor Hotel Reviews\")\n",
        "plt.xlim(sample_sizes[0], sample_sizes[-1])\n",
        "plt.ylim(np.min(mixed_scores), 1)\n",
        "plt.xlabel(\"Gamma\")\n",
        "plt.ylabel(\"Classification accuracy\")\n",
        "plt.legend(loc='best')\n",
        "if log_plot == True:\n",
        "  plt.semilogx(basex=2)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loDIkSXQd5qK"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_y-7y9Nb4IW",
        "outputId": "f2786920-8c78-4f65-94bc-52b36ef228d7"
      },
      "source": [
        "set_gamma_a =  np.logspace(2, 11, num=10, base=2).tolist()\n",
        "# set_lr_b = [0.00005, 0.0005, 0.5, 10]\n",
        "# set_lr_c = [0.000005, 0.00005, 1, 10]\n",
        "# set_lr_d = [0.0005, 0.005, 0.05, 0.5, 1, 10]\n",
        "# set_lr_e = [0.0005, 0.0025, 0.005]\n",
        "\n",
        "for gamma in set_gamma_a:\n",
        "  exp_time = time()\n",
        "  qmkdc1_dig = models.QMKDClassifierSGD(input_dim=100, dim_x=1000, num_classes=4, num_eig=50, gamma=gamma, random_state=17)\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=0.0025)\n",
        "  qmkdc1_dig.compile(optimizer, loss=tf.keras.losses.CategoricalCrossentropy())\n",
        "  y_train_bin = tf.reshape(tf.keras.backend.one_hot(y_train, 4), (-1,4))\n",
        "  qmkdc1_dig.fit(X_train, y_train_bin, epochs=10)\n",
        "  out = qmkdc1_dig.predict(X_test)\n",
        "  print(f'gamma = {gamma}')\n",
        "  print(f'acc = {accuracy_score(y_test, np.argmax(out, axis=1))}')\n",
        "  print(\"--------time QMKDCsgd---------------\")\n",
        "  print(time() - exp_time)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7417\n",
            "Epoch 2/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6885\n",
            "Epoch 3/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6678\n",
            "Epoch 4/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6506\n",
            "Epoch 5/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6373\n",
            "Epoch 6/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6255\n",
            "Epoch 7/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6155\n",
            "Epoch 8/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6068\n",
            "Epoch 9/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5986\n",
            "Epoch 10/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5906\n",
            "lr = 4.0\n",
            "acc = 0.5466092334879611\n",
            "--------time QMKDCsgd---------------\n",
            "47.8021981716156\n",
            "Epoch 1/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7233\n",
            "Epoch 2/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6610\n",
            "Epoch 3/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6263\n",
            "Epoch 4/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6017\n",
            "Epoch 5/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5801\n",
            "Epoch 6/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5639\n",
            "Epoch 7/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5508\n",
            "Epoch 8/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5369\n",
            "Epoch 9/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5213\n",
            "Epoch 10/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5104\n",
            "lr = 4.0\n",
            "acc = 0.5943229511817981\n",
            "--------time QMKDCsgd---------------\n",
            "47.91221618652344\n",
            "Epoch 1/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6929\n",
            "Epoch 2/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6028\n",
            "Epoch 3/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5538\n",
            "Epoch 4/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5199\n",
            "Epoch 5/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4930\n",
            "Epoch 6/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4718\n",
            "Epoch 7/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4530\n",
            "Epoch 8/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4392\n",
            "Epoch 9/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4268\n",
            "Epoch 10/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4173\n",
            "lr = 4.0\n",
            "acc = 0.6436933951844489\n",
            "--------time QMKDCsgd---------------\n",
            "47.040284395217896\n",
            "Epoch 1/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6622\n",
            "Epoch 2/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5218\n",
            "Epoch 3/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4679\n",
            "Epoch 4/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4394\n",
            "Epoch 5/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4172\n",
            "Epoch 6/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.4014\n",
            "Epoch 7/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.3895\n",
            "Epoch 8/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.3788\n",
            "Epoch 9/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.3689\n",
            "Epoch 10/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.3621\n",
            "lr = 4.0\n",
            "acc = 0.6749502982107356\n",
            "--------time QMKDCsgd---------------\n",
            "46.80012035369873\n",
            "Epoch 1/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6818\n",
            "Epoch 2/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5130\n",
            "Epoch 3/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4685\n",
            "Epoch 4/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4450\n",
            "Epoch 5/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4269\n",
            "Epoch 6/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4133\n",
            "Epoch 7/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4022\n",
            "Epoch 8/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.3924\n",
            "Epoch 9/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.3831\n",
            "Epoch 10/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.3748\n",
            "lr = 4.0\n",
            "acc = 0.6666666666666666\n",
            "--------time QMKDCsgd---------------\n",
            "48.01685619354248\n",
            "Epoch 1/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7889\n",
            "Epoch 2/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5918\n",
            "Epoch 3/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5530\n",
            "Epoch 4/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5300\n",
            "Epoch 5/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5132\n",
            "Epoch 6/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5002\n",
            "Epoch 7/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.4888\n",
            "Epoch 8/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4799\n",
            "Epoch 9/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.4712\n",
            "Epoch 10/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.4630\n",
            "lr = 4.0\n",
            "acc = 0.6287828584051248\n",
            "--------time QMKDCsgd---------------\n",
            "46.34747076034546\n",
            "Epoch 1/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.9133\n",
            "Epoch 2/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6967\n",
            "Epoch 3/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6545\n",
            "Epoch 4/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6320\n",
            "Epoch 5/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6167\n",
            "Epoch 6/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6040\n",
            "Epoch 7/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5932\n",
            "Epoch 8/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5847\n",
            "Epoch 9/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5760\n",
            "Epoch 10/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5689\n",
            "lr = 4.0\n",
            "acc = 0.5606361829025845\n",
            "--------time QMKDCsgd---------------\n",
            "47.91819930076599\n",
            "Epoch 1/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.9952\n",
            "Epoch 2/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7751\n",
            "Epoch 3/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7292\n",
            "Epoch 4/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7053\n",
            "Epoch 5/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6888\n",
            "Epoch 6/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6764\n",
            "Epoch 7/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6660\n",
            "Epoch 8/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6572\n",
            "Epoch 9/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6490\n",
            "Epoch 10/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6423\n",
            "lr = 4.0\n",
            "acc = 0.4855312569030263\n",
            "--------time QMKDCsgd---------------\n",
            "46.4060435295105\n",
            "Epoch 1/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.0415\n",
            "Epoch 2/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8219\n",
            "Epoch 3/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7769\n",
            "Epoch 4/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7524\n",
            "Epoch 5/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7364\n",
            "Epoch 6/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7233\n",
            "Epoch 7/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7132\n",
            "Epoch 8/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7044\n",
            "Epoch 9/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6966\n",
            "Epoch 10/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6892\n",
            "lr = 4.0\n",
            "acc = 0.41296664457698257\n",
            "--------time QMKDCsgd---------------\n",
            "45.893019676208496\n",
            "Epoch 1/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.0790\n",
            "Epoch 2/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.8587\n",
            "Epoch 3/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8111\n",
            "Epoch 4/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7848\n",
            "Epoch 5/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7677\n",
            "Epoch 6/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7542\n",
            "Epoch 7/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7437\n",
            "Epoch 8/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7342\n",
            "Epoch 9/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7268\n",
            "Epoch 10/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7200\n",
            "lr = 4.0\n",
            "acc = 0.3527722553567484\n",
            "--------time QMKDCsgd---------------\n",
            "47.06901502609253\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-n8l1z6tf16T",
        "outputId": "e2623eef-24d2-4435-ebda-5c7d35211d6e"
      },
      "source": [
        "set_gamma_a =  np.logspace(2, 11, num=10, base=2).tolist()\n",
        "# set_lr_b = [0.00005, 0.0005, 0.5, 10]\n",
        "# set_lr_c = [0.000005, 0.00005, 1, 10]\n",
        "# set_lr_d = [0.0005, 0.005, 0.05, 0.5, 1, 10]\n",
        "# set_lr_e = [0.0005, 0.0025, 0.005]\n",
        "\n",
        "for gamma in set_gamma_a:\n",
        "  exp_time = time()\n",
        "  qmkdc1_dig = models.QMKDClassifierSGD(input_dim=100, dim_x=1000, num_classes=4, num_eig=50, gamma=gamma, random_state=17)\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=0.0025)\n",
        "  qmkdc1_dig.compile(optimizer, loss=tf.keras.losses.CategoricalCrossentropy())\n",
        "  y_train_bin = tf.reshape(tf.keras.backend.one_hot(y_train, 4), (-1,4))\n",
        "  qmkdc1_dig.fit(X_train, y_train_bin, epochs=10)\n",
        "  out = qmkdc1_dig.predict(X_test)\n",
        "  print(f'gamma = {gamma}')\n",
        "  print(f'acc = {accuracy_score(y_test, np.argmax(out, axis=1))}')\n",
        "  print(\"--------time QMKDCsgd---------------\")\n",
        "  print(time() - exp_time)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7420\n",
            "Epoch 2/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6849\n",
            "Epoch 3/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6660\n",
            "Epoch 4/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6475\n",
            "Epoch 5/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6367\n",
            "Epoch 6/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6261\n",
            "Epoch 7/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6166\n",
            "Epoch 8/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6051\n",
            "Epoch 9/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5980\n",
            "Epoch 10/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5906\n",
            "gamma = 4.0\n",
            "acc = 0.5532361387232163\n",
            "--------time QMKDCsgd---------------\n",
            "52.60134768486023\n",
            "Epoch 1/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7235\n",
            "Epoch 2/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6585\n",
            "Epoch 3/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6293\n",
            "Epoch 4/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6055\n",
            "Epoch 5/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5851\n",
            "Epoch 6/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5672\n",
            "Epoch 7/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5513\n",
            "Epoch 8/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5366\n",
            "Epoch 9/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5234\n",
            "Epoch 10/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5139\n",
            "gamma = 8.0\n",
            "acc = 0.5868124585818423\n",
            "--------time QMKDCsgd---------------\n",
            "43.754037857055664\n",
            "Epoch 1/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6943\n",
            "Epoch 2/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6040\n",
            "Epoch 3/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5539\n",
            "Epoch 4/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5178\n",
            "Epoch 5/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.4906\n",
            "Epoch 6/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.4680\n",
            "Epoch 7/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.4523\n",
            "Epoch 8/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.4362\n",
            "Epoch 9/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.4239\n",
            "Epoch 10/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.4135\n",
            "gamma = 16.0\n",
            "acc = 0.643914292025624\n",
            "--------time QMKDCsgd---------------\n",
            "44.255937814712524\n",
            "Epoch 1/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6664\n",
            "Epoch 2/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5280\n",
            "Epoch 3/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.4727\n",
            "Epoch 4/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.4398\n",
            "Epoch 5/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.4187\n",
            "Epoch 6/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.4034\n",
            "Epoch 7/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.3915\n",
            "Epoch 8/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.3771\n",
            "Epoch 9/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.3696\n",
            "Epoch 10/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.3603\n",
            "gamma = 32.0\n",
            "acc = 0.6742876076872101\n",
            "--------time QMKDCsgd---------------\n",
            "44.77781391143799\n",
            "Epoch 1/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6839\n",
            "Epoch 2/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5144\n",
            "Epoch 3/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.4695\n",
            "Epoch 4/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.4446\n",
            "Epoch 5/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.4273\n",
            "Epoch 6/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.4131\n",
            "Epoch 7/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4026\n",
            "Epoch 8/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.3920\n",
            "Epoch 9/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.3832\n",
            "Epoch 10/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.3743\n",
            "gamma = 64.0\n",
            "acc = 0.6641263529931521\n",
            "--------time QMKDCsgd---------------\n",
            "44.686641454696655\n",
            "Epoch 1/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7909\n",
            "Epoch 2/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5926\n",
            "Epoch 3/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5511\n",
            "Epoch 4/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5293\n",
            "Epoch 5/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5143\n",
            "Epoch 6/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5010\n",
            "Epoch 7/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.4899\n",
            "Epoch 8/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.4809\n",
            "Epoch 9/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.4716\n",
            "Epoch 10/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.4646\n",
            "gamma = 128.0\n",
            "acc = 0.6274574773580738\n",
            "--------time QMKDCsgd---------------\n",
            "45.45303773880005\n",
            "Epoch 1/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.9107\n",
            "Epoch 2/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6951\n",
            "Epoch 3/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6542\n",
            "Epoch 4/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6320\n",
            "Epoch 5/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6154\n",
            "Epoch 6/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6037\n",
            "Epoch 7/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5933\n",
            "Epoch 8/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5846\n",
            "Epoch 9/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5769\n",
            "Epoch 10/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5704\n",
            "gamma = 256.0\n",
            "acc = 0.5683675723437155\n",
            "--------time QMKDCsgd---------------\n",
            "44.87547421455383\n",
            "Epoch 1/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.9956\n",
            "Epoch 2/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7735\n",
            "Epoch 3/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7290\n",
            "Epoch 4/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7045\n",
            "Epoch 5/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6880\n",
            "Epoch 6/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6753\n",
            "Epoch 7/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6650\n",
            "Epoch 8/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6561\n",
            "Epoch 9/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6481\n",
            "Epoch 10/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6410\n",
            "gamma = 512.0\n",
            "acc = 0.47879390324718357\n",
            "--------time QMKDCsgd---------------\n",
            "44.56942868232727\n",
            "Epoch 1/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.0482\n",
            "Epoch 2/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8250\n",
            "Epoch 3/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7792\n",
            "Epoch 4/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7548\n",
            "Epoch 5/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7377\n",
            "Epoch 6/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7253\n",
            "Epoch 7/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7141\n",
            "Epoch 8/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7053\n",
            "Epoch 9/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6975\n",
            "Epoch 10/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6904\n",
            "gamma = 1024.0\n",
            "acc = 0.4088800530152419\n",
            "--------time QMKDCsgd---------------\n",
            "45.18595814704895\n",
            "Epoch 1/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.0835\n",
            "Epoch 2/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.8622\n",
            "Epoch 3/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8140\n",
            "Epoch 4/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7885\n",
            "Epoch 5/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7712\n",
            "Epoch 6/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7573\n",
            "Epoch 7/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7467\n",
            "Epoch 8/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7376\n",
            "Epoch 9/10\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7298\n",
            "Epoch 10/10\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7227\n",
            "gamma = 2048.0\n",
            "acc = 0.3512259774685222\n",
            "--------time QMKDCsgd---------------\n",
            "45.20932173728943\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMKHEDhZf14A",
        "outputId": "dcb25124-119e-402e-b21b-742669b0775c"
      },
      "source": [
        "set_gamma_a =  np.logspace(0, 11, num=10, base=2).tolist()\n",
        "# set_lr_b = [0.00005, 0.0005, 0.5, 10]\n",
        "# set_lr_c = [0.000005, 0.00005, 1, 10]\n",
        "# set_lr_d = [0.0005, 0.005, 0.05, 0.5, 1, 10]\n",
        "# set_lr_e = [0.0005, 0.0025, 0.005]\n",
        "\n",
        "for gamma in set_gamma_a:\n",
        "  exp_time = time()\n",
        "  qmkdc1_dig = models.QMKDClassifierSGD(input_dim=100, dim_x=5000, num_classes=4, num_eig=50, gamma=gamma, random_state=0)\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=0.000005)\n",
        "  qmkdc1_dig.compile(optimizer, loss=tf.keras.losses.CategoricalCrossentropy())\n",
        "  y_train_bin = tf.reshape(tf.keras.backend.one_hot(y_train, 4), (-1,4))\n",
        "  qmkdc1_dig.fit(X_train, y_train_bin, epochs=100)\n",
        "  out = qmkdc1_dig.predict(X_test)\n",
        "  print(f'gamma = {gamma}')\n",
        "  print(f'acc = {accuracy_score(y_test, np.argmax(out, axis=1))}')\n",
        "  print(\"--------time QMKDCsgd---------------\")\n",
        "  print(time() - exp_time)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.9583\n",
            "Epoch 2/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8266\n",
            "Epoch 3/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7988\n",
            "Epoch 4/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7819\n",
            "Epoch 5/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7697\n",
            "Epoch 6/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7605\n",
            "Epoch 7/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7526\n",
            "Epoch 8/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7461\n",
            "Epoch 9/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7409\n",
            "Epoch 10/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7364\n",
            "Epoch 11/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7323\n",
            "Epoch 12/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7285\n",
            "Epoch 13/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7252\n",
            "Epoch 14/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7221\n",
            "Epoch 15/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7194\n",
            "Epoch 16/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7165\n",
            "Epoch 17/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7146\n",
            "Epoch 18/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7120\n",
            "Epoch 19/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7103\n",
            "Epoch 20/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7080\n",
            "Epoch 21/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7060\n",
            "Epoch 22/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7042\n",
            "Epoch 23/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7027\n",
            "Epoch 24/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7008\n",
            "Epoch 25/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6991\n",
            "Epoch 26/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6975\n",
            "Epoch 27/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6959\n",
            "Epoch 28/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6950\n",
            "Epoch 29/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6935\n",
            "Epoch 30/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6920\n",
            "Epoch 31/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6911\n",
            "Epoch 32/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6898\n",
            "Epoch 33/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6884\n",
            "Epoch 34/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6872\n",
            "Epoch 35/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6863\n",
            "Epoch 36/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6852\n",
            "Epoch 37/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6841\n",
            "Epoch 38/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6830\n",
            "Epoch 39/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6819\n",
            "Epoch 40/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6811\n",
            "Epoch 41/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6797\n",
            "Epoch 42/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6789\n",
            "Epoch 43/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6782\n",
            "Epoch 44/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6773\n",
            "Epoch 45/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6761\n",
            "Epoch 46/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6756\n",
            "Epoch 47/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6748\n",
            "Epoch 48/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6735\n",
            "Epoch 49/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6730\n",
            "Epoch 50/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6721\n",
            "Epoch 51/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6712\n",
            "Epoch 52/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6705\n",
            "Epoch 53/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6695\n",
            "Epoch 54/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6686\n",
            "Epoch 55/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6679\n",
            "Epoch 56/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6675\n",
            "Epoch 57/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6670\n",
            "Epoch 58/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6660\n",
            "Epoch 59/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6652\n",
            "Epoch 60/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6644\n",
            "Epoch 61/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6641\n",
            "Epoch 62/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6630\n",
            "Epoch 63/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6626\n",
            "Epoch 64/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6620\n",
            "Epoch 65/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6610\n",
            "Epoch 66/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6605\n",
            "Epoch 67/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6598\n",
            "Epoch 68/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6593\n",
            "Epoch 69/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6585\n",
            "Epoch 70/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6583\n",
            "Epoch 71/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6576\n",
            "Epoch 72/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6572\n",
            "Epoch 73/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6561\n",
            "Epoch 74/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6556\n",
            "Epoch 75/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6550\n",
            "Epoch 76/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6546\n",
            "Epoch 77/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6541\n",
            "Epoch 78/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6533\n",
            "Epoch 79/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6530\n",
            "Epoch 80/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6521\n",
            "Epoch 81/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6516\n",
            "Epoch 82/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6514\n",
            "Epoch 83/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6506\n",
            "Epoch 84/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6501\n",
            "Epoch 85/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6494\n",
            "Epoch 86/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6489\n",
            "Epoch 87/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6484\n",
            "Epoch 88/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6481\n",
            "Epoch 89/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6476\n",
            "Epoch 90/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6469\n",
            "Epoch 91/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6464\n",
            "Epoch 92/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6462\n",
            "Epoch 93/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6458\n",
            "Epoch 94/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6451\n",
            "Epoch 95/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6445\n",
            "Epoch 96/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6444\n",
            "Epoch 97/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6436\n",
            "Epoch 98/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6431\n",
            "Epoch 99/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6425\n",
            "Epoch 100/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6419\n",
            "gamma = 1.0\n",
            "acc = 0.5345703556439143\n",
            "--------time QMKDCsgd---------------\n",
            "447.5043087005615\n",
            "Epoch 1/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.9517\n",
            "Epoch 2/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.8295\n",
            "Epoch 3/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7978\n",
            "Epoch 4/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7783\n",
            "Epoch 5/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7646\n",
            "Epoch 6/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7541\n",
            "Epoch 7/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7458\n",
            "Epoch 8/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7385\n",
            "Epoch 9/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7329\n",
            "Epoch 10/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7277\n",
            "Epoch 11/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7231\n",
            "Epoch 12/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7189\n",
            "Epoch 13/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7153\n",
            "Epoch 14/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7121\n",
            "Epoch 15/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7088\n",
            "Epoch 16/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7058\n",
            "Epoch 17/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7031\n",
            "Epoch 18/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7009\n",
            "Epoch 19/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6983\n",
            "Epoch 20/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6959\n",
            "Epoch 21/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6935\n",
            "Epoch 22/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6915\n",
            "Epoch 23/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6894\n",
            "Epoch 24/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6876\n",
            "Epoch 25/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6860\n",
            "Epoch 26/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6839\n",
            "Epoch 27/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6824\n",
            "Epoch 28/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6807\n",
            "Epoch 29/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6792\n",
            "Epoch 30/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6774\n",
            "Epoch 31/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6760\n",
            "Epoch 32/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6745\n",
            "Epoch 33/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6727\n",
            "Epoch 34/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6718\n",
            "Epoch 35/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6701\n",
            "Epoch 36/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6687\n",
            "Epoch 37/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6673\n",
            "Epoch 38/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6664\n",
            "Epoch 39/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6649\n",
            "Epoch 40/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6637\n",
            "Epoch 41/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6624\n",
            "Epoch 42/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6615\n",
            "Epoch 43/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6602\n",
            "Epoch 44/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6589\n",
            "Epoch 45/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6579\n",
            "Epoch 46/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6568\n",
            "Epoch 47/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6558\n",
            "Epoch 48/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6547\n",
            "Epoch 49/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6537\n",
            "Epoch 50/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6524\n",
            "Epoch 51/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6515\n",
            "Epoch 52/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6506\n",
            "Epoch 53/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6494\n",
            "Epoch 54/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6486\n",
            "Epoch 55/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6478\n",
            "Epoch 56/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6468\n",
            "Epoch 57/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6458\n",
            "Epoch 58/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6448\n",
            "Epoch 59/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6441\n",
            "Epoch 60/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6433\n",
            "Epoch 61/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6421\n",
            "Epoch 62/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6414\n",
            "Epoch 63/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6406\n",
            "Epoch 64/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6397\n",
            "Epoch 65/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6386\n",
            "Epoch 66/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6381\n",
            "Epoch 67/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6370\n",
            "Epoch 68/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6364\n",
            "Epoch 69/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6354\n",
            "Epoch 70/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6345\n",
            "Epoch 71/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6338\n",
            "Epoch 72/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6333\n",
            "Epoch 73/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6323\n",
            "Epoch 74/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6318\n",
            "Epoch 75/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6309\n",
            "Epoch 76/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6301\n",
            "Epoch 77/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6293\n",
            "Epoch 78/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6286\n",
            "Epoch 79/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6277\n",
            "Epoch 80/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6269\n",
            "Epoch 81/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6262\n",
            "Epoch 82/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6256\n",
            "Epoch 83/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6251\n",
            "Epoch 84/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6240\n",
            "Epoch 85/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6234\n",
            "Epoch 86/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6229\n",
            "Epoch 87/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6221\n",
            "Epoch 88/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6213\n",
            "Epoch 89/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6207\n",
            "Epoch 90/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6202\n",
            "Epoch 91/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6193\n",
            "Epoch 92/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6188\n",
            "Epoch 93/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6179\n",
            "Epoch 94/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6176\n",
            "Epoch 95/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6169\n",
            "Epoch 96/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6161\n",
            "Epoch 97/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6155\n",
            "Epoch 98/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6150\n",
            "Epoch 99/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6140\n",
            "Epoch 100/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6136\n",
            "gamma = 2.3330580791522335\n",
            "acc = 0.5455047492820853\n",
            "--------time QMKDCsgd---------------\n",
            "452.4629554748535\n",
            "Epoch 1/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.9387\n",
            "Epoch 2/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8352\n",
            "Epoch 3/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7999\n",
            "Epoch 4/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7780\n",
            "Epoch 5/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7623\n",
            "Epoch 6/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7504\n",
            "Epoch 7/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7409\n",
            "Epoch 8/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7330\n",
            "Epoch 9/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7263\n",
            "Epoch 10/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7205\n",
            "Epoch 11/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7153\n",
            "Epoch 12/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7105\n",
            "Epoch 13/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7061\n",
            "Epoch 14/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7022\n",
            "Epoch 15/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6986\n",
            "Epoch 16/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6949\n",
            "Epoch 17/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6916\n",
            "Epoch 18/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6888\n",
            "Epoch 19/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6855\n",
            "Epoch 20/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6828\n",
            "Epoch 21/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6800\n",
            "Epoch 22/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6774\n",
            "Epoch 23/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6748\n",
            "Epoch 24/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6724\n",
            "Epoch 25/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6699\n",
            "Epoch 26/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6677\n",
            "Epoch 27/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6653\n",
            "Epoch 28/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6632\n",
            "Epoch 29/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6610\n",
            "Epoch 30/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6588\n",
            "Epoch 31/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6568\n",
            "Epoch 32/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6549\n",
            "Epoch 33/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6530\n",
            "Epoch 34/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6512\n",
            "Epoch 35/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6493\n",
            "Epoch 36/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6473\n",
            "Epoch 37/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6455\n",
            "Epoch 38/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6439\n",
            "Epoch 39/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6421\n",
            "Epoch 40/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6405\n",
            "Epoch 41/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6387\n",
            "Epoch 42/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6373\n",
            "Epoch 43/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6357\n",
            "Epoch 44/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6340\n",
            "Epoch 45/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6324\n",
            "Epoch 46/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6309\n",
            "Epoch 47/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6295\n",
            "Epoch 48/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6281\n",
            "Epoch 49/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6267\n",
            "Epoch 50/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6251\n",
            "Epoch 51/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6237\n",
            "Epoch 52/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6224\n",
            "Epoch 53/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6210\n",
            "Epoch 54/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6196\n",
            "Epoch 55/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6184\n",
            "Epoch 56/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6172\n",
            "Epoch 57/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6159\n",
            "Epoch 58/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6144\n",
            "Epoch 59/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6132\n",
            "Epoch 60/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6120\n",
            "Epoch 61/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6108\n",
            "Epoch 62/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6096\n",
            "Epoch 63/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6084\n",
            "Epoch 64/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6071\n",
            "Epoch 65/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6059\n",
            "Epoch 66/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6049\n",
            "Epoch 67/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6037\n",
            "Epoch 68/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6025\n",
            "Epoch 69/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6014\n",
            "Epoch 70/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6002\n",
            "Epoch 71/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5992\n",
            "Epoch 72/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5980\n",
            "Epoch 73/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5972\n",
            "Epoch 74/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5960\n",
            "Epoch 75/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5949\n",
            "Epoch 76/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5939\n",
            "Epoch 77/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5928\n",
            "Epoch 78/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5919\n",
            "Epoch 79/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5906\n",
            "Epoch 80/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5896\n",
            "Epoch 81/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5886\n",
            "Epoch 82/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5877\n",
            "Epoch 83/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5866\n",
            "Epoch 84/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5857\n",
            "Epoch 85/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5848\n",
            "Epoch 86/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5840\n",
            "Epoch 87/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5828\n",
            "Epoch 88/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5819\n",
            "Epoch 89/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5809\n",
            "Epoch 90/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5799\n",
            "Epoch 91/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5790\n",
            "Epoch 92/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5782\n",
            "Epoch 93/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5773\n",
            "Epoch 94/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5762\n",
            "Epoch 95/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5755\n",
            "Epoch 96/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5746\n",
            "Epoch 97/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5736\n",
            "Epoch 98/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5727\n",
            "Epoch 99/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5719\n",
            "Epoch 100/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5711\n",
            "gamma = 5.4431600006975085\n",
            "acc = 0.5672630881378397\n",
            "--------time QMKDCsgd---------------\n",
            "455.03711557388306\n",
            "Epoch 1/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.9768\n",
            "Epoch 2/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.8545\n",
            "Epoch 3/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.8112\n",
            "Epoch 4/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7848\n",
            "Epoch 5/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7660\n",
            "Epoch 6/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7517\n",
            "Epoch 7/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7401\n",
            "Epoch 8/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7302\n",
            "Epoch 9/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7217\n",
            "Epoch 10/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7142\n",
            "Epoch 11/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7074\n",
            "Epoch 12/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7012\n",
            "Epoch 13/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6954\n",
            "Epoch 14/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6900\n",
            "Epoch 15/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6850\n",
            "Epoch 16/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6803\n",
            "Epoch 17/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6758\n",
            "Epoch 18/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6714\n",
            "Epoch 19/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6673\n",
            "Epoch 20/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6633\n",
            "Epoch 21/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6594\n",
            "Epoch 22/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6558\n",
            "Epoch 23/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6520\n",
            "Epoch 24/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6486\n",
            "Epoch 25/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6452\n",
            "Epoch 26/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6419\n",
            "Epoch 27/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6388\n",
            "Epoch 28/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6356\n",
            "Epoch 29/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6325\n",
            "Epoch 30/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6295\n",
            "Epoch 31/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6265\n",
            "Epoch 32/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6238\n",
            "Epoch 33/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6209\n",
            "Epoch 34/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6183\n",
            "Epoch 35/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6155\n",
            "Epoch 36/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6129\n",
            "Epoch 37/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6103\n",
            "Epoch 38/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6078\n",
            "Epoch 39/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6052\n",
            "Epoch 40/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6027\n",
            "Epoch 41/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6003\n",
            "Epoch 42/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5979\n",
            "Epoch 43/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5956\n",
            "Epoch 44/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5934\n",
            "Epoch 45/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5911\n",
            "Epoch 46/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5889\n",
            "Epoch 47/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5866\n",
            "Epoch 48/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5845\n",
            "Epoch 49/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5823\n",
            "Epoch 50/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5802\n",
            "Epoch 51/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5781\n",
            "Epoch 52/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5760\n",
            "Epoch 53/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5740\n",
            "Epoch 54/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5720\n",
            "Epoch 55/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5699\n",
            "Epoch 56/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5680\n",
            "Epoch 57/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5660\n",
            "Epoch 58/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5642\n",
            "Epoch 59/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5622\n",
            "Epoch 60/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5603\n",
            "Epoch 61/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5585\n",
            "Epoch 62/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5566\n",
            "Epoch 63/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5549\n",
            "Epoch 64/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5530\n",
            "Epoch 65/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5512\n",
            "Epoch 66/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5494\n",
            "Epoch 67/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5477\n",
            "Epoch 68/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5460\n",
            "Epoch 69/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5442\n",
            "Epoch 70/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5425\n",
            "Epoch 71/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5408\n",
            "Epoch 72/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5392\n",
            "Epoch 73/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5375\n",
            "Epoch 74/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5360\n",
            "Epoch 75/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5343\n",
            "Epoch 76/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5327\n",
            "Epoch 77/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5311\n",
            "Epoch 78/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5295\n",
            "Epoch 79/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5279\n",
            "Epoch 80/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5263\n",
            "Epoch 81/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5249\n",
            "Epoch 82/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5233\n",
            "Epoch 83/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5217\n",
            "Epoch 84/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5202\n",
            "Epoch 85/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5189\n",
            "Epoch 86/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5173\n",
            "Epoch 87/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5159\n",
            "Epoch 88/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5144\n",
            "Epoch 89/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5130\n",
            "Epoch 90/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5116\n",
            "Epoch 91/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5101\n",
            "Epoch 92/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5087\n",
            "Epoch 93/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5074\n",
            "Epoch 94/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5060\n",
            "Epoch 95/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5046\n",
            "Epoch 96/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5032\n",
            "Epoch 97/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5019\n",
            "Epoch 98/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5006\n",
            "Epoch 99/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4992\n",
            "Epoch 100/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4979\n",
            "gamma = 12.699208415745598\n",
            "acc = 0.6127678374199249\n",
            "--------time QMKDCsgd---------------\n",
            "461.86116123199463\n",
            "Epoch 1/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.0234\n",
            "Epoch 2/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.9132\n",
            "Epoch 3/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8619\n",
            "Epoch 4/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.8287\n",
            "Epoch 5/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.8040\n",
            "Epoch 6/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7844\n",
            "Epoch 7/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7680\n",
            "Epoch 8/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7542\n",
            "Epoch 9/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7420\n",
            "Epoch 10/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7312\n",
            "Epoch 11/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7214\n",
            "Epoch 12/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7124\n",
            "Epoch 13/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7041\n",
            "Epoch 14/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6963\n",
            "Epoch 15/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6890\n",
            "Epoch 16/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6821\n",
            "Epoch 17/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6756\n",
            "Epoch 18/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6693\n",
            "Epoch 19/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6633\n",
            "Epoch 20/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6576\n",
            "Epoch 21/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6519\n",
            "Epoch 22/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6466\n",
            "Epoch 23/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6414\n",
            "Epoch 24/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6363\n",
            "Epoch 25/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6313\n",
            "Epoch 26/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6265\n",
            "Epoch 27/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6218\n",
            "Epoch 28/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6172\n",
            "Epoch 29/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6126\n",
            "Epoch 30/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6083\n",
            "Epoch 31/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6040\n",
            "Epoch 32/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5997\n",
            "Epoch 33/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5956\n",
            "Epoch 34/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5915\n",
            "Epoch 35/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5875\n",
            "Epoch 36/100\n",
            "1132/1132 [==============================] - 5s 5ms/step - loss: 0.5836\n",
            "Epoch 37/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5798\n",
            "Epoch 38/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5760\n",
            "Epoch 39/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5722\n",
            "Epoch 40/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5686\n",
            "Epoch 41/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5650\n",
            "Epoch 42/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5614\n",
            "Epoch 43/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5579\n",
            "Epoch 44/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5545\n",
            "Epoch 45/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5511\n",
            "Epoch 46/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5478\n",
            "Epoch 47/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5444\n",
            "Epoch 48/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5412\n",
            "Epoch 49/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5380\n",
            "Epoch 50/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5348\n",
            "Epoch 51/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5317\n",
            "Epoch 52/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5287\n",
            "Epoch 53/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5256\n",
            "Epoch 54/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5226\n",
            "Epoch 55/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5197\n",
            "Epoch 56/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5167\n",
            "Epoch 57/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5139\n",
            "Epoch 58/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5110\n",
            "Epoch 59/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5082\n",
            "Epoch 60/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5054\n",
            "Epoch 61/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5026\n",
            "Epoch 62/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5000\n",
            "Epoch 63/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4973\n",
            "Epoch 64/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4946\n",
            "Epoch 65/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4920\n",
            "Epoch 66/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4894\n",
            "Epoch 67/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4869\n",
            "Epoch 68/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4843\n",
            "Epoch 69/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4818\n",
            "Epoch 70/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4794\n",
            "Epoch 71/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4769\n",
            "Epoch 72/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4745\n",
            "Epoch 73/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4721\n",
            "Epoch 74/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4697\n",
            "Epoch 75/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4674\n",
            "Epoch 76/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4651\n",
            "Epoch 77/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4628\n",
            "Epoch 78/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4605\n",
            "Epoch 79/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4583\n",
            "Epoch 80/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4560\n",
            "Epoch 81/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4539\n",
            "Epoch 82/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4517\n",
            "Epoch 83/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4495\n",
            "Epoch 84/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4474\n",
            "Epoch 85/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4453\n",
            "Epoch 86/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4432\n",
            "Epoch 87/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4411\n",
            "Epoch 88/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4391\n",
            "Epoch 89/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4370\n",
            "Epoch 90/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4351\n",
            "Epoch 91/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4331\n",
            "Epoch 92/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4311\n",
            "Epoch 93/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4292\n",
            "Epoch 94/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4272\n",
            "Epoch 95/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4253\n",
            "Epoch 96/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4234\n",
            "Epoch 97/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4215\n",
            "Epoch 98/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4197\n",
            "Epoch 99/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4179\n",
            "Epoch 100/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4160\n",
            "gamma = 29.6279907931933\n",
            "acc = 0.6626905235255136\n",
            "--------time QMKDCsgd---------------\n",
            "467.54153203964233\n",
            "Epoch 1/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.1141\n",
            "Epoch 2/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.0550\n",
            "Epoch 3/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.0103\n",
            "Epoch 4/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.9746\n",
            "Epoch 5/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.9447\n",
            "Epoch 6/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.9192\n",
            "Epoch 7/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.8969\n",
            "Epoch 8/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.8770\n",
            "Epoch 9/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.8591\n",
            "Epoch 10/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.8429\n",
            "Epoch 11/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.8279\n",
            "Epoch 12/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.8140\n",
            "Epoch 13/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.8010\n",
            "Epoch 14/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7888\n",
            "Epoch 15/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7773\n",
            "Epoch 16/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7664\n",
            "Epoch 17/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7561\n",
            "Epoch 18/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7462\n",
            "Epoch 19/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7367\n",
            "Epoch 20/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7275\n",
            "Epoch 21/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7187\n",
            "Epoch 22/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7102\n",
            "Epoch 23/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7020\n",
            "Epoch 24/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6940\n",
            "Epoch 25/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6863\n",
            "Epoch 26/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6788\n",
            "Epoch 27/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6714\n",
            "Epoch 28/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6643\n",
            "Epoch 29/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6573\n",
            "Epoch 30/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6505\n",
            "Epoch 31/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6439\n",
            "Epoch 32/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6374\n",
            "Epoch 33/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6311\n",
            "Epoch 34/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6249\n",
            "Epoch 35/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6188\n",
            "Epoch 36/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6128\n",
            "Epoch 37/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6070\n",
            "Epoch 38/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6013\n",
            "Epoch 39/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5957\n",
            "Epoch 40/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5902\n",
            "Epoch 41/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5848\n",
            "Epoch 42/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5795\n",
            "Epoch 43/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5743\n",
            "Epoch 44/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5692\n",
            "Epoch 45/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5642\n",
            "Epoch 46/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5592\n",
            "Epoch 47/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5544\n",
            "Epoch 48/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5496\n",
            "Epoch 49/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5449\n",
            "Epoch 50/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5403\n",
            "Epoch 51/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5357\n",
            "Epoch 52/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5312\n",
            "Epoch 53/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5268\n",
            "Epoch 54/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5225\n",
            "Epoch 55/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5183\n",
            "Epoch 56/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5141\n",
            "Epoch 57/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5099\n",
            "Epoch 58/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5059\n",
            "Epoch 59/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5018\n",
            "Epoch 60/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4979\n",
            "Epoch 61/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4940\n",
            "Epoch 62/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4902\n",
            "Epoch 63/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4864\n",
            "Epoch 64/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4826\n",
            "Epoch 65/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4790\n",
            "Epoch 66/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4753\n",
            "Epoch 67/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4718\n",
            "Epoch 68/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4682\n",
            "Epoch 69/100\n",
            "1132/1132 [==============================] - 5s 5ms/step - loss: 0.4648\n",
            "Epoch 70/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4613\n",
            "Epoch 71/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4580\n",
            "Epoch 72/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4546\n",
            "Epoch 73/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4513\n",
            "Epoch 74/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4481\n",
            "Epoch 75/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4449\n",
            "Epoch 76/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4417\n",
            "Epoch 77/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4386\n",
            "Epoch 78/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4355\n",
            "Epoch 79/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4325\n",
            "Epoch 80/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4295\n",
            "Epoch 81/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4265\n",
            "Epoch 82/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4236\n",
            "Epoch 83/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4208\n",
            "Epoch 84/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4179\n",
            "Epoch 85/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4151\n",
            "Epoch 86/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4124\n",
            "Epoch 87/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4096\n",
            "Epoch 88/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4069\n",
            "Epoch 89/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4043\n",
            "Epoch 90/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4016\n",
            "Epoch 91/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.3990\n",
            "Epoch 92/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.3965\n",
            "Epoch 93/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.3940\n",
            "Epoch 94/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.3914\n",
            "Epoch 95/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.3890\n",
            "Epoch 96/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.3865\n",
            "Epoch 97/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.3841\n",
            "Epoch 98/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.3818\n",
            "Epoch 99/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.3794\n",
            "Epoch 100/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.3771\n",
            "gamma = 69.12382328910762\n",
            "acc = 0.6914071128782858\n",
            "--------time QMKDCsgd---------------\n",
            "470.255322933197\n",
            "Epoch 1/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.1341\n",
            "Epoch 2/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.1171\n",
            "Epoch 3/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.1007\n",
            "Epoch 4/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.0849\n",
            "Epoch 5/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.0695\n",
            "Epoch 6/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.0547\n",
            "Epoch 7/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.0403\n",
            "Epoch 8/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.0263\n",
            "Epoch 9/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.0127\n",
            "Epoch 10/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.9996\n",
            "Epoch 11/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.9868\n",
            "Epoch 12/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.9744\n",
            "Epoch 13/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.9623\n",
            "Epoch 14/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.9506\n",
            "Epoch 15/100\n",
            "1132/1132 [==============================] - 5s 5ms/step - loss: 0.9391\n",
            "Epoch 16/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.9280\n",
            "Epoch 17/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.9172\n",
            "Epoch 18/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.9066\n",
            "Epoch 19/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.8963\n",
            "Epoch 20/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.8862\n",
            "Epoch 21/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.8764\n",
            "Epoch 22/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.8667\n",
            "Epoch 23/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.8573\n",
            "Epoch 24/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.8481\n",
            "Epoch 25/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.8391\n",
            "Epoch 26/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.8303\n",
            "Epoch 27/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.8217\n",
            "Epoch 28/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.8133\n",
            "Epoch 29/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.8050\n",
            "Epoch 30/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7969\n",
            "Epoch 31/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7889\n",
            "Epoch 32/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7811\n",
            "Epoch 33/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7735\n",
            "Epoch 34/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7659\n",
            "Epoch 35/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7586\n",
            "Epoch 36/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7513\n",
            "Epoch 37/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7442\n",
            "Epoch 38/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7372\n",
            "Epoch 39/100\n",
            "1132/1132 [==============================] - 5s 5ms/step - loss: 0.7304\n",
            "Epoch 40/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7236\n",
            "Epoch 41/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7170\n",
            "Epoch 42/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7105\n",
            "Epoch 43/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7041\n",
            "Epoch 44/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6978\n",
            "Epoch 45/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6916\n",
            "Epoch 46/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6855\n",
            "Epoch 47/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6795\n",
            "Epoch 48/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6736\n",
            "Epoch 49/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6678\n",
            "Epoch 50/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6621\n",
            "Epoch 51/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6565\n",
            "Epoch 52/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6510\n",
            "Epoch 53/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6456\n",
            "Epoch 54/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6402\n",
            "Epoch 55/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6349\n",
            "Epoch 56/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6298\n",
            "Epoch 57/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6246\n",
            "Epoch 58/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6196\n",
            "Epoch 59/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6147\n",
            "Epoch 60/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6098\n",
            "Epoch 61/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6050\n",
            "Epoch 62/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6003\n",
            "Epoch 63/100\n",
            "1132/1132 [==============================] - 5s 5ms/step - loss: 0.5956\n",
            "Epoch 64/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5910\n",
            "Epoch 65/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5865\n",
            "Epoch 66/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5821\n",
            "Epoch 67/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5777\n",
            "Epoch 68/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5734\n",
            "Epoch 69/100\n",
            "1132/1132 [==============================] - 5s 5ms/step - loss: 0.5691\n",
            "Epoch 70/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5649\n",
            "Epoch 71/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5608\n",
            "Epoch 72/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5567\n",
            "Epoch 73/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5527\n",
            "Epoch 74/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5487\n",
            "Epoch 75/100\n",
            "1132/1132 [==============================] - 5s 5ms/step - loss: 0.5448\n",
            "Epoch 76/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5410\n",
            "Epoch 77/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5372\n",
            "Epoch 78/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5334\n",
            "Epoch 79/100\n",
            "1132/1132 [==============================] - 5s 5ms/step - loss: 0.5297\n",
            "Epoch 80/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5261\n",
            "Epoch 81/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5225\n",
            "Epoch 82/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5190\n",
            "Epoch 83/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5155\n",
            "Epoch 84/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5120\n",
            "Epoch 85/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5087\n",
            "Epoch 86/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5053\n",
            "Epoch 87/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5020\n",
            "Epoch 88/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4987\n",
            "Epoch 89/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4955\n",
            "Epoch 90/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4924\n",
            "Epoch 91/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4892\n",
            "Epoch 92/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4861\n",
            "Epoch 93/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4831\n",
            "Epoch 94/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4801\n",
            "Epoch 95/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.4771\n",
            "Epoch 96/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4742\n",
            "Epoch 97/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4713\n",
            "Epoch 98/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.4685\n",
            "Epoch 99/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.4656\n",
            "Epoch 100/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.4629\n",
            "gamma = 161.26989438654383\n",
            "acc = 0.6551800309255578\n",
            "--------time QMKDCsgd---------------\n",
            "483.7443745136261\n",
            "Epoch 1/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.1502\n",
            "Epoch 2/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.1392\n",
            "Epoch 3/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.1285\n",
            "Epoch 4/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.1181\n",
            "Epoch 5/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.1080\n",
            "Epoch 6/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.0980\n",
            "Epoch 7/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.0882\n",
            "Epoch 8/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.0786\n",
            "Epoch 9/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.0692\n",
            "Epoch 10/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.0600\n",
            "Epoch 11/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.0508\n",
            "Epoch 12/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.0419\n",
            "Epoch 13/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.0331\n",
            "Epoch 14/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.0244\n",
            "Epoch 15/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.0159\n",
            "Epoch 16/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.0075\n",
            "Epoch 17/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.9992\n",
            "Epoch 18/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.9910\n",
            "Epoch 19/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.9830\n",
            "Epoch 20/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.9751\n",
            "Epoch 21/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.9673\n",
            "Epoch 22/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.9596\n",
            "Epoch 23/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.9520\n",
            "Epoch 24/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.9445\n",
            "Epoch 25/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.9372\n",
            "Epoch 26/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.9299\n",
            "Epoch 27/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.9227\n",
            "Epoch 28/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.9157\n",
            "Epoch 29/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.9087\n",
            "Epoch 30/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.9018\n",
            "Epoch 31/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8950\n",
            "Epoch 32/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8883\n",
            "Epoch 33/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8817\n",
            "Epoch 34/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8752\n",
            "Epoch 35/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8688\n",
            "Epoch 36/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8625\n",
            "Epoch 37/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8562\n",
            "Epoch 38/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8500\n",
            "Epoch 39/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8439\n",
            "Epoch 40/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8379\n",
            "Epoch 41/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.8320\n",
            "Epoch 42/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8261\n",
            "Epoch 43/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8203\n",
            "Epoch 44/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8146\n",
            "Epoch 45/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8089\n",
            "Epoch 46/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8034\n",
            "Epoch 47/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7979\n",
            "Epoch 48/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7925\n",
            "Epoch 49/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7871\n",
            "Epoch 50/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7818\n",
            "Epoch 51/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7766\n",
            "Epoch 52/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7714\n",
            "Epoch 53/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7663\n",
            "Epoch 54/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7613\n",
            "Epoch 55/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7563\n",
            "Epoch 56/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7514\n",
            "Epoch 57/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7465\n",
            "Epoch 58/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7418\n",
            "Epoch 59/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7370\n",
            "Epoch 60/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7324\n",
            "Epoch 61/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7277\n",
            "Epoch 62/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7232\n",
            "Epoch 63/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7187\n",
            "Epoch 64/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7142\n",
            "Epoch 65/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7099\n",
            "Epoch 66/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7055\n",
            "Epoch 67/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7012\n",
            "Epoch 68/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6970\n",
            "Epoch 69/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6928\n",
            "Epoch 70/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6887\n",
            "Epoch 71/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6846\n",
            "Epoch 72/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6806\n",
            "Epoch 73/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6766\n",
            "Epoch 74/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6726\n",
            "Epoch 75/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6687\n",
            "Epoch 76/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6649\n",
            "Epoch 77/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6611\n",
            "Epoch 78/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6574\n",
            "Epoch 79/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6536\n",
            "Epoch 80/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6500\n",
            "Epoch 81/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6463\n",
            "Epoch 82/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6428\n",
            "Epoch 83/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6392\n",
            "Epoch 84/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6357\n",
            "Epoch 85/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6323\n",
            "Epoch 86/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6289\n",
            "Epoch 87/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6255\n",
            "Epoch 88/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6221\n",
            "Epoch 89/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6188\n",
            "Epoch 90/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6156\n",
            "Epoch 91/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6124\n",
            "Epoch 92/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6092\n",
            "Epoch 93/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6060\n",
            "Epoch 94/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6029\n",
            "Epoch 95/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5998\n",
            "Epoch 96/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5968\n",
            "Epoch 97/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5938\n",
            "Epoch 98/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5908\n",
            "Epoch 99/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.5878\n",
            "Epoch 100/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.5849\n",
            "gamma = 376.2520300225537\n",
            "acc = 0.5345703556439143\n",
            "--------time QMKDCsgd---------------\n",
            "437.8263998031616\n",
            "Epoch 1/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.1434\n",
            "Epoch 2/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.1353\n",
            "Epoch 3/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.1274\n",
            "Epoch 4/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.1196\n",
            "Epoch 5/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.1120\n",
            "Epoch 6/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.1045\n",
            "Epoch 7/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.0971\n",
            "Epoch 8/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.0898\n",
            "Epoch 9/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.0827\n",
            "Epoch 10/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.0756\n",
            "Epoch 11/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.0686\n",
            "Epoch 12/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.0617\n",
            "Epoch 13/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.0549\n",
            "Epoch 14/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.0482\n",
            "Epoch 15/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.0416\n",
            "Epoch 16/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.0350\n",
            "Epoch 17/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.0286\n",
            "Epoch 18/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.0222\n",
            "Epoch 19/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.0159\n",
            "Epoch 20/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.0096\n",
            "Epoch 21/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.0034\n",
            "Epoch 22/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.9973\n",
            "Epoch 23/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.9913\n",
            "Epoch 24/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.9853\n",
            "Epoch 25/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.9794\n",
            "Epoch 26/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.9735\n",
            "Epoch 27/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.9678\n",
            "Epoch 28/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.9620\n",
            "Epoch 29/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.9564\n",
            "Epoch 30/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.9508\n",
            "Epoch 31/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.9452\n",
            "Epoch 32/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.9397\n",
            "Epoch 33/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.9343\n",
            "Epoch 34/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.9289\n",
            "Epoch 35/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.9236\n",
            "Epoch 36/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.9183\n",
            "Epoch 37/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.9131\n",
            "Epoch 38/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.9080\n",
            "Epoch 39/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.9028\n",
            "Epoch 40/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8978\n",
            "Epoch 41/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.8928\n",
            "Epoch 42/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8878\n",
            "Epoch 43/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8829\n",
            "Epoch 44/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8781\n",
            "Epoch 45/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8732\n",
            "Epoch 46/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8685\n",
            "Epoch 47/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8638\n",
            "Epoch 48/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8591\n",
            "Epoch 49/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.8545\n",
            "Epoch 50/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8499\n",
            "Epoch 51/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.8454\n",
            "Epoch 52/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8409\n",
            "Epoch 53/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.8364\n",
            "Epoch 54/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8320\n",
            "Epoch 55/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.8277\n",
            "Epoch 56/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.8234\n",
            "Epoch 57/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.8191\n",
            "Epoch 58/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.8149\n",
            "Epoch 59/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.8107\n",
            "Epoch 60/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8065\n",
            "Epoch 61/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8024\n",
            "Epoch 62/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7984\n",
            "Epoch 63/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7943\n",
            "Epoch 64/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7903\n",
            "Epoch 65/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7864\n",
            "Epoch 66/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7825\n",
            "Epoch 67/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7786\n",
            "Epoch 68/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7748\n",
            "Epoch 69/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7710\n",
            "Epoch 70/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7672\n",
            "Epoch 71/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7635\n",
            "Epoch 72/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7598\n",
            "Epoch 73/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7562\n",
            "Epoch 74/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7525\n",
            "Epoch 75/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7490\n",
            "Epoch 76/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7454\n",
            "Epoch 77/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7419\n",
            "Epoch 78/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7384\n",
            "Epoch 79/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7350\n",
            "Epoch 80/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7315\n",
            "Epoch 81/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7282\n",
            "Epoch 82/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7248\n",
            "Epoch 83/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7215\n",
            "Epoch 84/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7182\n",
            "Epoch 85/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7150\n",
            "Epoch 86/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7117\n",
            "Epoch 87/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7085\n",
            "Epoch 88/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7054\n",
            "Epoch 89/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7022\n",
            "Epoch 90/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6991\n",
            "Epoch 91/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6961\n",
            "Epoch 92/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6930\n",
            "Epoch 93/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6900\n",
            "Epoch 94/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.6870\n",
            "Epoch 95/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6841\n",
            "Epoch 96/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6811\n",
            "Epoch 97/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6782\n",
            "Epoch 98/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6754\n",
            "Epoch 99/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6725\n",
            "Epoch 100/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.6697\n",
            "gamma = 877.8178384415471\n",
            "acc = 0.4340622929092114\n",
            "--------time QMKDCsgd---------------\n",
            "444.88624715805054\n",
            "Epoch 1/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.1439\n",
            "Epoch 2/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.1373\n",
            "Epoch 3/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.1309\n",
            "Epoch 4/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.1245\n",
            "Epoch 5/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.1182\n",
            "Epoch 6/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.1121\n",
            "Epoch 7/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.1060\n",
            "Epoch 8/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.0999\n",
            "Epoch 9/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.0940\n",
            "Epoch 10/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.0881\n",
            "Epoch 11/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.0822\n",
            "Epoch 12/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.0765\n",
            "Epoch 13/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.0708\n",
            "Epoch 14/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.0651\n",
            "Epoch 15/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.0595\n",
            "Epoch 16/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.0540\n",
            "Epoch 17/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.0485\n",
            "Epoch 18/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.0431\n",
            "Epoch 19/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.0377\n",
            "Epoch 20/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.0324\n",
            "Epoch 21/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.0271\n",
            "Epoch 22/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.0219\n",
            "Epoch 23/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.0167\n",
            "Epoch 24/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.0115\n",
            "Epoch 25/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.0064\n",
            "Epoch 26/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.0014\n",
            "Epoch 27/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.9964\n",
            "Epoch 28/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.9914\n",
            "Epoch 29/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.9865\n",
            "Epoch 30/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.9816\n",
            "Epoch 31/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.9768\n",
            "Epoch 32/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.9720\n",
            "Epoch 33/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.9673\n",
            "Epoch 34/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.9625\n",
            "Epoch 35/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.9579\n",
            "Epoch 36/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.9532\n",
            "Epoch 37/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.9486\n",
            "Epoch 38/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.9441\n",
            "Epoch 39/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.9396\n",
            "Epoch 40/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.9351\n",
            "Epoch 41/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.9306\n",
            "Epoch 42/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.9262\n",
            "Epoch 43/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.9219\n",
            "Epoch 44/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.9175\n",
            "Epoch 45/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.9132\n",
            "Epoch 46/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.9089\n",
            "Epoch 47/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.9047\n",
            "Epoch 48/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.9005\n",
            "Epoch 49/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8964\n",
            "Epoch 50/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8922\n",
            "Epoch 51/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8881\n",
            "Epoch 52/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8841\n",
            "Epoch 53/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8800\n",
            "Epoch 54/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8760\n",
            "Epoch 55/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8721\n",
            "Epoch 56/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8681\n",
            "Epoch 57/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8642\n",
            "Epoch 58/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8604\n",
            "Epoch 59/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.8565\n",
            "Epoch 60/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8527\n",
            "Epoch 61/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8490\n",
            "Epoch 62/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8452\n",
            "Epoch 63/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.8415\n",
            "Epoch 64/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8378\n",
            "Epoch 65/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.8342\n",
            "Epoch 66/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8306\n",
            "Epoch 67/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8270\n",
            "Epoch 68/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.8234\n",
            "Epoch 69/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8199\n",
            "Epoch 70/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8164\n",
            "Epoch 71/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8129\n",
            "Epoch 72/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.8094\n",
            "Epoch 73/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8060\n",
            "Epoch 74/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8026\n",
            "Epoch 75/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7993\n",
            "Epoch 76/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7959\n",
            "Epoch 77/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7926\n",
            "Epoch 78/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7893\n",
            "Epoch 79/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7861\n",
            "Epoch 80/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7828\n",
            "Epoch 81/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7796\n",
            "Epoch 82/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7765\n",
            "Epoch 83/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7733\n",
            "Epoch 84/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7702\n",
            "Epoch 85/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7671\n",
            "Epoch 86/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7640\n",
            "Epoch 87/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7610\n",
            "Epoch 88/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7580\n",
            "Epoch 89/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7550\n",
            "Epoch 90/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7520\n",
            "Epoch 91/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7490\n",
            "Epoch 92/100\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7461\n",
            "Epoch 93/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7432\n",
            "Epoch 94/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7404\n",
            "Epoch 95/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7375\n",
            "Epoch 96/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7347\n",
            "Epoch 97/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7319\n",
            "Epoch 98/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7291\n",
            "Epoch 99/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7263\n",
            "Epoch 100/100\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7236\n",
            "gamma = 2048.0\n",
            "acc = 0.3626021647890435\n",
            "--------time QMKDCsgd---------------\n",
            "438.51861906051636\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhojzdVRigId",
        "outputId": "e940d8d4-ccb1-40db-f0e6-7933fbb59ef4"
      },
      "source": [
        "#set_gamma_a =  np.logspace(0, 11, num=10, base=2).tolist()\n",
        "set_dim_x = [1000, 2000, 3000, 4000, 5000]\n",
        "# set_lr_b = [0.00005, 0.0005, 0.5, 10]\n",
        "# set_lr_c = [0.000005, 0.00005, 1, 10]\n",
        "# set_lr_d = [0.0005, 0.005, 0.05, 0.5, 1, 10]\n",
        "# set_lr_e = [0.0005, 0.0025, 0.005]\n",
        "\n",
        "for dimm in set_dim_x:\n",
        "  exp_time = time()\n",
        "  qmkdc1_dig = models.QMKDClassifierSGD(input_dim=100, dim_x=dimm, num_classes=4, num_eig=50, gamma=2**0, random_state=0)\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=0.000005, beta_1=0.9, beta_2=0.999, decay=1e-9)\n",
        "  qmkdc1_dig.compile(optimizer, loss=tf.keras.losses.CategoricalCrossentropy())\n",
        "  y_train_bin = tf.reshape(tf.keras.backend.one_hot(y_train, 4), (-1,4))\n",
        "  qmkdc1_dig.fit(X_train, y_train_bin, epochs=20)\n",
        "  out = qmkdc1_dig.predict(X_test)\n",
        "  print(f'dim_x = {dimm}')\n",
        "  print(f'acc = {accuracy_score(y_test, np.argmax(out, axis=1))}')\n",
        "  print(\"--------time QMKDCsgd---------------\")\n",
        "  print(time() - exp_time)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 1.0411\n",
            "Epoch 2/20\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8982\n",
            "Epoch 3/20\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8415\n",
            "Epoch 4/20\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8235\n",
            "Epoch 5/20\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.8108\n",
            "Epoch 6/20\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8008\n",
            "Epoch 7/20\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7924\n",
            "Epoch 8/20\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7853\n",
            "Epoch 9/20\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7789\n",
            "Epoch 10/20\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7731\n",
            "Epoch 11/20\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7684\n",
            "Epoch 12/20\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7641\n",
            "Epoch 13/20\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7599\n",
            "Epoch 14/20\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7563\n",
            "Epoch 15/20\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7531\n",
            "Epoch 16/20\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7501\n",
            "Epoch 17/20\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7473\n",
            "Epoch 18/20\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7444\n",
            "Epoch 19/20\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7420\n",
            "Epoch 20/20\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7399\n",
            "dim_x = 1000\n",
            "acc = 0.4825491495471615\n",
            "--------time QMKDCsgd---------------\n",
            "88.75805997848511\n",
            "Epoch 1/20\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 1.0081\n",
            "Epoch 2/20\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8683\n",
            "Epoch 3/20\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8317\n",
            "Epoch 4/20\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8128\n",
            "Epoch 5/20\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7989\n",
            "Epoch 6/20\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7879\n",
            "Epoch 7/20\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7788\n",
            "Epoch 8/20\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7714\n",
            "Epoch 9/20\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7647\n",
            "Epoch 10/20\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7593\n",
            "Epoch 11/20\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7543\n",
            "Epoch 12/20\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7498\n",
            "Epoch 13/20\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7459\n",
            "Epoch 14/20\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7424\n",
            "Epoch 15/20\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7392\n",
            "Epoch 16/20\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7362\n",
            "Epoch 17/20\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7335\n",
            "Epoch 18/20\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7311\n",
            "Epoch 19/20\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7284\n",
            "Epoch 20/20\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7264\n",
            "dim_x = 2000\n",
            "acc = 0.48961784846476697\n",
            "--------time QMKDCsgd---------------\n",
            "88.00789833068848\n",
            "Epoch 1/20\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.9915\n",
            "Epoch 2/20\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8463\n",
            "Epoch 3/20\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8132\n",
            "Epoch 4/20\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7959\n",
            "Epoch 5/20\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7836\n",
            "Epoch 6/20\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7735\n",
            "Epoch 7/20\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7656\n",
            "Epoch 8/20\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7587\n",
            "Epoch 9/20\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7528\n",
            "Epoch 10/20\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7475\n",
            "Epoch 11/20\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7432\n",
            "Epoch 12/20\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7392\n",
            "Epoch 13/20\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7352\n",
            "Epoch 14/20\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7322\n",
            "Epoch 15/20\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7295\n",
            "Epoch 16/20\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7267\n",
            "Epoch 17/20\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7242\n",
            "Epoch 18/20\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7216\n",
            "Epoch 19/20\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7198\n",
            "Epoch 20/20\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7174\n",
            "dim_x = 3000\n",
            "acc = 0.48420587585597524\n",
            "--------time QMKDCsgd---------------\n",
            "88.5886435508728\n",
            "Epoch 1/20\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.9769\n",
            "Epoch 2/20\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.8387\n",
            "Epoch 3/20\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.8111\n",
            "Epoch 4/20\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7935\n",
            "Epoch 5/20\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7808\n",
            "Epoch 6/20\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7706\n",
            "Epoch 7/20\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7623\n",
            "Epoch 8/20\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7557\n",
            "Epoch 9/20\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7496\n",
            "Epoch 10/20\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7445\n",
            "Epoch 11/20\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7400\n",
            "Epoch 12/20\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7361\n",
            "Epoch 13/20\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7327\n",
            "Epoch 14/20\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7294\n",
            "Epoch 15/20\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7267\n",
            "Epoch 16/20\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7239\n",
            "Epoch 17/20\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7216\n",
            "Epoch 18/20\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7193\n",
            "Epoch 19/20\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7168\n",
            "Epoch 20/20\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7148\n",
            "dim_x = 4000\n",
            "acc = 0.48740888005301525\n",
            "--------time QMKDCsgd---------------\n",
            "91.98553895950317\n",
            "Epoch 1/20\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.9463\n",
            "Epoch 2/20\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.8286\n",
            "Epoch 3/20\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7999\n",
            "Epoch 4/20\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7815\n",
            "Epoch 5/20\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7688\n",
            "Epoch 6/20\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7584\n",
            "Epoch 7/20\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7508\n",
            "Epoch 8/20\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7442\n",
            "Epoch 9/20\n",
            "1132/1132 [==============================] - 5s 5ms/step - loss: 0.7384\n",
            "Epoch 10/20\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7342\n",
            "Epoch 11/20\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7301\n",
            "Epoch 12/20\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7265\n",
            "Epoch 13/20\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7234\n",
            "Epoch 14/20\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7204\n",
            "Epoch 15/20\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7173\n",
            "Epoch 16/20\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7150\n",
            "Epoch 17/20\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7127\n",
            "Epoch 18/20\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7107\n",
            "Epoch 19/20\n",
            "1132/1132 [==============================] - 4s 4ms/step - loss: 0.7088\n",
            "Epoch 20/20\n",
            "1132/1132 [==============================] - 5s 4ms/step - loss: 0.7063\n",
            "dim_x = 5000\n",
            "acc = 0.501325381047051\n",
            "--------time QMKDCsgd---------------\n",
            "93.22727084159851\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIFfKrl3Y-e6"
      },
      "source": [
        "###1. random_state=None, num_classes=5, lr=5e-07, input_dim=100, gamma=4, eig_percentage=0.25, decay=0, component_dim=1000"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHkfC1_u44Bo"
      },
      "source": [
        "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMWShZaC96dM"
      },
      "source": [
        "import os\n",
        "import keras\n",
        "from sklearn.metrics import make_scorer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NttrnXCRYvqE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9c380c16-c8d9-476b-d35c-c7aabf94aee7"
      },
      "source": [
        "input_dim = X_train.shape[1]\n",
        "num_classes = np.unique(y_train).shape[0]\n",
        "gammas = 4\n",
        "learning_rate= 5e-7\n",
        "component_dim= 1000\n",
        "num_eigs= 0.25\n",
        "epochs = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
        "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
        "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
        "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
        "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
        "decay = 0\n",
        "Acc_3 = []\n",
        "\n",
        "num_eig = round(num_eigs * component_dim)\n",
        "\n",
        "fm_x = layers.QFeatureMapRFF(input_dim=100, dim=component_dim, gamma=gammas, random_state=0)\n",
        "#qmkdc\n",
        "qmkdc = models.QMKDClassifier(fm_x=fm_x, dim_x=component_dim, num_classes=num_classes)\n",
        "qmkdc.compile()\n",
        "qmkdc.fit(X_train, y_train, epochs=1, batch_size = 256, verbose = 20)\n",
        "\n",
        "#qmkdc SGD\n",
        "qmkdc2 = models.QMKDClassifierSGD(input_dim=100, dim_x=component_dim, num_classes=num_classes, num_eig=num_eig, gamma=gammas, random_state=0)\n",
        "qmkdc2.set_rhos(qmkdc.get_rhos())\n",
        "\n",
        "#Train model with SGD\n",
        "qmkdc2.layers[0].trainable = False\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate, beta_1=0.9, beta_2=0.999, decay = decay)\n",
        "\n",
        "\n",
        "qmkdc2.compile(optimizer= optimizer, loss = tf.keras.losses.CategoricalCrossentropy(), metrics=['accuracy'])\n",
        "qmkdc2.set_rhos(qmkdc.get_rhos())\n",
        "\n",
        "y_train_bin = tf.reshape(tf.keras.backend.one_hot(y_train, num_classes), (-1,num_classes))\n",
        "y_test_bin = tf.reshape(tf.keras.backend.one_hot(y_test, num_classes), (-1,num_classes))\n",
        "\n",
        "checkpoint = ModelCheckpoint('/content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5', \n",
        "                            monitor='val_loss', \n",
        "                            verbose=1, \n",
        "                            save_best_only = True, \n",
        "                            mode='auto')\n",
        "early_stop = EarlyStopping( monitor=\"val_loss\",\n",
        "                            min_delta=0,\n",
        "                            patience=5,\n",
        "                            verbose=1,\n",
        "                            mode=\"auto\",\n",
        "                            restore_best_weights=True,\n",
        "                            )\n",
        "# for e in epochs:\n",
        "#   qmkdc2.fit(X_train, y_train_bin, validation_split = 0.2, epochs=e, batch_size = 256, callbacks = [checkpoint, early_stop])\n",
        "#   out = qmkdc2.predict(X_test)\n",
        "#   y_pred = np.argmax(out, axis = 1).reshape(np.argmax(out, axis = 1).shape[0],1)\n",
        "#   Result_3 = accuracy_score(y_test, y_pred)\n",
        "#   Acc_3.append(Result_3)\n",
        "\n",
        "# print(Acc_3)\n",
        "\n",
        "history = qmkdc2.fit(X_train, y_train_bin, validation_split = 0.2, epochs=1000, batch_size = 256, callbacks = [checkpoint, early_stop])\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "113/114 [============================>.] - ETA: 0s - loss: 1.5550 - accuracy: 0.4317\n",
            "Epoch 00001: val_loss improved from inf to 1.55398, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 9ms/step - loss: 1.5550 - accuracy: 0.4317 - val_loss: 1.5540 - val_accuracy: 0.4335\n",
            "Epoch 2/1000\n",
            "108/114 [===========================>..] - ETA: 0s - loss: 1.5546 - accuracy: 0.4467\n",
            "Epoch 00002: val_loss improved from 1.55398 to 1.55347, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5545 - accuracy: 0.4476 - val_loss: 1.5535 - val_accuracy: 0.4504\n",
            "Epoch 3/1000\n",
            "110/114 [===========================>..] - ETA: 0s - loss: 1.5540 - accuracy: 0.4603\n",
            "Epoch 00003: val_loss improved from 1.55347 to 1.55297, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5540 - accuracy: 0.4609 - val_loss: 1.5530 - val_accuracy: 0.4608\n",
            "Epoch 4/1000\n",
            "106/114 [==========================>...] - ETA: 0s - loss: 1.5533 - accuracy: 0.4643\n",
            "Epoch 00004: val_loss improved from 1.55297 to 1.55251, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5535 - accuracy: 0.4644 - val_loss: 1.5525 - val_accuracy: 0.4644\n",
            "Epoch 5/1000\n",
            "107/114 [===========================>..] - ETA: 0s - loss: 1.5531 - accuracy: 0.4663\n",
            "Epoch 00005: val_loss improved from 1.55251 to 1.55205, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5530 - accuracy: 0.4671 - val_loss: 1.5521 - val_accuracy: 0.4671\n",
            "Epoch 6/1000\n",
            "105/114 [==========================>...] - ETA: 0s - loss: 1.5527 - accuracy: 0.4660\n",
            "Epoch 00006: val_loss improved from 1.55205 to 1.55162, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.5526 - accuracy: 0.4660 - val_loss: 1.5516 - val_accuracy: 0.4714\n",
            "Epoch 7/1000\n",
            "110/114 [===========================>..] - ETA: 0s - loss: 1.5521 - accuracy: 0.4659\n",
            "Epoch 00007: val_loss improved from 1.55162 to 1.55118, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5522 - accuracy: 0.4651 - val_loss: 1.5512 - val_accuracy: 0.4736\n",
            "Epoch 8/1000\n",
            "112/114 [============================>.] - ETA: 0s - loss: 1.5517 - accuracy: 0.4636\n",
            "Epoch 00008: val_loss improved from 1.55118 to 1.55076, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5517 - accuracy: 0.4636 - val_loss: 1.5508 - val_accuracy: 0.4746\n",
            "Epoch 9/1000\n",
            "109/114 [===========================>..] - ETA: 0s - loss: 1.5514 - accuracy: 0.4653\n",
            "Epoch 00009: val_loss improved from 1.55076 to 1.55034, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5513 - accuracy: 0.4652 - val_loss: 1.5503 - val_accuracy: 0.4781\n",
            "Epoch 10/1000\n",
            "112/114 [============================>.] - ETA: 0s - loss: 1.5509 - accuracy: 0.4634\n",
            "Epoch 00010: val_loss improved from 1.55034 to 1.54992, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5509 - accuracy: 0.4640 - val_loss: 1.5499 - val_accuracy: 0.4767\n",
            "Epoch 11/1000\n",
            "110/114 [===========================>..] - ETA: 0s - loss: 1.5505 - accuracy: 0.4638\n",
            "Epoch 00011: val_loss improved from 1.54992 to 1.54951, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.5505 - accuracy: 0.4645 - val_loss: 1.5495 - val_accuracy: 0.4746\n",
            "Epoch 12/1000\n",
            "107/114 [===========================>..] - ETA: 0s - loss: 1.5501 - accuracy: 0.4617\n",
            "Epoch 00012: val_loss improved from 1.54951 to 1.54911, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5501 - accuracy: 0.4619 - val_loss: 1.5491 - val_accuracy: 0.4716\n",
            "Epoch 13/1000\n",
            "110/114 [===========================>..] - ETA: 0s - loss: 1.5497 - accuracy: 0.4616\n",
            "Epoch 00013: val_loss improved from 1.54911 to 1.54870, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5497 - accuracy: 0.4619 - val_loss: 1.5487 - val_accuracy: 0.4707\n",
            "Epoch 14/1000\n",
            "109/114 [===========================>..] - ETA: 0s - loss: 1.5493 - accuracy: 0.4599\n",
            "Epoch 00014: val_loss improved from 1.54870 to 1.54830, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5493 - accuracy: 0.4603 - val_loss: 1.5483 - val_accuracy: 0.4669\n",
            "Epoch 15/1000\n",
            "109/114 [===========================>..] - ETA: 0s - loss: 1.5490 - accuracy: 0.4596\n",
            "Epoch 00015: val_loss improved from 1.54830 to 1.54789, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5489 - accuracy: 0.4590 - val_loss: 1.5479 - val_accuracy: 0.4665\n",
            "Epoch 16/1000\n",
            "106/114 [==========================>...] - ETA: 0s - loss: 1.5486 - accuracy: 0.4570\n",
            "Epoch 00016: val_loss improved from 1.54789 to 1.54750, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5485 - accuracy: 0.4574 - val_loss: 1.5475 - val_accuracy: 0.4649\n",
            "Epoch 17/1000\n",
            "110/114 [===========================>..] - ETA: 0s - loss: 1.5481 - accuracy: 0.4575\n",
            "Epoch 00017: val_loss improved from 1.54750 to 1.54710, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5481 - accuracy: 0.4578 - val_loss: 1.5471 - val_accuracy: 0.4659\n",
            "Epoch 18/1000\n",
            "113/114 [============================>.] - ETA: 0s - loss: 1.5477 - accuracy: 0.4565\n",
            "Epoch 00018: val_loss improved from 1.54710 to 1.54670, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5477 - accuracy: 0.4564 - val_loss: 1.5467 - val_accuracy: 0.4659\n",
            "Epoch 19/1000\n",
            "108/114 [===========================>..] - ETA: 0s - loss: 1.5472 - accuracy: 0.4560\n",
            "Epoch 00019: val_loss improved from 1.54670 to 1.54630, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5473 - accuracy: 0.4563 - val_loss: 1.5463 - val_accuracy: 0.4663\n",
            "Epoch 20/1000\n",
            "109/114 [===========================>..] - ETA: 0s - loss: 1.5468 - accuracy: 0.4569\n",
            "Epoch 00020: val_loss improved from 1.54630 to 1.54591, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5469 - accuracy: 0.4555 - val_loss: 1.5459 - val_accuracy: 0.4658\n",
            "Epoch 21/1000\n",
            "112/114 [============================>.] - ETA: 0s - loss: 1.5465 - accuracy: 0.4551\n",
            "Epoch 00021: val_loss improved from 1.54591 to 1.54551, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5465 - accuracy: 0.4551 - val_loss: 1.5455 - val_accuracy: 0.4636\n",
            "Epoch 22/1000\n",
            "109/114 [===========================>..] - ETA: 0s - loss: 1.5462 - accuracy: 0.4541\n",
            "Epoch 00022: val_loss improved from 1.54551 to 1.54511, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.5461 - accuracy: 0.4549 - val_loss: 1.5451 - val_accuracy: 0.4637\n",
            "Epoch 23/1000\n",
            "106/114 [==========================>...] - ETA: 0s - loss: 1.5457 - accuracy: 0.4546\n",
            "Epoch 00023: val_loss improved from 1.54511 to 1.54472, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5457 - accuracy: 0.4538 - val_loss: 1.5447 - val_accuracy: 0.4625\n",
            "Epoch 24/1000\n",
            "114/114 [==============================] - ETA: 0s - loss: 1.5453 - accuracy: 0.4535\n",
            "Epoch 00024: val_loss improved from 1.54472 to 1.54433, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5453 - accuracy: 0.4535 - val_loss: 1.5443 - val_accuracy: 0.4630\n",
            "Epoch 25/1000\n",
            "112/114 [============================>.] - ETA: 0s - loss: 1.5449 - accuracy: 0.4528\n",
            "Epoch 00025: val_loss improved from 1.54433 to 1.54393, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5449 - accuracy: 0.4529 - val_loss: 1.5439 - val_accuracy: 0.4609\n",
            "Epoch 26/1000\n",
            "111/114 [============================>.] - ETA: 0s - loss: 1.5446 - accuracy: 0.4532\n",
            "Epoch 00026: val_loss improved from 1.54393 to 1.54354, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.5446 - accuracy: 0.4531 - val_loss: 1.5435 - val_accuracy: 0.4620\n",
            "Epoch 27/1000\n",
            "111/114 [============================>.] - ETA: 0s - loss: 1.5441 - accuracy: 0.4512\n",
            "Epoch 00027: val_loss improved from 1.54354 to 1.54314, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5442 - accuracy: 0.4509 - val_loss: 1.5431 - val_accuracy: 0.4608\n",
            "Epoch 28/1000\n",
            "108/114 [===========================>..] - ETA: 0s - loss: 1.5437 - accuracy: 0.4505\n",
            "Epoch 00028: val_loss improved from 1.54314 to 1.54275, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.5438 - accuracy: 0.4506 - val_loss: 1.5427 - val_accuracy: 0.4602\n",
            "Epoch 29/1000\n",
            "109/114 [===========================>..] - ETA: 0s - loss: 1.5433 - accuracy: 0.4518\n",
            "Epoch 00029: val_loss improved from 1.54275 to 1.54235, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5434 - accuracy: 0.4514 - val_loss: 1.5424 - val_accuracy: 0.4594\n",
            "Epoch 30/1000\n",
            "106/114 [==========================>...] - ETA: 0s - loss: 1.5430 - accuracy: 0.4504\n",
            "Epoch 00030: val_loss improved from 1.54235 to 1.54196, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5430 - accuracy: 0.4514 - val_loss: 1.5420 - val_accuracy: 0.4593\n",
            "Epoch 31/1000\n",
            "111/114 [============================>.] - ETA: 0s - loss: 1.5426 - accuracy: 0.4513\n",
            "Epoch 00031: val_loss improved from 1.54196 to 1.54157, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5426 - accuracy: 0.4508 - val_loss: 1.5416 - val_accuracy: 0.4589\n",
            "Epoch 32/1000\n",
            "106/114 [==========================>...] - ETA: 0s - loss: 1.5422 - accuracy: 0.4496\n",
            "Epoch 00032: val_loss improved from 1.54157 to 1.54118, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5422 - accuracy: 0.4505 - val_loss: 1.5412 - val_accuracy: 0.4583\n",
            "Epoch 33/1000\n",
            "109/114 [===========================>..] - ETA: 0s - loss: 1.5417 - accuracy: 0.4512\n",
            "Epoch 00033: val_loss improved from 1.54118 to 1.54078, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5418 - accuracy: 0.4503 - val_loss: 1.5408 - val_accuracy: 0.4586\n",
            "Epoch 34/1000\n",
            "107/114 [===========================>..] - ETA: 0s - loss: 1.5415 - accuracy: 0.4510\n",
            "Epoch 00034: val_loss improved from 1.54078 to 1.54039, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5414 - accuracy: 0.4508 - val_loss: 1.5404 - val_accuracy: 0.4589\n",
            "Epoch 35/1000\n",
            "110/114 [===========================>..] - ETA: 0s - loss: 1.5411 - accuracy: 0.4490\n",
            "Epoch 00035: val_loss improved from 1.54039 to 1.54000, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5411 - accuracy: 0.4496 - val_loss: 1.5400 - val_accuracy: 0.4597\n",
            "Epoch 36/1000\n",
            "107/114 [===========================>..] - ETA: 0s - loss: 1.5406 - accuracy: 0.4508\n",
            "Epoch 00036: val_loss improved from 1.54000 to 1.53961, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5407 - accuracy: 0.4498 - val_loss: 1.5396 - val_accuracy: 0.4590\n",
            "Epoch 37/1000\n",
            "107/114 [===========================>..] - ETA: 0s - loss: 1.5403 - accuracy: 0.4504\n",
            "Epoch 00037: val_loss improved from 1.53961 to 1.53921, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5403 - accuracy: 0.4499 - val_loss: 1.5392 - val_accuracy: 0.4584\n",
            "Epoch 38/1000\n",
            "110/114 [===========================>..] - ETA: 0s - loss: 1.5399 - accuracy: 0.4497\n",
            "Epoch 00038: val_loss improved from 1.53921 to 1.53882, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5399 - accuracy: 0.4495 - val_loss: 1.5388 - val_accuracy: 0.4601\n",
            "Epoch 39/1000\n",
            "110/114 [===========================>..] - ETA: 0s - loss: 1.5394 - accuracy: 0.4516\n",
            "Epoch 00039: val_loss improved from 1.53882 to 1.53843, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5395 - accuracy: 0.4521 - val_loss: 1.5384 - val_accuracy: 0.4598\n",
            "Epoch 40/1000\n",
            "110/114 [===========================>..] - ETA: 0s - loss: 1.5392 - accuracy: 0.4487\n",
            "Epoch 00040: val_loss improved from 1.53843 to 1.53804, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5391 - accuracy: 0.4492 - val_loss: 1.5380 - val_accuracy: 0.4600\n",
            "Epoch 41/1000\n",
            "110/114 [===========================>..] - ETA: 0s - loss: 1.5387 - accuracy: 0.4511\n",
            "Epoch 00041: val_loss improved from 1.53804 to 1.53764, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5387 - accuracy: 0.4507 - val_loss: 1.5376 - val_accuracy: 0.4593\n",
            "Epoch 42/1000\n",
            "107/114 [===========================>..] - ETA: 0s - loss: 1.5385 - accuracy: 0.4493\n",
            "Epoch 00042: val_loss improved from 1.53764 to 1.53725, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5383 - accuracy: 0.4511 - val_loss: 1.5373 - val_accuracy: 0.4601\n",
            "Epoch 43/1000\n",
            "110/114 [===========================>..] - ETA: 0s - loss: 1.5380 - accuracy: 0.4499\n",
            "Epoch 00043: val_loss improved from 1.53725 to 1.53686, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5379 - accuracy: 0.4506 - val_loss: 1.5369 - val_accuracy: 0.4601\n",
            "Epoch 44/1000\n",
            "109/114 [===========================>..] - ETA: 0s - loss: 1.5376 - accuracy: 0.4498\n",
            "Epoch 00044: val_loss improved from 1.53686 to 1.53647, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5375 - accuracy: 0.4504 - val_loss: 1.5365 - val_accuracy: 0.4602\n",
            "Epoch 45/1000\n",
            "113/114 [============================>.] - ETA: 0s - loss: 1.5372 - accuracy: 0.4524\n",
            "Epoch 00045: val_loss improved from 1.53647 to 1.53608, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5372 - accuracy: 0.4524 - val_loss: 1.5361 - val_accuracy: 0.4605\n",
            "Epoch 46/1000\n",
            "110/114 [===========================>..] - ETA: 0s - loss: 1.5368 - accuracy: 0.4508\n",
            "Epoch 00046: val_loss improved from 1.53608 to 1.53569, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5368 - accuracy: 0.4508 - val_loss: 1.5357 - val_accuracy: 0.4583\n",
            "Epoch 47/1000\n",
            "112/114 [============================>.] - ETA: 0s - loss: 1.5364 - accuracy: 0.4532\n",
            "Epoch 00047: val_loss improved from 1.53569 to 1.53529, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5364 - accuracy: 0.4529 - val_loss: 1.5353 - val_accuracy: 0.4604\n",
            "Epoch 48/1000\n",
            "105/114 [==========================>...] - ETA: 0s - loss: 1.5360 - accuracy: 0.4512\n",
            "Epoch 00048: val_loss improved from 1.53529 to 1.53490, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.5360 - accuracy: 0.4513 - val_loss: 1.5349 - val_accuracy: 0.4589\n",
            "Epoch 49/1000\n",
            "107/114 [===========================>..] - ETA: 0s - loss: 1.5355 - accuracy: 0.4528\n",
            "Epoch 00049: val_loss improved from 1.53490 to 1.53451, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5356 - accuracy: 0.4515 - val_loss: 1.5345 - val_accuracy: 0.4607\n",
            "Epoch 50/1000\n",
            "110/114 [===========================>..] - ETA: 0s - loss: 1.5352 - accuracy: 0.4511\n",
            "Epoch 00050: val_loss improved from 1.53451 to 1.53412, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.5352 - accuracy: 0.4514 - val_loss: 1.5341 - val_accuracy: 0.4613\n",
            "Epoch 51/1000\n",
            "108/114 [===========================>..] - ETA: 0s - loss: 1.5348 - accuracy: 0.4540\n",
            "Epoch 00051: val_loss improved from 1.53412 to 1.53373, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5348 - accuracy: 0.4533 - val_loss: 1.5337 - val_accuracy: 0.4626\n",
            "Epoch 52/1000\n",
            "106/114 [==========================>...] - ETA: 0s - loss: 1.5346 - accuracy: 0.4538\n",
            "Epoch 00052: val_loss improved from 1.53373 to 1.53334, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5344 - accuracy: 0.4536 - val_loss: 1.5333 - val_accuracy: 0.4623\n",
            "Epoch 53/1000\n",
            "114/114 [==============================] - ETA: 0s - loss: 1.5340 - accuracy: 0.4537\n",
            "Epoch 00053: val_loss improved from 1.53334 to 1.53294, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.5340 - accuracy: 0.4537 - val_loss: 1.5329 - val_accuracy: 0.4618\n",
            "Epoch 54/1000\n",
            "111/114 [============================>.] - ETA: 0s - loss: 1.5337 - accuracy: 0.4534\n",
            "Epoch 00054: val_loss improved from 1.53294 to 1.53255, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.5337 - accuracy: 0.4532 - val_loss: 1.5325 - val_accuracy: 0.4620\n",
            "Epoch 55/1000\n",
            "108/114 [===========================>..] - ETA: 0s - loss: 1.5333 - accuracy: 0.4544\n",
            "Epoch 00055: val_loss improved from 1.53255 to 1.53215, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5333 - accuracy: 0.4550 - val_loss: 1.5322 - val_accuracy: 0.4627\n",
            "Epoch 56/1000\n",
            "114/114 [==============================] - ETA: 0s - loss: 1.5329 - accuracy: 0.4537\n",
            "Epoch 00056: val_loss improved from 1.53215 to 1.53176, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5329 - accuracy: 0.4537 - val_loss: 1.5318 - val_accuracy: 0.4622\n",
            "Epoch 57/1000\n",
            "113/114 [============================>.] - ETA: 0s - loss: 1.5325 - accuracy: 0.4533\n",
            "Epoch 00057: val_loss improved from 1.53176 to 1.53136, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5325 - accuracy: 0.4531 - val_loss: 1.5314 - val_accuracy: 0.4618\n",
            "Epoch 58/1000\n",
            "106/114 [==========================>...] - ETA: 0s - loss: 1.5321 - accuracy: 0.4522\n",
            "Epoch 00058: val_loss improved from 1.53136 to 1.53097, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5321 - accuracy: 0.4533 - val_loss: 1.5310 - val_accuracy: 0.4612\n",
            "Epoch 59/1000\n",
            "105/114 [==========================>...] - ETA: 0s - loss: 1.5317 - accuracy: 0.4527\n",
            "Epoch 00059: val_loss improved from 1.53097 to 1.53058, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5317 - accuracy: 0.4534 - val_loss: 1.5306 - val_accuracy: 0.4611\n",
            "Epoch 60/1000\n",
            "106/114 [==========================>...] - ETA: 0s - loss: 1.5312 - accuracy: 0.4546\n",
            "Epoch 00060: val_loss improved from 1.53058 to 1.53019, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5313 - accuracy: 0.4538 - val_loss: 1.5302 - val_accuracy: 0.4618\n",
            "Epoch 61/1000\n",
            "110/114 [===========================>..] - ETA: 0s - loss: 1.5309 - accuracy: 0.4551\n",
            "Epoch 00061: val_loss improved from 1.53019 to 1.52979, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5309 - accuracy: 0.4550 - val_loss: 1.5298 - val_accuracy: 0.4626\n",
            "Epoch 62/1000\n",
            "107/114 [===========================>..] - ETA: 0s - loss: 1.5306 - accuracy: 0.4548\n",
            "Epoch 00062: val_loss improved from 1.52979 to 1.52939, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.5305 - accuracy: 0.4542 - val_loss: 1.5294 - val_accuracy: 0.4616\n",
            "Epoch 63/1000\n",
            "112/114 [============================>.] - ETA: 0s - loss: 1.5301 - accuracy: 0.4544\n",
            "Epoch 00063: val_loss improved from 1.52939 to 1.52900, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5301 - accuracy: 0.4546 - val_loss: 1.5290 - val_accuracy: 0.4620\n",
            "Epoch 64/1000\n",
            "113/114 [============================>.] - ETA: 0s - loss: 1.5298 - accuracy: 0.4540\n",
            "Epoch 00064: val_loss improved from 1.52900 to 1.52860, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5298 - accuracy: 0.4541 - val_loss: 1.5286 - val_accuracy: 0.4625\n",
            "Epoch 65/1000\n",
            "111/114 [============================>.] - ETA: 0s - loss: 1.5293 - accuracy: 0.4554\n",
            "Epoch 00065: val_loss improved from 1.52860 to 1.52821, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5294 - accuracy: 0.4552 - val_loss: 1.5282 - val_accuracy: 0.4627\n",
            "Epoch 66/1000\n",
            "114/114 [==============================] - ETA: 0s - loss: 1.5290 - accuracy: 0.4544\n",
            "Epoch 00066: val_loss improved from 1.52821 to 1.52781, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5290 - accuracy: 0.4544 - val_loss: 1.5278 - val_accuracy: 0.4622\n",
            "Epoch 67/1000\n",
            "108/114 [===========================>..] - ETA: 0s - loss: 1.5287 - accuracy: 0.4547\n",
            "Epoch 00067: val_loss improved from 1.52781 to 1.52742, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5286 - accuracy: 0.4554 - val_loss: 1.5274 - val_accuracy: 0.4623\n",
            "Epoch 68/1000\n",
            "111/114 [============================>.] - ETA: 0s - loss: 1.5281 - accuracy: 0.4553\n",
            "Epoch 00068: val_loss improved from 1.52742 to 1.52703, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.5282 - accuracy: 0.4551 - val_loss: 1.5270 - val_accuracy: 0.4630\n",
            "Epoch 69/1000\n",
            "109/114 [===========================>..] - ETA: 0s - loss: 1.5279 - accuracy: 0.4548\n",
            "Epoch 00069: val_loss improved from 1.52703 to 1.52663, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5278 - accuracy: 0.4551 - val_loss: 1.5266 - val_accuracy: 0.4633\n",
            "Epoch 70/1000\n",
            "114/114 [==============================] - ETA: 0s - loss: 1.5274 - accuracy: 0.4554\n",
            "Epoch 00070: val_loss improved from 1.52663 to 1.52624, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5274 - accuracy: 0.4554 - val_loss: 1.5262 - val_accuracy: 0.4629\n",
            "Epoch 71/1000\n",
            "108/114 [===========================>..] - ETA: 0s - loss: 1.5272 - accuracy: 0.4557\n",
            "Epoch 00071: val_loss improved from 1.52624 to 1.52585, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5270 - accuracy: 0.4556 - val_loss: 1.5258 - val_accuracy: 0.4633\n",
            "Epoch 72/1000\n",
            "106/114 [==========================>...] - ETA: 0s - loss: 1.5268 - accuracy: 0.4562\n",
            "Epoch 00072: val_loss improved from 1.52585 to 1.52545, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5266 - accuracy: 0.4558 - val_loss: 1.5254 - val_accuracy: 0.4633\n",
            "Epoch 73/1000\n",
            "105/114 [==========================>...] - ETA: 0s - loss: 1.5264 - accuracy: 0.4562\n",
            "Epoch 00073: val_loss improved from 1.52545 to 1.52505, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5262 - accuracy: 0.4572 - val_loss: 1.5251 - val_accuracy: 0.4636\n",
            "Epoch 74/1000\n",
            "104/114 [==========================>...] - ETA: 0s - loss: 1.5259 - accuracy: 0.4559\n",
            "Epoch 00074: val_loss improved from 1.52505 to 1.52466, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5258 - accuracy: 0.4566 - val_loss: 1.5247 - val_accuracy: 0.4636\n",
            "Epoch 75/1000\n",
            "106/114 [==========================>...] - ETA: 0s - loss: 1.5256 - accuracy: 0.4562\n",
            "Epoch 00075: val_loss improved from 1.52466 to 1.52426, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5255 - accuracy: 0.4564 - val_loss: 1.5243 - val_accuracy: 0.4633\n",
            "Epoch 76/1000\n",
            "106/114 [==========================>...] - ETA: 0s - loss: 1.5251 - accuracy: 0.4570\n",
            "Epoch 00076: val_loss improved from 1.52426 to 1.52386, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5251 - accuracy: 0.4566 - val_loss: 1.5239 - val_accuracy: 0.4636\n",
            "Epoch 77/1000\n",
            "109/114 [===========================>..] - ETA: 0s - loss: 1.5247 - accuracy: 0.4564\n",
            "Epoch 00077: val_loss improved from 1.52386 to 1.52347, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5247 - accuracy: 0.4561 - val_loss: 1.5235 - val_accuracy: 0.4634\n",
            "Epoch 78/1000\n",
            "111/114 [============================>.] - ETA: 0s - loss: 1.5242 - accuracy: 0.4571\n",
            "Epoch 00078: val_loss improved from 1.52347 to 1.52307, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.5243 - accuracy: 0.4566 - val_loss: 1.5231 - val_accuracy: 0.4634\n",
            "Epoch 79/1000\n",
            "107/114 [===========================>..] - ETA: 0s - loss: 1.5240 - accuracy: 0.4561\n",
            "Epoch 00079: val_loss improved from 1.52307 to 1.52268, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5239 - accuracy: 0.4568 - val_loss: 1.5227 - val_accuracy: 0.4647\n",
            "Epoch 80/1000\n",
            "105/114 [==========================>...] - ETA: 0s - loss: 1.5237 - accuracy: 0.4580\n",
            "Epoch 00080: val_loss improved from 1.52268 to 1.52228, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5235 - accuracy: 0.4565 - val_loss: 1.5223 - val_accuracy: 0.4638\n",
            "Epoch 81/1000\n",
            "106/114 [==========================>...] - ETA: 0s - loss: 1.5232 - accuracy: 0.4570\n",
            "Epoch 00081: val_loss improved from 1.52228 to 1.52189, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5231 - accuracy: 0.4575 - val_loss: 1.5219 - val_accuracy: 0.4652\n",
            "Epoch 82/1000\n",
            "106/114 [==========================>...] - ETA: 0s - loss: 1.5227 - accuracy: 0.4578\n",
            "Epoch 00082: val_loss improved from 1.52189 to 1.52149, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5227 - accuracy: 0.4589 - val_loss: 1.5215 - val_accuracy: 0.4660\n",
            "Epoch 83/1000\n",
            "106/114 [==========================>...] - ETA: 0s - loss: 1.5224 - accuracy: 0.4577\n",
            "Epoch 00083: val_loss improved from 1.52149 to 1.52109, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5223 - accuracy: 0.4578 - val_loss: 1.5211 - val_accuracy: 0.4648\n",
            "Epoch 84/1000\n",
            "106/114 [==========================>...] - ETA: 0s - loss: 1.5219 - accuracy: 0.4580\n",
            "Epoch 00084: val_loss improved from 1.52109 to 1.52069, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5219 - accuracy: 0.4583 - val_loss: 1.5207 - val_accuracy: 0.4645\n",
            "Epoch 85/1000\n",
            "113/114 [============================>.] - ETA: 0s - loss: 1.5215 - accuracy: 0.4586\n",
            "Epoch 00085: val_loss improved from 1.52069 to 1.52029, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5215 - accuracy: 0.4586 - val_loss: 1.5203 - val_accuracy: 0.4649\n",
            "Epoch 86/1000\n",
            "114/114 [==============================] - ETA: 0s - loss: 1.5211 - accuracy: 0.4585\n",
            "Epoch 00086: val_loss improved from 1.52029 to 1.51990, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.5211 - accuracy: 0.4585 - val_loss: 1.5199 - val_accuracy: 0.4644\n",
            "Epoch 87/1000\n",
            "108/114 [===========================>..] - ETA: 0s - loss: 1.5206 - accuracy: 0.4596\n",
            "Epoch 00087: val_loss improved from 1.51990 to 1.51950, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5207 - accuracy: 0.4588 - val_loss: 1.5195 - val_accuracy: 0.4654\n",
            "Epoch 88/1000\n",
            "113/114 [============================>.] - ETA: 0s - loss: 1.5204 - accuracy: 0.4594\n",
            "Epoch 00088: val_loss improved from 1.51950 to 1.51910, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 8ms/step - loss: 1.5203 - accuracy: 0.4593 - val_loss: 1.5191 - val_accuracy: 0.4658\n",
            "Epoch 89/1000\n",
            "107/114 [===========================>..] - ETA: 0s - loss: 1.5199 - accuracy: 0.4590\n",
            "Epoch 00089: val_loss improved from 1.51910 to 1.51871, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.5199 - accuracy: 0.4593 - val_loss: 1.5187 - val_accuracy: 0.4654\n",
            "Epoch 90/1000\n",
            "113/114 [============================>.] - ETA: 0s - loss: 1.5196 - accuracy: 0.4599\n",
            "Epoch 00090: val_loss improved from 1.51871 to 1.51831, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5196 - accuracy: 0.4599 - val_loss: 1.5183 - val_accuracy: 0.4638\n",
            "Epoch 91/1000\n",
            "106/114 [==========================>...] - ETA: 0s - loss: 1.5189 - accuracy: 0.4609\n",
            "Epoch 00091: val_loss improved from 1.51831 to 1.51791, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5192 - accuracy: 0.4597 - val_loss: 1.5179 - val_accuracy: 0.4644\n",
            "Epoch 92/1000\n",
            "106/114 [==========================>...] - ETA: 0s - loss: 1.5187 - accuracy: 0.4594\n",
            "Epoch 00092: val_loss improved from 1.51791 to 1.51751, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5188 - accuracy: 0.4600 - val_loss: 1.5175 - val_accuracy: 0.4654\n",
            "Epoch 93/1000\n",
            "113/114 [============================>.] - ETA: 0s - loss: 1.5184 - accuracy: 0.4602\n",
            "Epoch 00093: val_loss improved from 1.51751 to 1.51711, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5184 - accuracy: 0.4603 - val_loss: 1.5171 - val_accuracy: 0.4663\n",
            "Epoch 94/1000\n",
            "105/114 [==========================>...] - ETA: 0s - loss: 1.5181 - accuracy: 0.4606\n",
            "Epoch 00094: val_loss improved from 1.51711 to 1.51672, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5180 - accuracy: 0.4609 - val_loss: 1.5167 - val_accuracy: 0.4663\n",
            "Epoch 95/1000\n",
            "108/114 [===========================>..] - ETA: 0s - loss: 1.5175 - accuracy: 0.4616\n",
            "Epoch 00095: val_loss improved from 1.51672 to 1.51632, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.5176 - accuracy: 0.4607 - val_loss: 1.5163 - val_accuracy: 0.4667\n",
            "Epoch 96/1000\n",
            "105/114 [==========================>...] - ETA: 0s - loss: 1.5172 - accuracy: 0.4612\n",
            "Epoch 00096: val_loss improved from 1.51632 to 1.51592, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5172 - accuracy: 0.4616 - val_loss: 1.5159 - val_accuracy: 0.4670\n",
            "Epoch 97/1000\n",
            "114/114 [==============================] - ETA: 0s - loss: 1.5168 - accuracy: 0.4614\n",
            "Epoch 00097: val_loss improved from 1.51592 to 1.51552, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5168 - accuracy: 0.4614 - val_loss: 1.5155 - val_accuracy: 0.4671\n",
            "Epoch 98/1000\n",
            "105/114 [==========================>...] - ETA: 0s - loss: 1.5166 - accuracy: 0.4610\n",
            "Epoch 00098: val_loss improved from 1.51552 to 1.51512, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5164 - accuracy: 0.4617 - val_loss: 1.5151 - val_accuracy: 0.4676\n",
            "Epoch 99/1000\n",
            "108/114 [===========================>..] - ETA: 0s - loss: 1.5161 - accuracy: 0.4623\n",
            "Epoch 00099: val_loss improved from 1.51512 to 1.51472, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5160 - accuracy: 0.4617 - val_loss: 1.5147 - val_accuracy: 0.4671\n",
            "Epoch 100/1000\n",
            "111/114 [============================>.] - ETA: 0s - loss: 1.5157 - accuracy: 0.4618\n",
            "Epoch 00100: val_loss improved from 1.51472 to 1.51432, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5156 - accuracy: 0.4626 - val_loss: 1.5143 - val_accuracy: 0.4671\n",
            "Epoch 101/1000\n",
            "105/114 [==========================>...] - ETA: 0s - loss: 1.5153 - accuracy: 0.4629\n",
            "Epoch 00101: val_loss improved from 1.51432 to 1.51392, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5152 - accuracy: 0.4623 - val_loss: 1.5139 - val_accuracy: 0.4674\n",
            "Epoch 102/1000\n",
            "106/114 [==========================>...] - ETA: 0s - loss: 1.5148 - accuracy: 0.4620\n",
            "Epoch 00102: val_loss improved from 1.51392 to 1.51352, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5148 - accuracy: 0.4611 - val_loss: 1.5135 - val_accuracy: 0.4670\n",
            "Epoch 103/1000\n",
            "113/114 [============================>.] - ETA: 0s - loss: 1.5144 - accuracy: 0.4630\n",
            "Epoch 00103: val_loss improved from 1.51352 to 1.51313, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5144 - accuracy: 0.4629 - val_loss: 1.5131 - val_accuracy: 0.4684\n",
            "Epoch 104/1000\n",
            "112/114 [============================>.] - ETA: 0s - loss: 1.5140 - accuracy: 0.4628\n",
            "Epoch 00104: val_loss improved from 1.51313 to 1.51273, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5140 - accuracy: 0.4626 - val_loss: 1.5127 - val_accuracy: 0.4667\n",
            "Epoch 105/1000\n",
            "106/114 [==========================>...] - ETA: 0s - loss: 1.5136 - accuracy: 0.4637\n",
            "Epoch 00105: val_loss improved from 1.51273 to 1.51233, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.5136 - accuracy: 0.4628 - val_loss: 1.5123 - val_accuracy: 0.4680\n",
            "Epoch 106/1000\n",
            "108/114 [===========================>..] - ETA: 0s - loss: 1.5131 - accuracy: 0.4635\n",
            "Epoch 00106: val_loss improved from 1.51233 to 1.51193, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.5132 - accuracy: 0.4628 - val_loss: 1.5119 - val_accuracy: 0.4681\n",
            "Epoch 107/1000\n",
            "107/114 [===========================>..] - ETA: 0s - loss: 1.5129 - accuracy: 0.4632\n",
            "Epoch 00107: val_loss improved from 1.51193 to 1.51153, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5128 - accuracy: 0.4630 - val_loss: 1.5115 - val_accuracy: 0.4682\n",
            "Epoch 108/1000\n",
            "105/114 [==========================>...] - ETA: 0s - loss: 1.5123 - accuracy: 0.4627\n",
            "Epoch 00108: val_loss improved from 1.51153 to 1.51113, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.5124 - accuracy: 0.4622 - val_loss: 1.5111 - val_accuracy: 0.4682\n",
            "Epoch 109/1000\n",
            "108/114 [===========================>..] - ETA: 0s - loss: 1.5119 - accuracy: 0.4631\n",
            "Epoch 00109: val_loss improved from 1.51113 to 1.51072, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5120 - accuracy: 0.4630 - val_loss: 1.5107 - val_accuracy: 0.4685\n",
            "Epoch 110/1000\n",
            "104/114 [==========================>...] - ETA: 0s - loss: 1.5120 - accuracy: 0.4612\n",
            "Epoch 00110: val_loss improved from 1.51072 to 1.51033, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5116 - accuracy: 0.4632 - val_loss: 1.5103 - val_accuracy: 0.4702\n",
            "Epoch 111/1000\n",
            "107/114 [===========================>..] - ETA: 0s - loss: 1.5111 - accuracy: 0.4647\n",
            "Epoch 00111: val_loss improved from 1.51033 to 1.50992, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5112 - accuracy: 0.4639 - val_loss: 1.5099 - val_accuracy: 0.4684\n",
            "Epoch 112/1000\n",
            "107/114 [===========================>..] - ETA: 0s - loss: 1.5108 - accuracy: 0.4648\n",
            "Epoch 00112: val_loss improved from 1.50992 to 1.50952, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5108 - accuracy: 0.4641 - val_loss: 1.5095 - val_accuracy: 0.4706\n",
            "Epoch 113/1000\n",
            "105/114 [==========================>...] - ETA: 0s - loss: 1.5105 - accuracy: 0.4646\n",
            "Epoch 00113: val_loss improved from 1.50952 to 1.50912, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5104 - accuracy: 0.4641 - val_loss: 1.5091 - val_accuracy: 0.4692\n",
            "Epoch 114/1000\n",
            "114/114 [==============================] - ETA: 0s - loss: 1.5100 - accuracy: 0.4644\n",
            "Epoch 00114: val_loss improved from 1.50912 to 1.50871, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5100 - accuracy: 0.4644 - val_loss: 1.5087 - val_accuracy: 0.4709\n",
            "Epoch 115/1000\n",
            "112/114 [============================>.] - ETA: 0s - loss: 1.5096 - accuracy: 0.4650\n",
            "Epoch 00115: val_loss improved from 1.50871 to 1.50831, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5097 - accuracy: 0.4649 - val_loss: 1.5083 - val_accuracy: 0.4710\n",
            "Epoch 116/1000\n",
            "109/114 [===========================>..] - ETA: 0s - loss: 1.5090 - accuracy: 0.4665\n",
            "Epoch 00116: val_loss improved from 1.50831 to 1.50791, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.5093 - accuracy: 0.4648 - val_loss: 1.5079 - val_accuracy: 0.4711\n",
            "Epoch 117/1000\n",
            "114/114 [==============================] - ETA: 0s - loss: 1.5089 - accuracy: 0.4648\n",
            "Epoch 00117: val_loss improved from 1.50791 to 1.50751, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5089 - accuracy: 0.4648 - val_loss: 1.5075 - val_accuracy: 0.4695\n",
            "Epoch 118/1000\n",
            "106/114 [==========================>...] - ETA: 0s - loss: 1.5084 - accuracy: 0.4640\n",
            "Epoch 00118: val_loss improved from 1.50751 to 1.50711, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5085 - accuracy: 0.4640 - val_loss: 1.5071 - val_accuracy: 0.4692\n",
            "Epoch 119/1000\n",
            "106/114 [==========================>...] - ETA: 0s - loss: 1.5079 - accuracy: 0.4629\n",
            "Epoch 00119: val_loss improved from 1.50711 to 1.50671, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5081 - accuracy: 0.4639 - val_loss: 1.5067 - val_accuracy: 0.4685\n",
            "Epoch 120/1000\n",
            "112/114 [============================>.] - ETA: 0s - loss: 1.5076 - accuracy: 0.4646\n",
            "Epoch 00120: val_loss improved from 1.50671 to 1.50630, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5077 - accuracy: 0.4644 - val_loss: 1.5063 - val_accuracy: 0.4688\n",
            "Epoch 121/1000\n",
            "111/114 [============================>.] - ETA: 0s - loss: 1.5073 - accuracy: 0.4655\n",
            "Epoch 00121: val_loss improved from 1.50630 to 1.50590, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5073 - accuracy: 0.4651 - val_loss: 1.5059 - val_accuracy: 0.4699\n",
            "Epoch 122/1000\n",
            "107/114 [===========================>..] - ETA: 0s - loss: 1.5069 - accuracy: 0.4634\n",
            "Epoch 00122: val_loss improved from 1.50590 to 1.50550, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.5069 - accuracy: 0.4641 - val_loss: 1.5055 - val_accuracy: 0.4691\n",
            "Epoch 123/1000\n",
            "108/114 [===========================>..] - ETA: 0s - loss: 1.5064 - accuracy: 0.4633\n",
            "Epoch 00123: val_loss improved from 1.50550 to 1.50509, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5065 - accuracy: 0.4633 - val_loss: 1.5051 - val_accuracy: 0.4678\n",
            "Epoch 124/1000\n",
            "111/114 [============================>.] - ETA: 0s - loss: 1.5061 - accuracy: 0.4640\n",
            "Epoch 00124: val_loss improved from 1.50509 to 1.50469, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5061 - accuracy: 0.4639 - val_loss: 1.5047 - val_accuracy: 0.4692\n",
            "Epoch 125/1000\n",
            "113/114 [============================>.] - ETA: 0s - loss: 1.5056 - accuracy: 0.4642\n",
            "Epoch 00125: val_loss improved from 1.50469 to 1.50428, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5057 - accuracy: 0.4640 - val_loss: 1.5043 - val_accuracy: 0.4691\n",
            "Epoch 126/1000\n",
            "104/114 [==========================>...] - ETA: 0s - loss: 1.5053 - accuracy: 0.4645\n",
            "Epoch 00126: val_loss improved from 1.50428 to 1.50388, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5053 - accuracy: 0.4646 - val_loss: 1.5039 - val_accuracy: 0.4703\n",
            "Epoch 127/1000\n",
            "111/114 [============================>.] - ETA: 0s - loss: 1.5049 - accuracy: 0.4636\n",
            "Epoch 00127: val_loss improved from 1.50388 to 1.50348, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5049 - accuracy: 0.4639 - val_loss: 1.5035 - val_accuracy: 0.4709\n",
            "Epoch 128/1000\n",
            "105/114 [==========================>...] - ETA: 0s - loss: 1.5045 - accuracy: 0.4640\n",
            "Epoch 00128: val_loss improved from 1.50348 to 1.50307, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.5045 - accuracy: 0.4650 - val_loss: 1.5031 - val_accuracy: 0.4713\n",
            "Epoch 129/1000\n",
            "106/114 [==========================>...] - ETA: 0s - loss: 1.5038 - accuracy: 0.4646\n",
            "Epoch 00129: val_loss improved from 1.50307 to 1.50267, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5041 - accuracy: 0.4651 - val_loss: 1.5027 - val_accuracy: 0.4710\n",
            "Epoch 130/1000\n",
            "113/114 [============================>.] - ETA: 0s - loss: 1.5037 - accuracy: 0.4644\n",
            "Epoch 00130: val_loss improved from 1.50267 to 1.50226, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5037 - accuracy: 0.4646 - val_loss: 1.5023 - val_accuracy: 0.4717\n",
            "Epoch 131/1000\n",
            "112/114 [============================>.] - ETA: 0s - loss: 1.5033 - accuracy: 0.4652\n",
            "Epoch 00131: val_loss improved from 1.50226 to 1.50185, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5033 - accuracy: 0.4657 - val_loss: 1.5019 - val_accuracy: 0.4714\n",
            "Epoch 132/1000\n",
            "114/114 [==============================] - ETA: 0s - loss: 1.5029 - accuracy: 0.4643\n",
            "Epoch 00132: val_loss improved from 1.50185 to 1.50145, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.5029 - accuracy: 0.4643 - val_loss: 1.5015 - val_accuracy: 0.4723\n",
            "Epoch 133/1000\n",
            "109/114 [===========================>..] - ETA: 0s - loss: 1.5024 - accuracy: 0.4658\n",
            "Epoch 00133: val_loss improved from 1.50145 to 1.50105, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5025 - accuracy: 0.4657 - val_loss: 1.5010 - val_accuracy: 0.4716\n",
            "Epoch 134/1000\n",
            "114/114 [==============================] - ETA: 0s - loss: 1.5021 - accuracy: 0.4646\n",
            "Epoch 00134: val_loss improved from 1.50105 to 1.50064, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5021 - accuracy: 0.4646 - val_loss: 1.5006 - val_accuracy: 0.4717\n",
            "Epoch 135/1000\n",
            "106/114 [==========================>...] - ETA: 0s - loss: 1.5017 - accuracy: 0.4648\n",
            "Epoch 00135: val_loss improved from 1.50064 to 1.50024, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5016 - accuracy: 0.4653 - val_loss: 1.5002 - val_accuracy: 0.4724\n",
            "Epoch 136/1000\n",
            "105/114 [==========================>...] - ETA: 0s - loss: 1.5013 - accuracy: 0.4660\n",
            "Epoch 00136: val_loss improved from 1.50024 to 1.49983, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5012 - accuracy: 0.4654 - val_loss: 1.4998 - val_accuracy: 0.4720\n",
            "Epoch 137/1000\n",
            "106/114 [==========================>...] - ETA: 0s - loss: 1.5009 - accuracy: 0.4654\n",
            "Epoch 00137: val_loss improved from 1.49983 to 1.49942, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5008 - accuracy: 0.4657 - val_loss: 1.4994 - val_accuracy: 0.4725\n",
            "Epoch 138/1000\n",
            "107/114 [===========================>..] - ETA: 0s - loss: 1.5000 - accuracy: 0.4652\n",
            "Epoch 00138: val_loss improved from 1.49942 to 1.49902, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5004 - accuracy: 0.4651 - val_loss: 1.4990 - val_accuracy: 0.4725\n",
            "Epoch 139/1000\n",
            "107/114 [===========================>..] - ETA: 0s - loss: 1.4995 - accuracy: 0.4681\n",
            "Epoch 00139: val_loss improved from 1.49902 to 1.49861, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.5000 - accuracy: 0.4667 - val_loss: 1.4986 - val_accuracy: 0.4739\n",
            "Epoch 140/1000\n",
            "111/114 [============================>.] - ETA: 0s - loss: 1.4996 - accuracy: 0.4667\n",
            "Epoch 00140: val_loss improved from 1.49861 to 1.49820, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4996 - accuracy: 0.4668 - val_loss: 1.4982 - val_accuracy: 0.4732\n",
            "Epoch 141/1000\n",
            "114/114 [==============================] - ETA: 0s - loss: 1.4992 - accuracy: 0.4661\n",
            "Epoch 00141: val_loss improved from 1.49820 to 1.49780, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4992 - accuracy: 0.4661 - val_loss: 1.4978 - val_accuracy: 0.4736\n",
            "Epoch 142/1000\n",
            "113/114 [============================>.] - ETA: 0s - loss: 1.4988 - accuracy: 0.4658\n",
            "Epoch 00142: val_loss improved from 1.49780 to 1.49739, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4988 - accuracy: 0.4658 - val_loss: 1.4974 - val_accuracy: 0.4738\n",
            "Epoch 143/1000\n",
            "104/114 [==========================>...] - ETA: 0s - loss: 1.4984 - accuracy: 0.4653\n",
            "Epoch 00143: val_loss improved from 1.49739 to 1.49698, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4984 - accuracy: 0.4657 - val_loss: 1.4970 - val_accuracy: 0.4739\n",
            "Epoch 144/1000\n",
            "107/114 [===========================>..] - ETA: 0s - loss: 1.4980 - accuracy: 0.4674\n",
            "Epoch 00144: val_loss improved from 1.49698 to 1.49657, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4980 - accuracy: 0.4663 - val_loss: 1.4966 - val_accuracy: 0.4746\n",
            "Epoch 145/1000\n",
            "114/114 [==============================] - ETA: 0s - loss: 1.4976 - accuracy: 0.4665\n",
            "Epoch 00145: val_loss improved from 1.49657 to 1.49616, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4976 - accuracy: 0.4665 - val_loss: 1.4962 - val_accuracy: 0.4742\n",
            "Epoch 146/1000\n",
            "113/114 [============================>.] - ETA: 0s - loss: 1.4972 - accuracy: 0.4664\n",
            "Epoch 00146: val_loss improved from 1.49616 to 1.49575, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4972 - accuracy: 0.4666 - val_loss: 1.4958 - val_accuracy: 0.4745\n",
            "Epoch 147/1000\n",
            "113/114 [============================>.] - ETA: 0s - loss: 1.4968 - accuracy: 0.4667\n",
            "Epoch 00147: val_loss improved from 1.49575 to 1.49535, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4968 - accuracy: 0.4666 - val_loss: 1.4953 - val_accuracy: 0.4739\n",
            "Epoch 148/1000\n",
            "112/114 [============================>.] - ETA: 0s - loss: 1.4964 - accuracy: 0.4672\n",
            "Epoch 00148: val_loss improved from 1.49535 to 1.49494, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4964 - accuracy: 0.4671 - val_loss: 1.4949 - val_accuracy: 0.4746\n",
            "Epoch 149/1000\n",
            "114/114 [==============================] - ETA: 0s - loss: 1.4960 - accuracy: 0.4666\n",
            "Epoch 00149: val_loss improved from 1.49494 to 1.49453, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4960 - accuracy: 0.4666 - val_loss: 1.4945 - val_accuracy: 0.4747\n",
            "Epoch 150/1000\n",
            "104/114 [==========================>...] - ETA: 0s - loss: 1.4952 - accuracy: 0.4672\n",
            "Epoch 00150: val_loss improved from 1.49453 to 1.49411, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4956 - accuracy: 0.4663 - val_loss: 1.4941 - val_accuracy: 0.4739\n",
            "Epoch 151/1000\n",
            "105/114 [==========================>...] - ETA: 0s - loss: 1.4951 - accuracy: 0.4672\n",
            "Epoch 00151: val_loss improved from 1.49411 to 1.49371, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4952 - accuracy: 0.4668 - val_loss: 1.4937 - val_accuracy: 0.4740\n",
            "Epoch 152/1000\n",
            "110/114 [===========================>..] - ETA: 0s - loss: 1.4949 - accuracy: 0.4667\n",
            "Epoch 00152: val_loss improved from 1.49371 to 1.49330, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4948 - accuracy: 0.4663 - val_loss: 1.4933 - val_accuracy: 0.4738\n",
            "Epoch 153/1000\n",
            "111/114 [============================>.] - ETA: 0s - loss: 1.4944 - accuracy: 0.4658\n",
            "Epoch 00153: val_loss improved from 1.49330 to 1.49289, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4944 - accuracy: 0.4658 - val_loss: 1.4929 - val_accuracy: 0.4742\n",
            "Epoch 154/1000\n",
            "114/114 [==============================] - ETA: 0s - loss: 1.4940 - accuracy: 0.4664\n",
            "Epoch 00154: val_loss improved from 1.49289 to 1.49248, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4940 - accuracy: 0.4664 - val_loss: 1.4925 - val_accuracy: 0.4746\n",
            "Epoch 155/1000\n",
            "114/114 [==============================] - ETA: 0s - loss: 1.4936 - accuracy: 0.4670\n",
            "Epoch 00155: val_loss improved from 1.49248 to 1.49207, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4936 - accuracy: 0.4670 - val_loss: 1.4921 - val_accuracy: 0.4753\n",
            "Epoch 156/1000\n",
            "108/114 [===========================>..] - ETA: 0s - loss: 1.4932 - accuracy: 0.4654\n",
            "Epoch 00156: val_loss improved from 1.49207 to 1.49166, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4932 - accuracy: 0.4661 - val_loss: 1.4917 - val_accuracy: 0.4749\n",
            "Epoch 157/1000\n",
            "114/114 [==============================] - ETA: 0s - loss: 1.4927 - accuracy: 0.4667\n",
            "Epoch 00157: val_loss improved from 1.49166 to 1.49125, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4927 - accuracy: 0.4667 - val_loss: 1.4912 - val_accuracy: 0.4754\n",
            "Epoch 158/1000\n",
            "112/114 [============================>.] - ETA: 0s - loss: 1.4923 - accuracy: 0.4668\n",
            "Epoch 00158: val_loss improved from 1.49125 to 1.49084, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4923 - accuracy: 0.4667 - val_loss: 1.4908 - val_accuracy: 0.4742\n",
            "Epoch 159/1000\n",
            "105/114 [==========================>...] - ETA: 0s - loss: 1.4920 - accuracy: 0.4664\n",
            "Epoch 00159: val_loss improved from 1.49084 to 1.49043, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4919 - accuracy: 0.4668 - val_loss: 1.4904 - val_accuracy: 0.4743\n",
            "Epoch 160/1000\n",
            "105/114 [==========================>...] - ETA: 0s - loss: 1.4916 - accuracy: 0.4670\n",
            "Epoch 00160: val_loss improved from 1.49043 to 1.49001, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4915 - accuracy: 0.4662 - val_loss: 1.4900 - val_accuracy: 0.4746\n",
            "Epoch 161/1000\n",
            "112/114 [============================>.] - ETA: 0s - loss: 1.4912 - accuracy: 0.4669\n",
            "Epoch 00161: val_loss improved from 1.49001 to 1.48960, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4911 - accuracy: 0.4666 - val_loss: 1.4896 - val_accuracy: 0.4749\n",
            "Epoch 162/1000\n",
            "112/114 [============================>.] - ETA: 0s - loss: 1.4906 - accuracy: 0.4665\n",
            "Epoch 00162: val_loss improved from 1.48960 to 1.48919, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4907 - accuracy: 0.4665 - val_loss: 1.4892 - val_accuracy: 0.4734\n",
            "Epoch 163/1000\n",
            "112/114 [============================>.] - ETA: 0s - loss: 1.4902 - accuracy: 0.4670\n",
            "Epoch 00163: val_loss improved from 1.48919 to 1.48878, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4903 - accuracy: 0.4666 - val_loss: 1.4888 - val_accuracy: 0.4734\n",
            "Epoch 164/1000\n",
            "105/114 [==========================>...] - ETA: 0s - loss: 1.4900 - accuracy: 0.4670\n",
            "Epoch 00164: val_loss improved from 1.48878 to 1.48837, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4899 - accuracy: 0.4669 - val_loss: 1.4884 - val_accuracy: 0.4738\n",
            "Epoch 165/1000\n",
            "113/114 [============================>.] - ETA: 0s - loss: 1.4895 - accuracy: 0.4666\n",
            "Epoch 00165: val_loss improved from 1.48837 to 1.48796, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4895 - accuracy: 0.4666 - val_loss: 1.4880 - val_accuracy: 0.4746\n",
            "Epoch 166/1000\n",
            "113/114 [============================>.] - ETA: 0s - loss: 1.4891 - accuracy: 0.4668\n",
            "Epoch 00166: val_loss improved from 1.48796 to 1.48754, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4891 - accuracy: 0.4667 - val_loss: 1.4875 - val_accuracy: 0.4752\n",
            "Epoch 167/1000\n",
            "107/114 [===========================>..] - ETA: 0s - loss: 1.4887 - accuracy: 0.4677\n",
            "Epoch 00167: val_loss improved from 1.48754 to 1.48713, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4887 - accuracy: 0.4675 - val_loss: 1.4871 - val_accuracy: 0.4750\n",
            "Epoch 168/1000\n",
            "108/114 [===========================>..] - ETA: 0s - loss: 1.4880 - accuracy: 0.4676\n",
            "Epoch 00168: val_loss improved from 1.48713 to 1.48672, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 8ms/step - loss: 1.4883 - accuracy: 0.4673 - val_loss: 1.4867 - val_accuracy: 0.4745\n",
            "Epoch 169/1000\n",
            "109/114 [===========================>..] - ETA: 0s - loss: 1.4878 - accuracy: 0.4666\n",
            "Epoch 00169: val_loss improved from 1.48672 to 1.48630, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4878 - accuracy: 0.4666 - val_loss: 1.4863 - val_accuracy: 0.4753\n",
            "Epoch 170/1000\n",
            "114/114 [==============================] - ETA: 0s - loss: 1.4874 - accuracy: 0.4669\n",
            "Epoch 00170: val_loss improved from 1.48630 to 1.48589, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4874 - accuracy: 0.4669 - val_loss: 1.4859 - val_accuracy: 0.4752\n",
            "Epoch 171/1000\n",
            "111/114 [============================>.] - ETA: 0s - loss: 1.4869 - accuracy: 0.4673\n",
            "Epoch 00171: val_loss improved from 1.48589 to 1.48548, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4870 - accuracy: 0.4673 - val_loss: 1.4855 - val_accuracy: 0.4756\n",
            "Epoch 172/1000\n",
            "106/114 [==========================>...] - ETA: 0s - loss: 1.4865 - accuracy: 0.4672\n",
            "Epoch 00172: val_loss improved from 1.48548 to 1.48506, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4866 - accuracy: 0.4676 - val_loss: 1.4851 - val_accuracy: 0.4750\n",
            "Epoch 173/1000\n",
            "112/114 [============================>.] - ETA: 0s - loss: 1.4862 - accuracy: 0.4678\n",
            "Epoch 00173: val_loss improved from 1.48506 to 1.48465, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4862 - accuracy: 0.4677 - val_loss: 1.4846 - val_accuracy: 0.4750\n",
            "Epoch 174/1000\n",
            "108/114 [===========================>..] - ETA: 0s - loss: 1.4858 - accuracy: 0.4679\n",
            "Epoch 00174: val_loss improved from 1.48465 to 1.48423, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4858 - accuracy: 0.4675 - val_loss: 1.4842 - val_accuracy: 0.4754\n",
            "Epoch 175/1000\n",
            "114/114 [==============================] - ETA: 0s - loss: 1.4854 - accuracy: 0.4673\n",
            "Epoch 00175: val_loss improved from 1.48423 to 1.48382, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4854 - accuracy: 0.4673 - val_loss: 1.4838 - val_accuracy: 0.4756\n",
            "Epoch 176/1000\n",
            "111/114 [============================>.] - ETA: 0s - loss: 1.4850 - accuracy: 0.4667\n",
            "Epoch 00176: val_loss improved from 1.48382 to 1.48341, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4850 - accuracy: 0.4672 - val_loss: 1.4834 - val_accuracy: 0.4750\n",
            "Epoch 177/1000\n",
            "114/114 [==============================] - ETA: 0s - loss: 1.4846 - accuracy: 0.4679\n",
            "Epoch 00177: val_loss improved from 1.48341 to 1.48299, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4846 - accuracy: 0.4679 - val_loss: 1.4830 - val_accuracy: 0.4757\n",
            "Epoch 178/1000\n",
            "105/114 [==========================>...] - ETA: 0s - loss: 1.4842 - accuracy: 0.4672\n",
            "Epoch 00178: val_loss improved from 1.48299 to 1.48257, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4842 - accuracy: 0.4669 - val_loss: 1.4826 - val_accuracy: 0.4758\n",
            "Epoch 179/1000\n",
            "110/114 [===========================>..] - ETA: 0s - loss: 1.4838 - accuracy: 0.4669\n",
            "Epoch 00179: val_loss improved from 1.48257 to 1.48216, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4837 - accuracy: 0.4671 - val_loss: 1.4822 - val_accuracy: 0.4760\n",
            "Epoch 180/1000\n",
            "107/114 [===========================>..] - ETA: 0s - loss: 1.4831 - accuracy: 0.4692\n",
            "Epoch 00180: val_loss improved from 1.48216 to 1.48174, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4833 - accuracy: 0.4686 - val_loss: 1.4817 - val_accuracy: 0.4756\n",
            "Epoch 181/1000\n",
            "109/114 [===========================>..] - ETA: 0s - loss: 1.4828 - accuracy: 0.4690\n",
            "Epoch 00181: val_loss improved from 1.48174 to 1.48133, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4829 - accuracy: 0.4686 - val_loss: 1.4813 - val_accuracy: 0.4761\n",
            "Epoch 182/1000\n",
            "110/114 [===========================>..] - ETA: 0s - loss: 1.4824 - accuracy: 0.4681\n",
            "Epoch 00182: val_loss improved from 1.48133 to 1.48091, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 8ms/step - loss: 1.4825 - accuracy: 0.4676 - val_loss: 1.4809 - val_accuracy: 0.4756\n",
            "Epoch 183/1000\n",
            "109/114 [===========================>..] - ETA: 0s - loss: 1.4822 - accuracy: 0.4678\n",
            "Epoch 00183: val_loss improved from 1.48091 to 1.48049, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4821 - accuracy: 0.4682 - val_loss: 1.4805 - val_accuracy: 0.4756\n",
            "Epoch 184/1000\n",
            "107/114 [===========================>..] - ETA: 0s - loss: 1.4819 - accuracy: 0.4678\n",
            "Epoch 00184: val_loss improved from 1.48049 to 1.48008, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4817 - accuracy: 0.4690 - val_loss: 1.4801 - val_accuracy: 0.4771\n",
            "Epoch 185/1000\n",
            "108/114 [===========================>..] - ETA: 0s - loss: 1.4812 - accuracy: 0.4672\n",
            "Epoch 00185: val_loss improved from 1.48008 to 1.47966, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4813 - accuracy: 0.4678 - val_loss: 1.4797 - val_accuracy: 0.4758\n",
            "Epoch 186/1000\n",
            "114/114 [==============================] - ETA: 0s - loss: 1.4809 - accuracy: 0.4683\n",
            "Epoch 00186: val_loss improved from 1.47966 to 1.47924, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 8ms/step - loss: 1.4809 - accuracy: 0.4683 - val_loss: 1.4792 - val_accuracy: 0.4763\n",
            "Epoch 187/1000\n",
            "108/114 [===========================>..] - ETA: 0s - loss: 1.4806 - accuracy: 0.4683\n",
            "Epoch 00187: val_loss improved from 1.47924 to 1.47883, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 8ms/step - loss: 1.4804 - accuracy: 0.4688 - val_loss: 1.4788 - val_accuracy: 0.4771\n",
            "Epoch 188/1000\n",
            "105/114 [==========================>...] - ETA: 0s - loss: 1.4803 - accuracy: 0.4678\n",
            "Epoch 00188: val_loss improved from 1.47883 to 1.47841, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4800 - accuracy: 0.4682 - val_loss: 1.4784 - val_accuracy: 0.4763\n",
            "Epoch 189/1000\n",
            "108/114 [===========================>..] - ETA: 0s - loss: 1.4797 - accuracy: 0.4698\n",
            "Epoch 00189: val_loss improved from 1.47841 to 1.47799, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4796 - accuracy: 0.4693 - val_loss: 1.4780 - val_accuracy: 0.4776\n",
            "Epoch 190/1000\n",
            "112/114 [============================>.] - ETA: 0s - loss: 1.4792 - accuracy: 0.4690\n",
            "Epoch 00190: val_loss improved from 1.47799 to 1.47757, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4792 - accuracy: 0.4693 - val_loss: 1.4776 - val_accuracy: 0.4775\n",
            "Epoch 191/1000\n",
            "108/114 [===========================>..] - ETA: 0s - loss: 1.4789 - accuracy: 0.4691\n",
            "Epoch 00191: val_loss improved from 1.47757 to 1.47714, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4788 - accuracy: 0.4698 - val_loss: 1.4771 - val_accuracy: 0.4778\n",
            "Epoch 192/1000\n",
            "111/114 [============================>.] - ETA: 0s - loss: 1.4783 - accuracy: 0.4696\n",
            "Epoch 00192: val_loss improved from 1.47714 to 1.47672, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 8ms/step - loss: 1.4784 - accuracy: 0.4697 - val_loss: 1.4767 - val_accuracy: 0.4778\n",
            "Epoch 193/1000\n",
            "112/114 [============================>.] - ETA: 0s - loss: 1.4781 - accuracy: 0.4703\n",
            "Epoch 00193: val_loss improved from 1.47672 to 1.47630, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4780 - accuracy: 0.4706 - val_loss: 1.4763 - val_accuracy: 0.4774\n",
            "Epoch 194/1000\n",
            "110/114 [===========================>..] - ETA: 0s - loss: 1.4775 - accuracy: 0.4697\n",
            "Epoch 00194: val_loss improved from 1.47630 to 1.47588, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4775 - accuracy: 0.4702 - val_loss: 1.4759 - val_accuracy: 0.4776\n",
            "Epoch 195/1000\n",
            "109/114 [===========================>..] - ETA: 0s - loss: 1.4771 - accuracy: 0.4694\n",
            "Epoch 00195: val_loss improved from 1.47588 to 1.47546, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4771 - accuracy: 0.4698 - val_loss: 1.4755 - val_accuracy: 0.4778\n",
            "Epoch 196/1000\n",
            "112/114 [============================>.] - ETA: 0s - loss: 1.4767 - accuracy: 0.4693\n",
            "Epoch 00196: val_loss improved from 1.47546 to 1.47504, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4767 - accuracy: 0.4692 - val_loss: 1.4750 - val_accuracy: 0.4782\n",
            "Epoch 197/1000\n",
            "105/114 [==========================>...] - ETA: 0s - loss: 1.4759 - accuracy: 0.4710\n",
            "Epoch 00197: val_loss improved from 1.47504 to 1.47462, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4763 - accuracy: 0.4696 - val_loss: 1.4746 - val_accuracy: 0.4774\n",
            "Epoch 198/1000\n",
            "108/114 [===========================>..] - ETA: 0s - loss: 1.4759 - accuracy: 0.4711\n",
            "Epoch 00198: val_loss improved from 1.47462 to 1.47420, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4759 - accuracy: 0.4707 - val_loss: 1.4742 - val_accuracy: 0.4772\n",
            "Epoch 199/1000\n",
            "108/114 [===========================>..] - ETA: 0s - loss: 1.4757 - accuracy: 0.4711\n",
            "Epoch 00199: val_loss improved from 1.47420 to 1.47378, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4755 - accuracy: 0.4705 - val_loss: 1.4738 - val_accuracy: 0.4774\n",
            "Epoch 200/1000\n",
            "108/114 [===========================>..] - ETA: 0s - loss: 1.4750 - accuracy: 0.4703\n",
            "Epoch 00200: val_loss improved from 1.47378 to 1.47336, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4750 - accuracy: 0.4709 - val_loss: 1.4734 - val_accuracy: 0.4775\n",
            "Epoch 201/1000\n",
            "114/114 [==============================] - ETA: 0s - loss: 1.4746 - accuracy: 0.4708\n",
            "Epoch 00201: val_loss improved from 1.47336 to 1.47294, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4746 - accuracy: 0.4708 - val_loss: 1.4729 - val_accuracy: 0.4776\n",
            "Epoch 202/1000\n",
            "111/114 [============================>.] - ETA: 0s - loss: 1.4743 - accuracy: 0.4705\n",
            "Epoch 00202: val_loss improved from 1.47294 to 1.47252, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4742 - accuracy: 0.4708 - val_loss: 1.4725 - val_accuracy: 0.4781\n",
            "Epoch 203/1000\n",
            "110/114 [===========================>..] - ETA: 0s - loss: 1.4736 - accuracy: 0.4710\n",
            "Epoch 00203: val_loss improved from 1.47252 to 1.47209, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4738 - accuracy: 0.4710 - val_loss: 1.4721 - val_accuracy: 0.4772\n",
            "Epoch 204/1000\n",
            "109/114 [===========================>..] - ETA: 0s - loss: 1.4735 - accuracy: 0.4703\n",
            "Epoch 00204: val_loss improved from 1.47209 to 1.47167, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4734 - accuracy: 0.4704 - val_loss: 1.4717 - val_accuracy: 0.4778\n",
            "Epoch 205/1000\n",
            "111/114 [============================>.] - ETA: 0s - loss: 1.4729 - accuracy: 0.4721\n",
            "Epoch 00205: val_loss improved from 1.47167 to 1.47125, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4729 - accuracy: 0.4715 - val_loss: 1.4712 - val_accuracy: 0.4775\n",
            "Epoch 206/1000\n",
            "105/114 [==========================>...] - ETA: 0s - loss: 1.4725 - accuracy: 0.4709\n",
            "Epoch 00206: val_loss improved from 1.47125 to 1.47082, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4725 - accuracy: 0.4719 - val_loss: 1.4708 - val_accuracy: 0.4774\n",
            "Epoch 207/1000\n",
            "112/114 [============================>.] - ETA: 0s - loss: 1.4722 - accuracy: 0.4716\n",
            "Epoch 00207: val_loss improved from 1.47082 to 1.47040, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4721 - accuracy: 0.4717 - val_loss: 1.4704 - val_accuracy: 0.4775\n",
            "Epoch 208/1000\n",
            "105/114 [==========================>...] - ETA: 0s - loss: 1.4716 - accuracy: 0.4722\n",
            "Epoch 00208: val_loss improved from 1.47040 to 1.46998, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 8ms/step - loss: 1.4717 - accuracy: 0.4720 - val_loss: 1.4700 - val_accuracy: 0.4774\n",
            "Epoch 209/1000\n",
            "113/114 [============================>.] - ETA: 0s - loss: 1.4713 - accuracy: 0.4723\n",
            "Epoch 00209: val_loss improved from 1.46998 to 1.46955, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4713 - accuracy: 0.4723 - val_loss: 1.4696 - val_accuracy: 0.4772\n",
            "Epoch 210/1000\n",
            "112/114 [============================>.] - ETA: 0s - loss: 1.4708 - accuracy: 0.4724\n",
            "Epoch 00210: val_loss improved from 1.46955 to 1.46913, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4709 - accuracy: 0.4720 - val_loss: 1.4691 - val_accuracy: 0.4776\n",
            "Epoch 211/1000\n",
            "111/114 [============================>.] - ETA: 0s - loss: 1.4704 - accuracy: 0.4726\n",
            "Epoch 00211: val_loss improved from 1.46913 to 1.46870, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4704 - accuracy: 0.4721 - val_loss: 1.4687 - val_accuracy: 0.4775\n",
            "Epoch 212/1000\n",
            "110/114 [===========================>..] - ETA: 0s - loss: 1.4700 - accuracy: 0.4721\n",
            "Epoch 00212: val_loss improved from 1.46870 to 1.46828, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 8ms/step - loss: 1.4700 - accuracy: 0.4717 - val_loss: 1.4683 - val_accuracy: 0.4775\n",
            "Epoch 213/1000\n",
            "108/114 [===========================>..] - ETA: 0s - loss: 1.4694 - accuracy: 0.4719\n",
            "Epoch 00213: val_loss improved from 1.46828 to 1.46785, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4696 - accuracy: 0.4718 - val_loss: 1.4679 - val_accuracy: 0.4781\n",
            "Epoch 214/1000\n",
            "108/114 [===========================>..] - ETA: 0s - loss: 1.4691 - accuracy: 0.4723\n",
            "Epoch 00214: val_loss improved from 1.46785 to 1.46743, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4692 - accuracy: 0.4720 - val_loss: 1.4674 - val_accuracy: 0.4776\n",
            "Epoch 215/1000\n",
            "110/114 [===========================>..] - ETA: 0s - loss: 1.4690 - accuracy: 0.4713\n",
            "Epoch 00215: val_loss improved from 1.46743 to 1.46701, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4687 - accuracy: 0.4718 - val_loss: 1.4670 - val_accuracy: 0.4781\n",
            "Epoch 216/1000\n",
            "110/114 [===========================>..] - ETA: 0s - loss: 1.4685 - accuracy: 0.4716\n",
            "Epoch 00216: val_loss improved from 1.46701 to 1.46658, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4683 - accuracy: 0.4718 - val_loss: 1.4666 - val_accuracy: 0.4778\n",
            "Epoch 217/1000\n",
            "112/114 [============================>.] - ETA: 0s - loss: 1.4678 - accuracy: 0.4726\n",
            "Epoch 00217: val_loss improved from 1.46658 to 1.46615, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4679 - accuracy: 0.4723 - val_loss: 1.4662 - val_accuracy: 0.4781\n",
            "Epoch 218/1000\n",
            "107/114 [===========================>..] - ETA: 0s - loss: 1.4675 - accuracy: 0.4717\n",
            "Epoch 00218: val_loss improved from 1.46615 to 1.46572, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4675 - accuracy: 0.4719 - val_loss: 1.4657 - val_accuracy: 0.4783\n",
            "Epoch 219/1000\n",
            "111/114 [============================>.] - ETA: 0s - loss: 1.4668 - accuracy: 0.4724\n",
            "Epoch 00219: val_loss improved from 1.46572 to 1.46529, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4671 - accuracy: 0.4718 - val_loss: 1.4653 - val_accuracy: 0.4779\n",
            "Epoch 220/1000\n",
            "107/114 [===========================>..] - ETA: 0s - loss: 1.4663 - accuracy: 0.4725\n",
            "Epoch 00220: val_loss improved from 1.46529 to 1.46487, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4666 - accuracy: 0.4718 - val_loss: 1.4649 - val_accuracy: 0.4782\n",
            "Epoch 221/1000\n",
            "111/114 [============================>.] - ETA: 0s - loss: 1.4661 - accuracy: 0.4718\n",
            "Epoch 00221: val_loss improved from 1.46487 to 1.46444, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4662 - accuracy: 0.4723 - val_loss: 1.4644 - val_accuracy: 0.4786\n",
            "Epoch 222/1000\n",
            "108/114 [===========================>..] - ETA: 0s - loss: 1.4655 - accuracy: 0.4733\n",
            "Epoch 00222: val_loss improved from 1.46444 to 1.46401, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4658 - accuracy: 0.4721 - val_loss: 1.4640 - val_accuracy: 0.4781\n",
            "Epoch 223/1000\n",
            "111/114 [============================>.] - ETA: 0s - loss: 1.4653 - accuracy: 0.4721\n",
            "Epoch 00223: val_loss improved from 1.46401 to 1.46358, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4654 - accuracy: 0.4719 - val_loss: 1.4636 - val_accuracy: 0.4785\n",
            "Epoch 224/1000\n",
            "111/114 [============================>.] - ETA: 0s - loss: 1.4648 - accuracy: 0.4724\n",
            "Epoch 00224: val_loss improved from 1.46358 to 1.46315, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4649 - accuracy: 0.4721 - val_loss: 1.4632 - val_accuracy: 0.4782\n",
            "Epoch 225/1000\n",
            "110/114 [===========================>..] - ETA: 0s - loss: 1.4643 - accuracy: 0.4724\n",
            "Epoch 00225: val_loss improved from 1.46315 to 1.46273, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4645 - accuracy: 0.4721 - val_loss: 1.4627 - val_accuracy: 0.4790\n",
            "Epoch 226/1000\n",
            "107/114 [===========================>..] - ETA: 0s - loss: 1.4640 - accuracy: 0.4721\n",
            "Epoch 00226: val_loss improved from 1.46273 to 1.46230, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4641 - accuracy: 0.4718 - val_loss: 1.4623 - val_accuracy: 0.4789\n",
            "Epoch 227/1000\n",
            "109/114 [===========================>..] - ETA: 0s - loss: 1.4638 - accuracy: 0.4728\n",
            "Epoch 00227: val_loss improved from 1.46230 to 1.46187, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4637 - accuracy: 0.4727 - val_loss: 1.4619 - val_accuracy: 0.4793\n",
            "Epoch 228/1000\n",
            "110/114 [===========================>..] - ETA: 0s - loss: 1.4631 - accuracy: 0.4723\n",
            "Epoch 00228: val_loss improved from 1.46187 to 1.46144, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4632 - accuracy: 0.4724 - val_loss: 1.4614 - val_accuracy: 0.4793\n",
            "Epoch 229/1000\n",
            "109/114 [===========================>..] - ETA: 0s - loss: 1.4629 - accuracy: 0.4732\n",
            "Epoch 00229: val_loss improved from 1.46144 to 1.46101, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4628 - accuracy: 0.4726 - val_loss: 1.4610 - val_accuracy: 0.4796\n",
            "Epoch 230/1000\n",
            "104/114 [==========================>...] - ETA: 0s - loss: 1.4627 - accuracy: 0.4731\n",
            "Epoch 00230: val_loss improved from 1.46101 to 1.46058, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4624 - accuracy: 0.4735 - val_loss: 1.4606 - val_accuracy: 0.4787\n",
            "Epoch 231/1000\n",
            "108/114 [===========================>..] - ETA: 0s - loss: 1.4621 - accuracy: 0.4716\n",
            "Epoch 00231: val_loss improved from 1.46058 to 1.46015, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4620 - accuracy: 0.4725 - val_loss: 1.4602 - val_accuracy: 0.4793\n",
            "Epoch 232/1000\n",
            "106/114 [==========================>...] - ETA: 0s - loss: 1.4616 - accuracy: 0.4740\n",
            "Epoch 00232: val_loss improved from 1.46015 to 1.45972, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4615 - accuracy: 0.4729 - val_loss: 1.4597 - val_accuracy: 0.4794\n",
            "Epoch 233/1000\n",
            "105/114 [==========================>...] - ETA: 0s - loss: 1.4611 - accuracy: 0.4722\n",
            "Epoch 00233: val_loss improved from 1.45972 to 1.45929, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4611 - accuracy: 0.4728 - val_loss: 1.4593 - val_accuracy: 0.4798\n",
            "Epoch 234/1000\n",
            "112/114 [============================>.] - ETA: 0s - loss: 1.4606 - accuracy: 0.4722\n",
            "Epoch 00234: val_loss improved from 1.45929 to 1.45886, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4607 - accuracy: 0.4719 - val_loss: 1.4589 - val_accuracy: 0.4793\n",
            "Epoch 235/1000\n",
            "113/114 [============================>.] - ETA: 0s - loss: 1.4602 - accuracy: 0.4723\n",
            "Epoch 00235: val_loss improved from 1.45886 to 1.45843, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4603 - accuracy: 0.4724 - val_loss: 1.4584 - val_accuracy: 0.4796\n",
            "Epoch 236/1000\n",
            "111/114 [============================>.] - ETA: 0s - loss: 1.4601 - accuracy: 0.4722\n",
            "Epoch 00236: val_loss improved from 1.45843 to 1.45800, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4598 - accuracy: 0.4727 - val_loss: 1.4580 - val_accuracy: 0.4790\n",
            "Epoch 237/1000\n",
            "112/114 [============================>.] - ETA: 0s - loss: 1.4595 - accuracy: 0.4726\n",
            "Epoch 00237: val_loss improved from 1.45800 to 1.45756, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4594 - accuracy: 0.4725 - val_loss: 1.4576 - val_accuracy: 0.4794\n",
            "Epoch 238/1000\n",
            "114/114 [==============================] - ETA: 0s - loss: 1.4590 - accuracy: 0.4727\n",
            "Epoch 00238: val_loss improved from 1.45756 to 1.45713, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 8ms/step - loss: 1.4590 - accuracy: 0.4727 - val_loss: 1.4571 - val_accuracy: 0.4794\n",
            "Epoch 239/1000\n",
            "110/114 [===========================>..] - ETA: 0s - loss: 1.4586 - accuracy: 0.4726\n",
            "Epoch 00239: val_loss improved from 1.45713 to 1.45670, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4585 - accuracy: 0.4729 - val_loss: 1.4567 - val_accuracy: 0.4793\n",
            "Epoch 240/1000\n",
            "110/114 [===========================>..] - ETA: 0s - loss: 1.4579 - accuracy: 0.4732\n",
            "Epoch 00240: val_loss improved from 1.45670 to 1.45626, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4581 - accuracy: 0.4725 - val_loss: 1.4563 - val_accuracy: 0.4790\n",
            "Epoch 241/1000\n",
            "109/114 [===========================>..] - ETA: 0s - loss: 1.4572 - accuracy: 0.4744\n",
            "Epoch 00241: val_loss improved from 1.45626 to 1.45583, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4577 - accuracy: 0.4726 - val_loss: 1.4558 - val_accuracy: 0.4793\n",
            "Epoch 242/1000\n",
            "107/114 [===========================>..] - ETA: 0s - loss: 1.4573 - accuracy: 0.4727\n",
            "Epoch 00242: val_loss improved from 1.45583 to 1.45540, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4573 - accuracy: 0.4728 - val_loss: 1.4554 - val_accuracy: 0.4786\n",
            "Epoch 243/1000\n",
            "109/114 [===========================>..] - ETA: 0s - loss: 1.4567 - accuracy: 0.4737\n",
            "Epoch 00243: val_loss improved from 1.45540 to 1.45496, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4568 - accuracy: 0.4728 - val_loss: 1.4550 - val_accuracy: 0.4798\n",
            "Epoch 244/1000\n",
            "109/114 [===========================>..] - ETA: 0s - loss: 1.4565 - accuracy: 0.4729\n",
            "Epoch 00244: val_loss improved from 1.45496 to 1.45453, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4564 - accuracy: 0.4724 - val_loss: 1.4545 - val_accuracy: 0.4790\n",
            "Epoch 245/1000\n",
            "105/114 [==========================>...] - ETA: 0s - loss: 1.4553 - accuracy: 0.4741\n",
            "Epoch 00245: val_loss improved from 1.45453 to 1.45409, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4560 - accuracy: 0.4728 - val_loss: 1.4541 - val_accuracy: 0.4790\n",
            "Epoch 246/1000\n",
            "108/114 [===========================>..] - ETA: 0s - loss: 1.4555 - accuracy: 0.4728\n",
            "Epoch 00246: val_loss improved from 1.45409 to 1.45365, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4555 - accuracy: 0.4731 - val_loss: 1.4537 - val_accuracy: 0.4793\n",
            "Epoch 247/1000\n",
            "110/114 [===========================>..] - ETA: 0s - loss: 1.4550 - accuracy: 0.4735\n",
            "Epoch 00247: val_loss improved from 1.45365 to 1.45322, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4551 - accuracy: 0.4732 - val_loss: 1.4532 - val_accuracy: 0.4792\n",
            "Epoch 248/1000\n",
            "106/114 [==========================>...] - ETA: 0s - loss: 1.4549 - accuracy: 0.4726\n",
            "Epoch 00248: val_loss improved from 1.45322 to 1.45278, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4547 - accuracy: 0.4726 - val_loss: 1.4528 - val_accuracy: 0.4793\n",
            "Epoch 249/1000\n",
            "111/114 [============================>.] - ETA: 0s - loss: 1.4542 - accuracy: 0.4729\n",
            "Epoch 00249: val_loss improved from 1.45278 to 1.45235, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4542 - accuracy: 0.4728 - val_loss: 1.4523 - val_accuracy: 0.4789\n",
            "Epoch 250/1000\n",
            "107/114 [===========================>..] - ETA: 0s - loss: 1.4538 - accuracy: 0.4724\n",
            "Epoch 00250: val_loss improved from 1.45235 to 1.45191, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4538 - accuracy: 0.4727 - val_loss: 1.4519 - val_accuracy: 0.4796\n",
            "Epoch 251/1000\n",
            "107/114 [===========================>..] - ETA: 0s - loss: 1.4535 - accuracy: 0.4723\n",
            "Epoch 00251: val_loss improved from 1.45191 to 1.45147, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4534 - accuracy: 0.4728 - val_loss: 1.4515 - val_accuracy: 0.4790\n",
            "Epoch 252/1000\n",
            "108/114 [===========================>..] - ETA: 0s - loss: 1.4525 - accuracy: 0.4731\n",
            "Epoch 00252: val_loss improved from 1.45147 to 1.45103, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4529 - accuracy: 0.4728 - val_loss: 1.4510 - val_accuracy: 0.4793\n",
            "Epoch 253/1000\n",
            "105/114 [==========================>...] - ETA: 0s - loss: 1.4526 - accuracy: 0.4737\n",
            "Epoch 00253: val_loss improved from 1.45103 to 1.45060, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4525 - accuracy: 0.4734 - val_loss: 1.4506 - val_accuracy: 0.4793\n",
            "Epoch 254/1000\n",
            "109/114 [===========================>..] - ETA: 0s - loss: 1.4520 - accuracy: 0.4738\n",
            "Epoch 00254: val_loss improved from 1.45060 to 1.45015, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4521 - accuracy: 0.4735 - val_loss: 1.4502 - val_accuracy: 0.4793\n",
            "Epoch 255/1000\n",
            "108/114 [===========================>..] - ETA: 0s - loss: 1.4518 - accuracy: 0.4728\n",
            "Epoch 00255: val_loss improved from 1.45015 to 1.44971, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4516 - accuracy: 0.4730 - val_loss: 1.4497 - val_accuracy: 0.4794\n",
            "Epoch 256/1000\n",
            "108/114 [===========================>..] - ETA: 0s - loss: 1.4511 - accuracy: 0.4734\n",
            "Epoch 00256: val_loss improved from 1.44971 to 1.44928, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4512 - accuracy: 0.4731 - val_loss: 1.4493 - val_accuracy: 0.4794\n",
            "Epoch 257/1000\n",
            "113/114 [============================>.] - ETA: 0s - loss: 1.4507 - accuracy: 0.4732\n",
            "Epoch 00257: val_loss improved from 1.44928 to 1.44884, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4508 - accuracy: 0.4730 - val_loss: 1.4488 - val_accuracy: 0.4803\n",
            "Epoch 258/1000\n",
            "111/114 [============================>.] - ETA: 0s - loss: 1.4505 - accuracy: 0.4726\n",
            "Epoch 00258: val_loss improved from 1.44884 to 1.44840, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4503 - accuracy: 0.4729 - val_loss: 1.4484 - val_accuracy: 0.4807\n",
            "Epoch 259/1000\n",
            "109/114 [===========================>..] - ETA: 0s - loss: 1.4501 - accuracy: 0.4720\n",
            "Epoch 00259: val_loss improved from 1.44840 to 1.44795, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4499 - accuracy: 0.4730 - val_loss: 1.4480 - val_accuracy: 0.4800\n",
            "Epoch 260/1000\n",
            "109/114 [===========================>..] - ETA: 0s - loss: 1.4495 - accuracy: 0.4729\n",
            "Epoch 00260: val_loss improved from 1.44795 to 1.44751, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4495 - accuracy: 0.4729 - val_loss: 1.4475 - val_accuracy: 0.4798\n",
            "Epoch 261/1000\n",
            "112/114 [============================>.] - ETA: 0s - loss: 1.4490 - accuracy: 0.4726\n",
            "Epoch 00261: val_loss improved from 1.44751 to 1.44707, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4490 - accuracy: 0.4730 - val_loss: 1.4471 - val_accuracy: 0.4797\n",
            "Epoch 262/1000\n",
            "107/114 [===========================>..] - ETA: 0s - loss: 1.4488 - accuracy: 0.4724\n",
            "Epoch 00262: val_loss improved from 1.44707 to 1.44663, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4486 - accuracy: 0.4731 - val_loss: 1.4466 - val_accuracy: 0.4803\n",
            "Epoch 263/1000\n",
            "109/114 [===========================>..] - ETA: 0s - loss: 1.4481 - accuracy: 0.4732\n",
            "Epoch 00263: val_loss improved from 1.44663 to 1.44619, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4481 - accuracy: 0.4728 - val_loss: 1.4462 - val_accuracy: 0.4800\n",
            "Epoch 264/1000\n",
            "114/114 [==============================] - ETA: 0s - loss: 1.4477 - accuracy: 0.4730\n",
            "Epoch 00264: val_loss improved from 1.44619 to 1.44574, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4477 - accuracy: 0.4730 - val_loss: 1.4457 - val_accuracy: 0.4804\n",
            "Epoch 265/1000\n",
            "108/114 [===========================>..] - ETA: 0s - loss: 1.4470 - accuracy: 0.4736\n",
            "Epoch 00265: val_loss improved from 1.44574 to 1.44530, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4473 - accuracy: 0.4730 - val_loss: 1.4453 - val_accuracy: 0.4800\n",
            "Epoch 266/1000\n",
            "113/114 [============================>.] - ETA: 0s - loss: 1.4469 - accuracy: 0.4734\n",
            "Epoch 00266: val_loss improved from 1.44530 to 1.44486, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4468 - accuracy: 0.4735 - val_loss: 1.4449 - val_accuracy: 0.4804\n",
            "Epoch 267/1000\n",
            "108/114 [===========================>..] - ETA: 0s - loss: 1.4465 - accuracy: 0.4726\n",
            "Epoch 00267: val_loss improved from 1.44486 to 1.44441, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4464 - accuracy: 0.4738 - val_loss: 1.4444 - val_accuracy: 0.4807\n",
            "Epoch 268/1000\n",
            "107/114 [===========================>..] - ETA: 0s - loss: 1.4454 - accuracy: 0.4742\n",
            "Epoch 00268: val_loss improved from 1.44441 to 1.44397, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4459 - accuracy: 0.4737 - val_loss: 1.4440 - val_accuracy: 0.4807\n",
            "Epoch 269/1000\n",
            "108/114 [===========================>..] - ETA: 0s - loss: 1.4457 - accuracy: 0.4737\n",
            "Epoch 00269: val_loss improved from 1.44397 to 1.44352, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4455 - accuracy: 0.4741 - val_loss: 1.4435 - val_accuracy: 0.4811\n",
            "Epoch 270/1000\n",
            "109/114 [===========================>..] - ETA: 0s - loss: 1.4451 - accuracy: 0.4738\n",
            "Epoch 00270: val_loss improved from 1.44352 to 1.44308, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4451 - accuracy: 0.4739 - val_loss: 1.4431 - val_accuracy: 0.4805\n",
            "Epoch 271/1000\n",
            "107/114 [===========================>..] - ETA: 0s - loss: 1.4445 - accuracy: 0.4739\n",
            "Epoch 00271: val_loss improved from 1.44308 to 1.44263, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4446 - accuracy: 0.4740 - val_loss: 1.4426 - val_accuracy: 0.4801\n",
            "Epoch 272/1000\n",
            "108/114 [===========================>..] - ETA: 0s - loss: 1.4441 - accuracy: 0.4738\n",
            "Epoch 00272: val_loss improved from 1.44263 to 1.44219, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4442 - accuracy: 0.4737 - val_loss: 1.4422 - val_accuracy: 0.4814\n",
            "Epoch 273/1000\n",
            "106/114 [==========================>...] - ETA: 0s - loss: 1.4442 - accuracy: 0.4726\n",
            "Epoch 00273: val_loss improved from 1.44219 to 1.44175, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4437 - accuracy: 0.4737 - val_loss: 1.4417 - val_accuracy: 0.4804\n",
            "Epoch 274/1000\n",
            "108/114 [===========================>..] - ETA: 0s - loss: 1.4435 - accuracy: 0.4739\n",
            "Epoch 00274: val_loss improved from 1.44175 to 1.44130, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4433 - accuracy: 0.4738 - val_loss: 1.4413 - val_accuracy: 0.4811\n",
            "Epoch 275/1000\n",
            "107/114 [===========================>..] - ETA: 0s - loss: 1.4429 - accuracy: 0.4735\n",
            "Epoch 00275: val_loss improved from 1.44130 to 1.44085, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4429 - accuracy: 0.4737 - val_loss: 1.4409 - val_accuracy: 0.4808\n",
            "Epoch 276/1000\n",
            "107/114 [===========================>..] - ETA: 0s - loss: 1.4423 - accuracy: 0.4735\n",
            "Epoch 00276: val_loss improved from 1.44085 to 1.44040, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4424 - accuracy: 0.4738 - val_loss: 1.4404 - val_accuracy: 0.4811\n",
            "Epoch 277/1000\n",
            "107/114 [===========================>..] - ETA: 0s - loss: 1.4419 - accuracy: 0.4732\n",
            "Epoch 00277: val_loss improved from 1.44040 to 1.43996, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4420 - accuracy: 0.4739 - val_loss: 1.4400 - val_accuracy: 0.4816\n",
            "Epoch 278/1000\n",
            "107/114 [===========================>..] - ETA: 0s - loss: 1.4416 - accuracy: 0.4716\n",
            "Epoch 00278: val_loss improved from 1.43996 to 1.43951, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4415 - accuracy: 0.4735 - val_loss: 1.4395 - val_accuracy: 0.4819\n",
            "Epoch 279/1000\n",
            "112/114 [============================>.] - ETA: 0s - loss: 1.4410 - accuracy: 0.4743\n",
            "Epoch 00279: val_loss improved from 1.43951 to 1.43906, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4411 - accuracy: 0.4741 - val_loss: 1.4391 - val_accuracy: 0.4815\n",
            "Epoch 280/1000\n",
            "107/114 [===========================>..] - ETA: 0s - loss: 1.4407 - accuracy: 0.4736\n",
            "Epoch 00280: val_loss improved from 1.43906 to 1.43861, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4406 - accuracy: 0.4741 - val_loss: 1.4386 - val_accuracy: 0.4821\n",
            "Epoch 281/1000\n",
            "110/114 [===========================>..] - ETA: 0s - loss: 1.4403 - accuracy: 0.4741\n",
            "Epoch 00281: val_loss improved from 1.43861 to 1.43817, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4402 - accuracy: 0.4741 - val_loss: 1.4382 - val_accuracy: 0.4821\n",
            "Epoch 282/1000\n",
            "108/114 [===========================>..] - ETA: 0s - loss: 1.4395 - accuracy: 0.4743\n",
            "Epoch 00282: val_loss improved from 1.43817 to 1.43771, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4398 - accuracy: 0.4738 - val_loss: 1.4377 - val_accuracy: 0.4818\n",
            "Epoch 283/1000\n",
            "109/114 [===========================>..] - ETA: 0s - loss: 1.4391 - accuracy: 0.4746\n",
            "Epoch 00283: val_loss improved from 1.43771 to 1.43727, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4393 - accuracy: 0.4737 - val_loss: 1.4373 - val_accuracy: 0.4821\n",
            "Epoch 284/1000\n",
            "106/114 [==========================>...] - ETA: 0s - loss: 1.4390 - accuracy: 0.4741\n",
            "Epoch 00284: val_loss improved from 1.43727 to 1.43682, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4389 - accuracy: 0.4740 - val_loss: 1.4368 - val_accuracy: 0.4818\n",
            "Epoch 285/1000\n",
            "108/114 [===========================>..] - ETA: 0s - loss: 1.4384 - accuracy: 0.4747\n",
            "Epoch 00285: val_loss improved from 1.43682 to 1.43637, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4384 - accuracy: 0.4739 - val_loss: 1.4364 - val_accuracy: 0.4819\n",
            "Epoch 286/1000\n",
            "114/114 [==============================] - ETA: 0s - loss: 1.4380 - accuracy: 0.4739\n",
            "Epoch 00286: val_loss improved from 1.43637 to 1.43592, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4380 - accuracy: 0.4739 - val_loss: 1.4359 - val_accuracy: 0.4819\n",
            "Epoch 287/1000\n",
            "113/114 [============================>.] - ETA: 0s - loss: 1.4376 - accuracy: 0.4740\n",
            "Epoch 00287: val_loss improved from 1.43592 to 1.43546, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4375 - accuracy: 0.4740 - val_loss: 1.4355 - val_accuracy: 0.4821\n",
            "Epoch 288/1000\n",
            "106/114 [==========================>...] - ETA: 0s - loss: 1.4368 - accuracy: 0.4748\n",
            "Epoch 00288: val_loss improved from 1.43546 to 1.43501, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4371 - accuracy: 0.4739 - val_loss: 1.4350 - val_accuracy: 0.4819\n",
            "Epoch 289/1000\n",
            "109/114 [===========================>..] - ETA: 0s - loss: 1.4366 - accuracy: 0.4739\n",
            "Epoch 00289: val_loss improved from 1.43501 to 1.43456, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4366 - accuracy: 0.4738 - val_loss: 1.4346 - val_accuracy: 0.4819\n",
            "Epoch 290/1000\n",
            "113/114 [============================>.] - ETA: 0s - loss: 1.4362 - accuracy: 0.4738\n",
            "Epoch 00290: val_loss improved from 1.43456 to 1.43410, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4362 - accuracy: 0.4739 - val_loss: 1.4341 - val_accuracy: 0.4822\n",
            "Epoch 291/1000\n",
            "107/114 [===========================>..] - ETA: 0s - loss: 1.4357 - accuracy: 0.4739\n",
            "Epoch 00291: val_loss improved from 1.43410 to 1.43365, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4357 - accuracy: 0.4738 - val_loss: 1.4336 - val_accuracy: 0.4825\n",
            "Epoch 292/1000\n",
            "105/114 [==========================>...] - ETA: 0s - loss: 1.4356 - accuracy: 0.4739\n",
            "Epoch 00292: val_loss improved from 1.43365 to 1.43320, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4353 - accuracy: 0.4741 - val_loss: 1.4332 - val_accuracy: 0.4822\n",
            "Epoch 293/1000\n",
            "105/114 [==========================>...] - ETA: 0s - loss: 1.4346 - accuracy: 0.4745\n",
            "Epoch 00293: val_loss improved from 1.43320 to 1.43274, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4348 - accuracy: 0.4743 - val_loss: 1.4327 - val_accuracy: 0.4823\n",
            "Epoch 294/1000\n",
            "107/114 [===========================>..] - ETA: 0s - loss: 1.4343 - accuracy: 0.4735\n",
            "Epoch 00294: val_loss improved from 1.43274 to 1.43229, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4344 - accuracy: 0.4738 - val_loss: 1.4323 - val_accuracy: 0.4823\n",
            "Epoch 295/1000\n",
            "108/114 [===========================>..] - ETA: 0s - loss: 1.4339 - accuracy: 0.4753\n",
            "Epoch 00295: val_loss improved from 1.43229 to 1.43183, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 8ms/step - loss: 1.4339 - accuracy: 0.4746 - val_loss: 1.4318 - val_accuracy: 0.4823\n",
            "Epoch 296/1000\n",
            "112/114 [============================>.] - ETA: 0s - loss: 1.4335 - accuracy: 0.4746\n",
            "Epoch 00296: val_loss improved from 1.43183 to 1.43138, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4335 - accuracy: 0.4744 - val_loss: 1.4314 - val_accuracy: 0.4825\n",
            "Epoch 297/1000\n",
            "114/114 [==============================] - ETA: 0s - loss: 1.4330 - accuracy: 0.4740\n",
            "Epoch 00297: val_loss improved from 1.43138 to 1.43092, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4330 - accuracy: 0.4740 - val_loss: 1.4309 - val_accuracy: 0.4822\n",
            "Epoch 298/1000\n",
            "106/114 [==========================>...] - ETA: 0s - loss: 1.4325 - accuracy: 0.4743\n",
            "Epoch 00298: val_loss improved from 1.43092 to 1.43046, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4326 - accuracy: 0.4744 - val_loss: 1.4305 - val_accuracy: 0.4827\n",
            "Epoch 299/1000\n",
            "114/114 [==============================] - ETA: 0s - loss: 1.4321 - accuracy: 0.4740\n",
            "Epoch 00299: val_loss improved from 1.43046 to 1.43000, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4321 - accuracy: 0.4740 - val_loss: 1.4300 - val_accuracy: 0.4819\n",
            "Epoch 300/1000\n",
            "105/114 [==========================>...] - ETA: 0s - loss: 1.4316 - accuracy: 0.4746\n",
            "Epoch 00300: val_loss improved from 1.43000 to 1.42955, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4317 - accuracy: 0.4741 - val_loss: 1.4295 - val_accuracy: 0.4821\n",
            "Epoch 301/1000\n",
            "113/114 [============================>.] - ETA: 0s - loss: 1.4312 - accuracy: 0.4741\n",
            "Epoch 00301: val_loss improved from 1.42955 to 1.42909, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4312 - accuracy: 0.4741 - val_loss: 1.4291 - val_accuracy: 0.4825\n",
            "Epoch 302/1000\n",
            "109/114 [===========================>..] - ETA: 0s - loss: 1.4307 - accuracy: 0.4742\n",
            "Epoch 00302: val_loss improved from 1.42909 to 1.42863, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4308 - accuracy: 0.4741 - val_loss: 1.4286 - val_accuracy: 0.4825\n",
            "Epoch 303/1000\n",
            "112/114 [============================>.] - ETA: 0s - loss: 1.4304 - accuracy: 0.4742\n",
            "Epoch 00303: val_loss improved from 1.42863 to 1.42817, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4303 - accuracy: 0.4743 - val_loss: 1.4282 - val_accuracy: 0.4832\n",
            "Epoch 304/1000\n",
            "105/114 [==========================>...] - ETA: 0s - loss: 1.4299 - accuracy: 0.4753\n",
            "Epoch 00304: val_loss improved from 1.42817 to 1.42771, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4299 - accuracy: 0.4750 - val_loss: 1.4277 - val_accuracy: 0.4825\n",
            "Epoch 305/1000\n",
            "107/114 [===========================>..] - ETA: 0s - loss: 1.4295 - accuracy: 0.4728\n",
            "Epoch 00305: val_loss improved from 1.42771 to 1.42725, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4294 - accuracy: 0.4740 - val_loss: 1.4273 - val_accuracy: 0.4823\n",
            "Epoch 306/1000\n",
            "111/114 [============================>.] - ETA: 0s - loss: 1.4290 - accuracy: 0.4744\n",
            "Epoch 00306: val_loss improved from 1.42725 to 1.42679, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4290 - accuracy: 0.4747 - val_loss: 1.4268 - val_accuracy: 0.4823\n",
            "Epoch 307/1000\n",
            "105/114 [==========================>...] - ETA: 0s - loss: 1.4286 - accuracy: 0.4751\n",
            "Epoch 00307: val_loss improved from 1.42679 to 1.42633, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4285 - accuracy: 0.4743 - val_loss: 1.4263 - val_accuracy: 0.4825\n",
            "Epoch 308/1000\n",
            "106/114 [==========================>...] - ETA: 0s - loss: 1.4276 - accuracy: 0.4758\n",
            "Epoch 00308: val_loss improved from 1.42633 to 1.42587, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4280 - accuracy: 0.4749 - val_loss: 1.4259 - val_accuracy: 0.4825\n",
            "Epoch 309/1000\n",
            "113/114 [============================>.] - ETA: 0s - loss: 1.4276 - accuracy: 0.4749\n",
            "Epoch 00309: val_loss improved from 1.42587 to 1.42541, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4276 - accuracy: 0.4748 - val_loss: 1.4254 - val_accuracy: 0.4825\n",
            "Epoch 310/1000\n",
            "114/114 [==============================] - ETA: 0s - loss: 1.4271 - accuracy: 0.4754\n",
            "Epoch 00310: val_loss improved from 1.42541 to 1.42495, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4271 - accuracy: 0.4754 - val_loss: 1.4249 - val_accuracy: 0.4826\n",
            "Epoch 311/1000\n",
            "114/114 [==============================] - ETA: 0s - loss: 1.4267 - accuracy: 0.4753\n",
            "Epoch 00311: val_loss improved from 1.42495 to 1.42449, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4267 - accuracy: 0.4753 - val_loss: 1.4245 - val_accuracy: 0.4830\n",
            "Epoch 312/1000\n",
            "114/114 [==============================] - ETA: 0s - loss: 1.4262 - accuracy: 0.4745\n",
            "Epoch 00312: val_loss improved from 1.42449 to 1.42402, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4262 - accuracy: 0.4745 - val_loss: 1.4240 - val_accuracy: 0.4833\n",
            "Epoch 313/1000\n",
            "107/114 [===========================>..] - ETA: 0s - loss: 1.4258 - accuracy: 0.4747\n",
            "Epoch 00313: val_loss improved from 1.42402 to 1.42356, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4258 - accuracy: 0.4754 - val_loss: 1.4236 - val_accuracy: 0.4834\n",
            "Epoch 314/1000\n",
            "108/114 [===========================>..] - ETA: 0s - loss: 1.4254 - accuracy: 0.4758\n",
            "Epoch 00314: val_loss improved from 1.42356 to 1.42310, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4253 - accuracy: 0.4752 - val_loss: 1.4231 - val_accuracy: 0.4834\n",
            "Epoch 315/1000\n",
            "113/114 [============================>.] - ETA: 0s - loss: 1.4249 - accuracy: 0.4754\n",
            "Epoch 00315: val_loss improved from 1.42310 to 1.42263, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4248 - accuracy: 0.4753 - val_loss: 1.4226 - val_accuracy: 0.4836\n",
            "Epoch 316/1000\n",
            "105/114 [==========================>...] - ETA: 0s - loss: 1.4243 - accuracy: 0.4753\n",
            "Epoch 00316: val_loss improved from 1.42263 to 1.42217, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4244 - accuracy: 0.4753 - val_loss: 1.4222 - val_accuracy: 0.4844\n",
            "Epoch 317/1000\n",
            "106/114 [==========================>...] - ETA: 0s - loss: 1.4239 - accuracy: 0.4762\n",
            "Epoch 00317: val_loss improved from 1.42217 to 1.42170, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4239 - accuracy: 0.4753 - val_loss: 1.4217 - val_accuracy: 0.4841\n",
            "Epoch 318/1000\n",
            "109/114 [===========================>..] - ETA: 0s - loss: 1.4236 - accuracy: 0.4759\n",
            "Epoch 00318: val_loss improved from 1.42170 to 1.42124, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4235 - accuracy: 0.4754 - val_loss: 1.4212 - val_accuracy: 0.4843\n",
            "Epoch 319/1000\n",
            "107/114 [===========================>..] - ETA: 0s - loss: 1.4232 - accuracy: 0.4762\n",
            "Epoch 00319: val_loss improved from 1.42124 to 1.42077, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4230 - accuracy: 0.4755 - val_loss: 1.4208 - val_accuracy: 0.4848\n",
            "Epoch 320/1000\n",
            "107/114 [===========================>..] - ETA: 0s - loss: 1.4222 - accuracy: 0.4765\n",
            "Epoch 00320: val_loss improved from 1.42077 to 1.42031, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4225 - accuracy: 0.4755 - val_loss: 1.4203 - val_accuracy: 0.4851\n",
            "Epoch 321/1000\n",
            "108/114 [===========================>..] - ETA: 0s - loss: 1.4215 - accuracy: 0.4771\n",
            "Epoch 00321: val_loss improved from 1.42031 to 1.41984, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4221 - accuracy: 0.4758 - val_loss: 1.4198 - val_accuracy: 0.4851\n",
            "Epoch 322/1000\n",
            "110/114 [===========================>..] - ETA: 0s - loss: 1.4214 - accuracy: 0.4766\n",
            "Epoch 00322: val_loss improved from 1.41984 to 1.41937, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4216 - accuracy: 0.4761 - val_loss: 1.4194 - val_accuracy: 0.4850\n",
            "Epoch 323/1000\n",
            "106/114 [==========================>...] - ETA: 0s - loss: 1.4211 - accuracy: 0.4756\n",
            "Epoch 00323: val_loss improved from 1.41937 to 1.41891, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4211 - accuracy: 0.4759 - val_loss: 1.4189 - val_accuracy: 0.4850\n",
            "Epoch 324/1000\n",
            "106/114 [==========================>...] - ETA: 0s - loss: 1.4206 - accuracy: 0.4766\n",
            "Epoch 00324: val_loss improved from 1.41891 to 1.41844, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4207 - accuracy: 0.4756 - val_loss: 1.4184 - val_accuracy: 0.4850\n",
            "Epoch 325/1000\n",
            "113/114 [============================>.] - ETA: 0s - loss: 1.4202 - accuracy: 0.4759\n",
            "Epoch 00325: val_loss improved from 1.41844 to 1.41797, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4202 - accuracy: 0.4759 - val_loss: 1.4180 - val_accuracy: 0.4855\n",
            "Epoch 326/1000\n",
            "114/114 [==============================] - ETA: 0s - loss: 1.4198 - accuracy: 0.4759\n",
            "Epoch 00326: val_loss improved from 1.41797 to 1.41750, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4198 - accuracy: 0.4759 - val_loss: 1.4175 - val_accuracy: 0.4852\n",
            "Epoch 327/1000\n",
            "106/114 [==========================>...] - ETA: 0s - loss: 1.4193 - accuracy: 0.4758\n",
            "Epoch 00327: val_loss improved from 1.41750 to 1.41703, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4193 - accuracy: 0.4758 - val_loss: 1.4170 - val_accuracy: 0.4850\n",
            "Epoch 328/1000\n",
            "112/114 [============================>.] - ETA: 0s - loss: 1.4190 - accuracy: 0.4759\n",
            "Epoch 00328: val_loss improved from 1.41703 to 1.41656, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4188 - accuracy: 0.4759 - val_loss: 1.4166 - val_accuracy: 0.4854\n",
            "Epoch 329/1000\n",
            "111/114 [============================>.] - ETA: 0s - loss: 1.4185 - accuracy: 0.4753\n",
            "Epoch 00329: val_loss improved from 1.41656 to 1.41609, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4184 - accuracy: 0.4761 - val_loss: 1.4161 - val_accuracy: 0.4851\n",
            "Epoch 330/1000\n",
            "112/114 [============================>.] - ETA: 0s - loss: 1.4178 - accuracy: 0.4760\n",
            "Epoch 00330: val_loss improved from 1.41609 to 1.41561, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4179 - accuracy: 0.4762 - val_loss: 1.4156 - val_accuracy: 0.4852\n",
            "Epoch 331/1000\n",
            "109/114 [===========================>..] - ETA: 0s - loss: 1.4173 - accuracy: 0.4763\n",
            "Epoch 00331: val_loss improved from 1.41561 to 1.41514, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4174 - accuracy: 0.4758 - val_loss: 1.4151 - val_accuracy: 0.4851\n",
            "Epoch 332/1000\n",
            "106/114 [==========================>...] - ETA: 0s - loss: 1.4172 - accuracy: 0.4751\n",
            "Epoch 00332: val_loss improved from 1.41514 to 1.41467, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4170 - accuracy: 0.4760 - val_loss: 1.4147 - val_accuracy: 0.4851\n",
            "Epoch 333/1000\n",
            "113/114 [============================>.] - ETA: 0s - loss: 1.4165 - accuracy: 0.4759\n",
            "Epoch 00333: val_loss improved from 1.41467 to 1.41420, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4165 - accuracy: 0.4760 - val_loss: 1.4142 - val_accuracy: 0.4848\n",
            "Epoch 334/1000\n",
            "111/114 [============================>.] - ETA: 0s - loss: 1.4159 - accuracy: 0.4763\n",
            "Epoch 00334: val_loss improved from 1.41420 to 1.41372, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4160 - accuracy: 0.4760 - val_loss: 1.4137 - val_accuracy: 0.4851\n",
            "Epoch 335/1000\n",
            "107/114 [===========================>..] - ETA: 0s - loss: 1.4162 - accuracy: 0.4755\n",
            "Epoch 00335: val_loss improved from 1.41372 to 1.41325, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4156 - accuracy: 0.4762 - val_loss: 1.4133 - val_accuracy: 0.4850\n",
            "Epoch 336/1000\n",
            "112/114 [============================>.] - ETA: 0s - loss: 1.4151 - accuracy: 0.4760\n",
            "Epoch 00336: val_loss improved from 1.41325 to 1.41278, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4151 - accuracy: 0.4762 - val_loss: 1.4128 - val_accuracy: 0.4854\n",
            "Epoch 337/1000\n",
            "112/114 [============================>.] - ETA: 0s - loss: 1.4147 - accuracy: 0.4763\n",
            "Epoch 00337: val_loss improved from 1.41278 to 1.41230, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4146 - accuracy: 0.4762 - val_loss: 1.4123 - val_accuracy: 0.4850\n",
            "Epoch 338/1000\n",
            "114/114 [==============================] - ETA: 0s - loss: 1.4141 - accuracy: 0.4763\n",
            "Epoch 00338: val_loss improved from 1.41230 to 1.41183, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4141 - accuracy: 0.4763 - val_loss: 1.4118 - val_accuracy: 0.4848\n",
            "Epoch 339/1000\n",
            "107/114 [===========================>..] - ETA: 0s - loss: 1.4135 - accuracy: 0.4767\n",
            "Epoch 00339: val_loss improved from 1.41183 to 1.41135, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4137 - accuracy: 0.4764 - val_loss: 1.4113 - val_accuracy: 0.4851\n",
            "Epoch 340/1000\n",
            "111/114 [============================>.] - ETA: 0s - loss: 1.4133 - accuracy: 0.4765\n",
            "Epoch 00340: val_loss improved from 1.41135 to 1.41087, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4132 - accuracy: 0.4765 - val_loss: 1.4109 - val_accuracy: 0.4845\n",
            "Epoch 341/1000\n",
            "109/114 [===========================>..] - ETA: 0s - loss: 1.4124 - accuracy: 0.4772\n",
            "Epoch 00341: val_loss improved from 1.41087 to 1.41040, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4127 - accuracy: 0.4764 - val_loss: 1.4104 - val_accuracy: 0.4845\n",
            "Epoch 342/1000\n",
            "108/114 [===========================>..] - ETA: 0s - loss: 1.4119 - accuracy: 0.4771\n",
            "Epoch 00342: val_loss improved from 1.41040 to 1.40992, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4123 - accuracy: 0.4766 - val_loss: 1.4099 - val_accuracy: 0.4848\n",
            "Epoch 343/1000\n",
            "114/114 [==============================] - ETA: 0s - loss: 1.4118 - accuracy: 0.4764\n",
            "Epoch 00343: val_loss improved from 1.40992 to 1.40945, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4118 - accuracy: 0.4764 - val_loss: 1.4094 - val_accuracy: 0.4848\n",
            "Epoch 344/1000\n",
            "107/114 [===========================>..] - ETA: 0s - loss: 1.4117 - accuracy: 0.4774\n",
            "Epoch 00344: val_loss improved from 1.40945 to 1.40897, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4113 - accuracy: 0.4761 - val_loss: 1.4090 - val_accuracy: 0.4848\n",
            "Epoch 345/1000\n",
            "107/114 [===========================>..] - ETA: 0s - loss: 1.4110 - accuracy: 0.4763\n",
            "Epoch 00345: val_loss improved from 1.40897 to 1.40849, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 6ms/step - loss: 1.4109 - accuracy: 0.4765 - val_loss: 1.4085 - val_accuracy: 0.4851\n",
            "Epoch 346/1000\n",
            "113/114 [============================>.] - ETA: 0s - loss: 1.4104 - accuracy: 0.4763\n",
            "Epoch 00346: val_loss improved from 1.40849 to 1.40802, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 8ms/step - loss: 1.4104 - accuracy: 0.4763 - val_loss: 1.4080 - val_accuracy: 0.4856\n",
            "Epoch 347/1000\n",
            "113/114 [============================>.] - ETA: 0s - loss: 1.4099 - accuracy: 0.4763\n",
            "Epoch 00347: val_loss improved from 1.40802 to 1.40753, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4099 - accuracy: 0.4763 - val_loss: 1.4075 - val_accuracy: 0.4858\n",
            "Epoch 348/1000\n",
            "105/114 [==========================>...] - ETA: 0s - loss: 1.4097 - accuracy: 0.4759\n",
            "Epoch 00348: val_loss improved from 1.40753 to 1.40706, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4094 - accuracy: 0.4761 - val_loss: 1.4071 - val_accuracy: 0.4852\n",
            "Epoch 349/1000\n",
            "114/114 [==============================] - ETA: 0s - loss: 1.4090 - accuracy: 0.4767\n",
            "Epoch 00349: val_loss improved from 1.40706 to 1.40658, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4090 - accuracy: 0.4767 - val_loss: 1.4066 - val_accuracy: 0.4851\n",
            "Epoch 350/1000\n",
            "111/114 [============================>.] - ETA: 0s - loss: 1.4085 - accuracy: 0.4773\n",
            "Epoch 00350: val_loss improved from 1.40658 to 1.40610, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4085 - accuracy: 0.4766 - val_loss: 1.4061 - val_accuracy: 0.4855\n",
            "Epoch 351/1000\n",
            "105/114 [==========================>...] - ETA: 0s - loss: 1.4086 - accuracy: 0.4764\n",
            "Epoch 00351: val_loss improved from 1.40610 to 1.40562, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4080 - accuracy: 0.4765 - val_loss: 1.4056 - val_accuracy: 0.4856\n",
            "Epoch 352/1000\n",
            "114/114 [==============================] - ETA: 0s - loss: 1.4075 - accuracy: 0.4763\n",
            "Epoch 00352: val_loss improved from 1.40562 to 1.40513, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4075 - accuracy: 0.4763 - val_loss: 1.4051 - val_accuracy: 0.4854\n",
            "Epoch 353/1000\n",
            "109/114 [===========================>..] - ETA: 0s - loss: 1.4072 - accuracy: 0.4769\n",
            "Epoch 00353: val_loss improved from 1.40513 to 1.40465, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4071 - accuracy: 0.4766 - val_loss: 1.4046 - val_accuracy: 0.4855\n",
            "Epoch 354/1000\n",
            "111/114 [============================>.] - ETA: 0s - loss: 1.4065 - accuracy: 0.4773\n",
            "Epoch 00354: val_loss improved from 1.40465 to 1.40417, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4066 - accuracy: 0.4770 - val_loss: 1.4042 - val_accuracy: 0.4848\n",
            "Epoch 355/1000\n",
            "113/114 [============================>.] - ETA: 0s - loss: 1.4061 - accuracy: 0.4767\n",
            "Epoch 00355: val_loss improved from 1.40417 to 1.40368, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4061 - accuracy: 0.4768 - val_loss: 1.4037 - val_accuracy: 0.4848\n",
            "Epoch 356/1000\n",
            "114/114 [==============================] - ETA: 0s - loss: 1.4056 - accuracy: 0.4769\n",
            "Epoch 00356: val_loss improved from 1.40368 to 1.40320, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 8ms/step - loss: 1.4056 - accuracy: 0.4769 - val_loss: 1.4032 - val_accuracy: 0.4844\n",
            "Epoch 357/1000\n",
            "106/114 [==========================>...] - ETA: 0s - loss: 1.4051 - accuracy: 0.4767\n",
            "Epoch 00357: val_loss improved from 1.40320 to 1.40272, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4051 - accuracy: 0.4767 - val_loss: 1.4027 - val_accuracy: 0.4858\n",
            "Epoch 358/1000\n",
            "114/114 [==============================] - ETA: 0s - loss: 1.4047 - accuracy: 0.4765\n",
            "Epoch 00358: val_loss improved from 1.40272 to 1.40223, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4047 - accuracy: 0.4765 - val_loss: 1.4022 - val_accuracy: 0.4856\n",
            "Epoch 359/1000\n",
            "112/114 [============================>.] - ETA: 0s - loss: 1.4044 - accuracy: 0.4766\n",
            "Epoch 00359: val_loss improved from 1.40223 to 1.40175, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4042 - accuracy: 0.4766 - val_loss: 1.4017 - val_accuracy: 0.4848\n",
            "Epoch 360/1000\n",
            "112/114 [============================>.] - ETA: 0s - loss: 1.4037 - accuracy: 0.4769\n",
            "Epoch 00360: val_loss improved from 1.40175 to 1.40126, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 7ms/step - loss: 1.4037 - accuracy: 0.4768 - val_loss: 1.4013 - val_accuracy: 0.4854\n",
            "Epoch 361/1000\n",
            "112/114 [============================>.] - ETA: 0s - loss: 1.4031 - accuracy: 0.4768\n",
            "Epoch 00361: val_loss improved from 1.40126 to 1.40077, saving model to /content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_2vrs/one_qmkdcsgd_true_1.h5\n",
            "114/114 [==============================] - 1s 8ms/step - loss: 1.4032 - accuracy: 0.4769 - val_loss: 1.4008 - val_accuracy: 0.4855\n",
            "Epoch 362/1000\n",
            " 47/114 [===========>..................] - ETA: 0s - loss: 1.4016 - accuracy: 0.4776"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-f74904dddcc3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;31m# print(Acc_3)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqmkdc2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_bin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "id": "RI37flci7_fx",
        "outputId": "dde854a5-9039-44dc-e91f-1f607a79aaf7"
      },
      "source": [
        "input_dim = X_train.shape[1]\n",
        "num_classes = np.unique(y_train).shape[0]\n",
        "gamma = 4\n",
        "learning_rate= 5e-7\n",
        "component_dim= 1000\n",
        "eig_percentage= 0.25\n",
        "epochs = 100\n",
        "decay = 0\n",
        "random_state = 0\n",
        "\n",
        "num_eig = round(eig_percentage * component_dim)\n",
        "\n",
        "opt = keras.optimizers.Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, decay=decay)\n",
        "\n",
        "fm_x = layers.QFeatureMapRFF(input_dim, dim=component_dim, gamma=gamma, random_state=random_state)\n",
        "\n",
        "qmkdc = models.QMKDClassifier(fm_x=fm_x, dim_x=component_dim, num_classes=num_classes)\n",
        "qmkdc.compile()\n",
        "qmkdc.fit(X_train, y_train, epochs=1, batch_size=256, verbose=20)\n",
        "\n",
        "qmkdc1 = models.QMKDClassifierSGD(input_dim=input_dim, dim_x=component_dim, num_eig=num_eig,\n",
        "                                  num_classes=num_classes, gamma=gamma, random_state=random_state, fm_x=fm_x)\n",
        "\n",
        "qmkdc1.layers[0].trainable = False\n",
        "\n",
        "qmkdc1.compile(loss=tf.keras.losses.CategoricalCrossentropy(), optimizer=opt, metrics=['accuracy'])\n",
        "\n",
        "qmkdc1.set_rhos(qmkdc.get_rhos())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-ce1dbb1a2ad9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m qmkdc1 = models.QMKDClassifierSGD(input_dim=input_dim, dim_x=component_dim, num_eig=num_eig,\n\u001b[0;32m---> 22\u001b[0;31m                                   num_classes=num_classes, gamma=gamma, random_state=random_state, fm_x=fm_x)\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mqmkdc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'fm_x'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhUAjIfS8pGK"
      },
      "source": [
        "# Tensorboard \n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir \"/content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_w2v\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yd_bP-pd8tV-"
      },
      "source": [
        "# Creation of the model without random search. It can be used to verify the initial performance of the model.\n",
        "import datetime, os\n",
        "logdir = os.path.join(\"/content/drive/MyDrive/Tesis_Maestria/Experiments/Experiment_2/result_w2v\", \"letters-\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
        "\n",
        "# model_mnist = create_model(input_dim, num_classes, component_dim=5000, gamma=2**0, lr=0.000005, decay=1e-9,\n",
        "#                   random_state=None, eig_percentage=0.5, initialize_with_rff=True,\n",
        "#                   type_of_rff=\"rff\", fix_rff=False)\n",
        "\n",
        "y_train_bin = tf.reshape(tf.keras.backend.one_hot(y_train, num_classes), (-1, num_classes))\n",
        "y_test_bin = tf.reshape(tf.keras.backend.one_hot(y_test, num_classes), (-1, num_classes))\n",
        "qmkdc1.fit(X_train, y_train_bin.numpy(), epochs=100, batch_size=256, validation_data=(X_test, y_test_bin.numpy()), \n",
        "            callbacks=[tensorboard_callback])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}